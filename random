

# Load Spacy model
nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])

# Adding custom stopwords from the provided list
df_stopwords = pd.read_csv('list_stopwords.csv')
stopword_list = df_stopwords['Words'].to_list()

# Adding stopwords to Spacy module
for word in stopword_list:
    STOP_WORDS.add(word)
    nlp.vocab[word].is_stop = True

# Additional stopwords to filter out single characters and unwanted terms
extra_stopwords = set(['d', 'ir', '--', '---', '-', 't', 'k'])
for word in extra_stopwords:
    STOP_WORDS.add(word)
    nlp.vocab[word].is_stop = True

# Define multi-word phrases for normalization
multi_word_phrases = {
    "account open": ["account open", "open account", "account opening", "open accounts"],
    "client acquisition": ["acquired new client", "acquiring new clients"],
    "new account": ["account new", "open new account"],
    "client request": ["request client"],
    "market disruption": ["disruption market"],
    "new client": ["client new"],
    "commercial card": ["card commercial"],
    "bb transfer": ["transfer bb"],
    "core cash": ["cash core"],
    "cash liquidity": ["liquidity cash"],
}

# Function to expand contractions
contractions_dict = {
    "ain't": "are not", "'s": " is", "aren't": "are not", "can't": "cannot",
    "'cause": "because", "could've": "could have", "couldn't": "could not", 
    "didn't": "did not", "doesn't": "does not", "don't": "do not",
    "hadn't": "had not", "hasn't": "has not", "haven't": "have not", 
    "he'd": "he would", "he'll": "he will", "I'd": "I would", "I'll": "I will", 
    "I'm": "I am", "I've": "I have", "isn't": "is not", "it'd": "it would", 
    "it'll": "it will", "let's": "let us", "ma'am": "madam", "might've": "might have", 
    "must've": "must have", "needn't": "need not", "o'clock": "of the clock", 
    "shan't": "shall not", "she'd": "she would", "she'll": "she will", 
    "should've": "should have", "shouldn't": "should not", "that'd": "that would", 
    "there'd": "there would", "they'd": "they would", "they'll": "they will", 
    "they're": "they are", "they've": "they have", "won't": "will not", 
    "would've": "would have", "wouldn't": "would not", "y'all": "you all"
}

contractions_re = re.compile('(%s)' % '|'.join(contractions_dict.keys()))

def expand_contractions(text):
    def replace(match):
        return contractions_dict[match.group(0)]
    return contractions_re.sub(replace, text)

# **Function to remove unwanted dashes and special characters**
def remove_hyphens_and_special_chars(text):
    return re.sub(r'\s*[-–—]+\s*', ' ', text)

# Function to normalize multi-word phrases
def normalize_phrases(text):
    for normalized, variants in multi_word_phrases.items():
        for variant in variants:
            text = text.replace(variant, normalized)
    return text

# Function to remove digits
def remove_digits(text):
    return re.sub(r'\d+', '', text)

# Function to remove extra punctuation/marks
def remove_extra_marks(text):
    return re.sub(r'[^\w\s]', '', text)

# **Function to remove single-character tokens**
def remove_single_char_tokens(text):
    return ' '.join([word for word in text.split() if len(word) > 1])

# Function to remove stopwords
def remove_stopwords(text):
    doc = nlp(text)
    return ' '.join([token.text for token in doc if not token.is_stop])

# Function to remove erroneous tokens like 'th'
def remove_invalid_tokens(text):
    invalid_tokens = ['th']
    tokens = text.split()
    return ' '.join([word for word in tokens if word not in invalid_tokens])

# Function to lemmatize text
def lemmatize(text):
    doc = nlp(text)
    return ' '.join([token.lemma_ for token in doc])

# **Rare word filtering**
def remove_rare_words(text, word_counter, min_freq=2):
    tokens = text.split()
    return ' '.join([word for word in tokens if word_counter[word] >= min_freq])

# **Full Preprocessing**
def full_preprocess(text, word_counter=None, min_freq=2):
    text = expand_contractions(text)  # Step 1: Expand contractions
    text = remove_hyphens_and_special_chars(text)  # Step 2: Remove hyphens
    text = text.lower()  # Step 3: Convert to lowercase
    text = remove_digits(text)  # Step 4: Remove digits
    text = remove_extra_marks(text)  # Step 5: Remove extra marks
    text = normalize_phrases(text)  # Step 6: Normalize multi-word phrases
    text = remove_stopwords(text)  # Step 7: Remove stopwords
    text = lemmatize(text)  # Step 8: Lemmatization
    text = remove_invalid_tokens(text)  # Step 9: Remove invalid tokens
    text = remove_single_char_tokens(text)  # Step 10: Remove single-character tokens

    if word_counter is not None:
        text = remove_rare_words(text, word_counter, min_freq=min_freq)

    return text

# Test sentence to validate preprocessing
test_sentence = """
    The account opening process was tedious. Client acquisition was smooth for the Union Bank.
    The market disruption caused by core cash management issues affected the overall performance.
"""

# Simulate word frequencies (in a real case, this should be built from the dataset)
test_word_counter = Counter(test_sentence.lower().split())

# Apply preprocessing to test sentence
processed_test_sentence = full_preprocess(test_sentence, word_counter=test_word_counter, min_freq=1)

# Print the processed test sentence
print("Original Test Sentence:")
print(test_sentence)

print("\nProcessed Test Sentence:")
print(processed_test_sentence)

# Step 3: Apply Preprocessing to Non-RST Data

# Assuming `non_rst_cleaned` dataframe is already cleaned with missing values removed.
# Apply preprocessing to the cleaned Non-RST data
non_rst_cleaned['processed_text'] = non_rst_cleaned['combined_text'].apply(lambda x: full_preprocess(x))

# Print a sample of the processed Non-RST dataset
print(non_rst_cleaned[['combined_text', 'processed_text']].head())

# Save the processed data for future analysis
non_rst_cleaned.to_csv('non_rst_cleaned_processed.csv', index=False)
