import pandas as pd
import spacy
from collections import Counter
import matplotlib.pyplot as plt
import seaborn as sns
import re
import gensim
from datetime import datetime

# Load Spacy model (without custom stopwords)
nlp = spacy.load('en_core_web_md', disable=['parser', 'ner'])
nlp.max_length = 2000000  # Increase SpaCy's max_length to handle larger texts

# Contractions and abbreviations mapping
contractions = {
    "won't": "will not",
    "can't": "cannot",
    "i'm": "i am",
    "it's": "it is",
    "we've": "we have",
    "they're": "they are",
    "corp": "corporation",
    "inc": "incorporated",
}

# Custom phrase mappings (similar bigrams grouped under a common name)
phrase_mappings = {
    "account open": "account_open",
    "open account": "account_open",
    "account opening": "account_open",
    "new account": "account_open",
    # Add more custom mappings as needed
}

# Function to expand contractions
def expand_contractions(text):
    for contraction, expansion in contractions.items():
        text = re.sub(r"\b" + contraction + r"\b", expansion, text)
    return text

# Function to apply custom phrase mappings
def apply_custom_phrases(tokens):
    return [phrase_mappings.get(token, token) for token in tokens]

# Function to detect whether a word is a date
def is_date(string):
    try:
        datetime.strptime(string, '%Y-%m-%d')
        return True
    except ValueError:
        return False

# Function to preprocess text in chunks
def process_text_in_chunks(text, nlp, chunk_size=100000):
    """Process text in chunks to avoid exceeding SpaCy's max length"""
    chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]
    tokens = []
    for chunk in chunks:
        doc = nlp(chunk)
        tokens.extend([token.lemma_ for token in doc if not token.is_stop and not token.is_punct and len(token.lemma_) > 2])
    return tokens

# Function to preprocess text and remove rare words, separating rare words into date and text categories
def preprocess(document, word_freqs, rare_threshold, rare_words_date, rare_words_text):
    # Convert to lowercase
    document = document.lower()

    # Expand contractions
    document = expand_contractions(document)

    # Remove special characters, numbers, and underscores
    document = re.sub(r'[^a-zA-Z\s]', '', document)

    # Remove extra spaces
    document = re.sub(r'\s+', ' ', document).strip()

    # Tokenize and Lemmatize
    doc = nlp(document)

    words = []
    for token in doc:
        if not token.is_stop and not token.is_punct and len(token.lemma_) > 2:
            lemma = token.lemma_
            if word_freqs[lemma] <= rare_threshold:
                if is_date(lemma):
                    rare_words_date.append(lemma)
                else:
                    rare_words_text.append(lemma)
            else:
                words.append(lemma)
    
    # Apply custom phrase mappings
    words = apply_custom_phrases(words)
    
    return words

# Function to get n-gram frequency
def get_ngram_freqs(tokens_list, n=1):
    if n == 1:
        return Counter(tokens_list).most_common(20)
    else:
        ngrams = zip(*[tokens_list[i:] for i in range(n)])
        return Counter([' '.join(ngram) for ngram in ngrams]).most_common(20)

# Function to display and plot n-grams
def display_ngrams(ngram_freq, title):
    # Create a DataFrame to display as a table
    ngram_df = pd.DataFrame(ngram_freq, columns=['ngram', 'frequency'])
    
    # Print frequency table
    print(f"\n{'='*40}")
    print(f"{title} Frequency Table:")
    print(ngram_df.to_string(index=False))
    print(f"{'='*40}")
    
    # Plot the top 20 n-grams using Seaborn
    plt.figure(figsize=(12, 8))
    sns.barplot(x='ngram', y='frequency', data=ngram_df, palette='coolwarm')
    plt.xticks(rotation=45, ha='right', fontsize=12)
    plt.yticks(fontsize=12)
    plt.title(f'Top 20 {title}', fontsize=16, weight='bold')
    plt.xlabel('N-gram', fontsize=14, weight='bold')
    plt.ylabel('Frequency', fontsize=14, weight='bold')
    plt.tight_layout()
    plt.show()

# Read data from Salesforce analytics
df_salesforce = pd.read_excel('Data/exclude_won_rst.xlsx')

# Concatenate text columns into a single string variable
df_salesforce['combined_text'] = df_salesforce[['record_comment_text', 
                                                'executive_summary_text', 
                                                'win_loss_reason_text', 
                                                'win_loss_comments_text']].apply(lambda x: ' '.join(x.dropna()), axis=1)

# First pass to calculate word frequencies across the entire dataset
all_text = ' '.join(df_salesforce['combined_text'].tolist())

# Process the text in chunks to avoid length issues
all_tokens = process_text_in_chunks(all_text, nlp)

# Continue with the rest of your analysis using all_tokens
word_freqs = Counter(all_tokens)

# Define a threshold for rare words (e.g., words that occur fewer than 5 times)
rare_threshold = 5

# Lists to store rare words categorized as dates and text
rare_words_date = []
rare_words_text = []

# Preprocess the combined text, separating rare words
df_salesforce['processed_text'] = df_salesforce['combined_text'].apply(lambda doc: preprocess(doc, word_freqs, rare_threshold, rare_words_date, rare_words_text))

# Token count for each row
df_salesforce['token_count'] = df_salesforce['processed_text'].apply(len)

# Save rare words to an Excel file with separate columns for date and text rare words
rare_words_df = pd.DataFrame({
    'Rare Date Words': pd.Series(rare_words_date),
    'Rare Text Words': pd.Series(rare_words_text)
})
rare_words_df.to_excel('rare_words.xlsx', index=False)

# Print summary count of rare words
print(f"\nSummary of Rare Words:")
print(f"Number of rare words categorized as dates: {len(rare_words_date)}")
print(f"Number of rare words categorized as text: {len(rare_words_text)}")
print("\nRare words have been saved to 'rare_words.xlsx'.")

# ---- DEALS OVERVIEW ----

print("\n==== DEALS OVERVIEW ====\n")

# 1. Total Count of Deals
total_deal_count = len(df_salesforce)
print(f"Total Count of Deals: {total_deal_count}")

# 2. Count of RST Deals
rst_deal_count = len(df_salesforce[df_salesforce['account_eci'] == 'RST'])
print(f"Count of RST Deals: {rst_deal_count}")

# 3. Count of Non-RST Deals
non_rst_deal_count = len(df_salesforce[df_salesforce['account_eci'] != 'RST'])
print(f"Count of Non-RST Deals: {non_rst_deal_count}")

# 4. Count of Deals by Win/Loss Reason
win_loss_reason_counts = df_salesforce['win_loss_reason_text'].value_counts()
print("\nCount of Deals by Win/Loss Reason:")
print(win_loss_reason_counts)

# Filter Non-RST data (without token length constraints)
non_rst_data = df_salesforce[df_salesforce['account_eci'] != 'RST']

# Preprocessing the text for analysis (Non-RST Data)
processed_texts = non_rst_data['processed_text'].tolist()

# Building bi-gram and tri-gram models
bigram = gensim.models.Phrases(processed_texts, min_count=10)
trigram = gensim.models.Phrases(bigram[processed_texts], min_count=10)

# Add bigrams and trigrams to the processed texts
for idx in range(len(processed_texts)):
    for token in trigram[bigram[processed_texts[idx]]]:
        if '_' in token:
            processed_texts[idx].append(token)

# Recreate processed text back into the dataframe
non_rst_data['processed_text'] = processed_texts

# Extracting N-Gram frequencies for unigrams, bigrams, trigrams, and four-grams for Non-RST Data
all_tokens_non_rst = [token for sublist in non_rst_data['processed_text'] for token in sublist]

# Unigrams, Bigrams, Trigrams, Four-Grams Frequency Analysis for Non-RST Data
unigram_freq = get_ngram_freqs(all_tokens_non_rst, n=1)
bigram_freq = get_ngram_freqs(all_tokens_non_rst, n=2)
trigram_freq = get_ngram_freqs(all_tokens_non_rst, n=3)
fourgram_freq = get_ngram_freqs(all_tokens_non_rst, n=4)

# Display and plot the n-grams for Non-RST Data
display_ngrams(unigram_freq, 'Unigrams (Non-RST)')
display_ngrams(bigram_freq, 'Bigrams (Non-RST)')
display_ngrams(trigram_freq, 'Trigrams (Non-RST)')
display_ngrams(fourgram_freq, 'Four-Grams (Non-RST)')

print("\nN-Gram frequency analysis for Non-RST data completed.")
