import pandas as pd
import re
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
import torch
from tqdm import tqdm
import time

# Model and Tokenizer Setup
model_path = "mistral/Mistral-7B-Instruct-v0.2"
tokenizer = AutoTokenizer.from_pretrained(
    model_path,
    padding_side="left",
    use_fast=True,
    add_bos_token=True,
    add_eos_token=True
)

tokenizer.pad_token = tokenizer.eos_token

# 4-bit quantization for cost optimization
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

model = AutoModelForCausalLM.from_pretrained(
    model_path,
    quantization_config=bnb_config,
    torch_dtype=torch.bfloat16,
    device_map="auto",
    trust_remote_code=True
)

model.config.use_cache = False
model.config.pretraining_tp = 1
model.gradient_checkpointing_enable()

# Load Data
df = pd.read_csv('Data/closed_won_final.csv')
df = df.head(1000)  # Limiting to 1000 rows for demonstration
total_rows = len(df)

# Function to truncate text to a maximum of 4096 tokens
def truncate_to_4096_tokens(text):
    tokens = text.split()
    if len(tokens) > 4096:
        tokens = tokens[:4096]
    return ' '.join(tokens)

# Apply the function to the relevant text column in the DataFrame
df['preprocess_text_truncated'] = df['combined_text'].apply(truncate_to_4096_tokens)

# Dictionary for caching results to avoid redundant computations
classification_cache = {}

# Function to chunk text based on token length
def chunk_text(text, max_length=128):
    tokens = tokenizer.encode(text, truncation=False)
    # Split tokens into chunks of size `max_length`
    return [tokens[i:i+max_length] for i in range(0, len(tokens), max_length)]

# Function to classify texts with chunking and caching
def classify_texts(df_batch):
    assigned_categories = []
    sub_categories = []
    specific_processes = []
    account_opening_flags = []

    updated_prompt_template = """
    You are an AI tasked with reading a text related to financial and business processes. First, determine if this case is related to **Account Opening**.

    - If it mentions actual account creation (e.g., "Account Opening", "New Account", "Account Setup"), assign it as **Account Opening** and include any specific location mentioned.
    - If it mentions **interest** or **inquiry** without actual opening (e.g., "Client interested", "Inquiry about account"), assign the category as **Not Account Opening** and use "Client Interest" as the sub-category.
    - If it involves modifications to an **existing account**, assign it as **Already Existing Account**.

    After categorizing the text into one of these high-level categories, you must also provide a sub-category and describe the specific process in 2-3 words that is happening in the text.

    If none of the categories fit, assign **Others** for both High-Level and Sub-Category.

    Text: "{input_text}"

    ### Response Format:
    1. High-Level Category: [Account Opening / Already Existing Account / Not Account Opening / Others]
    2. Sub-Category: [Assigned Sub-Category with Location (if applicable) or "Others"]
    3. Specific Process: [2-3 word process description]
    """

    batch_texts = df_batch["preprocess_text_truncated"].tolist()  # Using the truncated text

    for text in batch_texts:
        if text in classification_cache:
            # Use cached result if available
            high_level_category, sub_category, process = classification_cache[text]
        else:
            prompt = updated_prompt_template.format(input_text=text)

            # Chunk the input text if it's too long
            text_chunks = chunk_text(prompt, max_length=128)

            # Variables to combine the results from multiple chunks
            combined_high_level_category = []
            combined_sub_category = []
            combined_specific_process = []

            for chunk in text_chunks:
                input_tokens = torch.tensor([chunk]).to(model.device)

                # Generate output with a max limit of 200 tokens
                output = model.generate(input_tokens, max_new_tokens=200, pad_token_id=tokenizer.eos_token_id)

                # Decode generated output for the chunk
                generated_text = tokenizer.decode(output[0], skip_special_tokens=True)

                # Extract the categories, sub-categories, and process from the chunk
                high_level_category, sub_category, process = extract_classification(generated_text)

                combined_high_level_category.append(high_level_category)
                combined_sub_category.append(sub_category)
                combined_specific_process.append(process)

            # Aggregate chunk results (you can customize how to combine the chunk results)
            final_high_level_category = max(set(combined_high_level_category), key=combined_high_level_category.count)
            final_sub_category = max(set(combined_sub_category), key=combined_sub_category.count)
            final_specific_process = ' '.join(combined_specific_process)

            # Cache the result
            classification_cache[text] = (final_high_level_category, final_sub_category, final_specific_process)

        assigned_categories.append(final_high_level_category)
        sub_categories.append(final_sub_category)
        specific_processes.append(final_specific_process)

        if final_high_level_category == "Account Opening":
            account_opening_flags.append("Yes")
        else:
            account_opening_flags.append("No")

    df_batch['assigned_category'] = assigned_categories
    df_batch['sub_category'] = sub_categories
    df_batch['specific_process'] = specific_processes
    df_batch['is_related_to_account_opening'] = account_opening_flags

    return df_batch

# Function to extract categories, sub-categories, and specific processes
def extract_classification(text):
    lines = text.split("\n")
    high_level_category = "Not available"
    sub_category = "Not available"
    specific_process = "Not available"

    for line in lines:
        line = line.strip()
        if line.startswith("1. High-Level Category:"):
            high_level_category = line.split(": ", 1)[1].strip() if ": " in line else "Not available"
        elif line.startswith("2. Sub-Category:"):
            sub_category = line.split(": ", 1)[1].strip() if ": " in line else "Not available"
        elif line.startswith("3. Specific Process:"):
            specific_process = line.split(": ", 1)[1].strip() if ": " in line else "Not available"

    return high_level_category, sub_category, specific_process

# Process data in batches
def process_batches(df, batch_size):
    all_results = pd.DataFrame()

    start_time = time.time()

    for start in tqdm(range(0, total_rows, batch_size), desc="Processing Batches"):
        end = min(start + batch_size, total_rows)

        df_batch = df.iloc[start:end].copy()

        # Run classification task for the batch
        df_batch = classify_texts(df_batch)

        # Concatenate results into the full dataframe
        all_results = pd.concat([all_results, df_batch], ignore_index=True)

    # Measure total time taken
    end_time = time.time()
    elapsed_time = end_time - start_time
    print(f"Time taken to process {len(df)} records: {elapsed_time:.2f} seconds")

    # Save final combined results to a CSV file
    all_results.to_csv('final_classification_output.csv', index=False)
    print("Final results saved to final_classification_output.csv.")

# Running the batch process with batch size of 100
process_batches(df, batch_size=100)

# Display the first few rows of the final output
df_final = pd.read_csv('final_classification_output.csv')
print(df_final.head())
