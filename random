import pandas as pd
import spacy
from spacy.lang.en.stop_words import STOP_WORDS
import gensim
import matplotlib.pyplot as plt
import seaborn as sns
import re
import pyLDAvis.gensim_models as gensimvis
import pyLDAvis

# Load Spacy model (without custom stopwords)
nlp = spacy.load('en_core_web_md', disable=['parser', 'ner'])

# Function to preprocess text
def preprocess(document):
    document = document.lower()
    document = re.sub(r'[^a-zA-Z\s]', '', document)
    document = re.sub(r'\s+', ' ', document).strip()
    doc = nlp(document)
    words = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct and len(token.lemma_) > 2]
    return words

# Function to analyze n-gram frequencies and plot them
def analyze_ngrams(df_subset, title):
    all_tokens = [token for sublist in df_subset['processed_text'] for token in sublist]
    total_token_count = len(all_tokens)
    print(f"Total Token Count ({title}): {total_token_count}")

    # Get the frequency of n-grams
    ngram_freq = Counter(all_tokens)

    # Select top 20 most frequent n-grams
    top_20_ngrams = ngram_freq.most_common(20)

    # Create DataFrame for visualization
    ngram_df = pd.DataFrame(top_20_ngrams, columns=['ngram', 'frequency'])

    # Plot the top 20 n-grams
    plt.figure(figsize=(12, 8))
    sns.barplot(x='ngram', y='frequency', data=ngram_df, palette='coolwarm')
    plt.xticks(rotation=45, ha='right', fontsize=12)
    plt.yticks(fontsize=12)
    plt.title(f'Top 20 Most Frequent N-grams ({title})', fontsize=16, weight='bold')
    plt.xlabel('N-gram', fontsize=14, weight='bold')
    plt.ylabel('Frequency', fontsize=14, weight='bold')

    # Add data labels for each bar
    for index, value in enumerate(ngram_df['frequency']):
        plt.text(index, value + 500, str(value), ha='center', fontsize=12)

    plt.tight_layout()
    plt.show()

# Function to perform topic modeling and save PyLDAvis visualizations
def perform_topic_modeling(df_subset, title, html_filename):
    # Preprocessing text columns for topic modeling
    df_subset['processed_text'] = df_subset['combined_text'].map(preprocess)
    processed_texts = df_subset['processed_text'].tolist()

    # Building bi-gram and tri-gram models
    bigram = gensim.models.Phrases(processed_texts, min_count=10)
    trigram = gensim.models.Phrases(bigram[processed_texts], min_count=10)

    # Add bigrams and trigrams to the processed texts
    for idx in range(len(processed_texts)):
        for token in trigram[bigram[processed_texts[idx]]]:
            if '_' in token:
                processed_texts[idx].append(token)

    # Creating dictionary
    dictionary = gensim.corpora.Dictionary(processed_texts)

    # Filtering dictionary
    dictionary.filter_extremes(no_below=int(len(processed_texts) * 0.01), no_above=0.5)

    # Creating the corpus
    corpus = [dictionary.doc2bow(text) for text in processed_texts]

    # Run topic modeling
    optimal_model = gensim.models.LdaMulticore(
        corpus=corpus,
        num_topics=5,  # Number of topics (this can be tuned based on coherence values)
        id2word=dictionary,
        passes=20,
        alpha='symmetric',
        eta='symmetric',
        workers=2,
        random_state=5
    )

    # Visualize topics with PyLDAvis
    vis = gensimvis.prepare(optimal_model, corpus, dictionary)

    # Save PyLDAvis visualization to HTML
    pyLDAvis.save_html(vis, html_filename)
    return vis

# Function to count and report total deals and their segmentation
def report_deals_info(df_salesforce):
    total_deal_count = len(df_salesforce)
    rst_deal_count = len(df_salesforce[df_salesforce['account_eci'] == 'RST'])
    non_rst_deal_count = len(df_salesforce[df_salesforce['account_eci'] != 'RST'])
    token_len_le_3_count = len(df_salesforce[df_salesforce['token_count'] <= 3])
    token_len_ge_4_count = len(df_salesforce[df_salesforce['token_count'] >= 4])

    print(f"Total Deal Count: {total_deal_count}")
    print(f"RST Deal Count: {rst_deal_count}")
    print(f"Non-RST Deal Count: {non_rst_deal_count}")
    print(f"Deals with Token Length <= 3: {token_len_le_3_count}")
    print(f"Deals with Token Length >= 4: {token_len_ge_4_count}")

# Read data from Salesforce analytics
df_salesforce = pd.read_excel('Data/exclude_won_rst.xlsx')

# Concatenate text columns into a single string variable
df_salesforce['combined_text'] = df_salesforce[['record_comment_text', 
                                                'executive_summary_text', 
                                                'win_loss_reason_text', 
                                                'win_loss_comments_text']].apply(lambda x: ' '.join(x.dropna()), axis=1)

# Preprocess and count word tokens for each row
df_salesforce['processed_text'] = df_salesforce['combined_text'].apply(preprocess)
df_salesforce['token_count'] = df_salesforce['processed_text'].apply(len)

# Report deal counts
report_deals_info(df_salesforce)

# 1) Analysis for RST Data
rst_data = df_salesforce[df_salesforce['account_eci'] == 'RST']
analyze_ngrams(rst_data, 'RST Data')
rst_vis = perform_topic_modeling(rst_data, 'RST Data', 'rst_topic_model.html')

# 2) Analysis for Non-RST Data
non_rst_data = df_salesforce[df_salesforce['account_eci'] != 'RST']
analyze_ngrams(non_rst_data, 'Non-RST Data')
non_rst_vis = perform_topic_modeling(non_rst_data, 'Non-RST Data', 'non_rst_topic_model.html')

# 3) Analysis for token length <= 3
token_len_le_3_data = df_salesforce[df_salesforce['token_count'] <= 3]
analyze_ngrams(token_len_le_3_data, 'Token Length <= 3')
le_3_vis = perform_topic_modeling(token_len_le_3_data, 'Token Length <= 3', 'token_len_le_3_topic_model.html')

# 4) Analysis for token length >= 4
token_len_ge_4_data = df_salesforce[df_salesforce['token_count'] >= 4]
analyze_ngrams(token_len_ge_4_data, 'Token Length >= 4')
ge_4_vis = perform_topic_modeling(token_len_ge_4_data, 'Token Length >= 4', 'token_len_ge_4_topic_model.html')

# After all visualizations are saved, combine them in a single HTML dashboard
with open('combined_pylda_dashboard.html', 'w') as f:
    f.write("""
    <!DOCTYPE html>
    <html>
    <head>
        <title>PyLDAvis Combined Dashboard</title>
        <style>
            iframe {
                width: 45%;
                height: 600px;
                margin: 20px;
            }
            body {
                display: flex;
                flex-wrap: wrap;
                justify-content: space-around;
            }
        </style>
    </head>
    <body>
        <h1>Combined PyLDAvis Dashboard</h1>

        <iframe src="rst_topic_model.html" title="RST Topic Model"></iframe>
        <iframe src="non_rst_topic_model.html" title="Non-RST Topic Model"></iframe>
        <iframe src="token_len_le_3_topic_model.html" title="Token Length <= 3 Topic Model"></iframe>
        <iframe src="token_len_ge_4_topic_model.html" title="Token Length >= 4 Topic Model"></iframe>

    </body>
    </html>
    """)
