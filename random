# Load Data
df = pd.read_csv('closed_won_final.csv')  # Adjust the path to your data
df = df.head(1)  # Limiting to 1000 rows for demonstration
total_rows = len(df)

# Function to chunk text based on token length
def chunk_text(text, max_length=128):
    tokens = tokenizer.encode(text, truncation=False)
    # Split tokens into chunks of size max_length
    return [tokens[i:i+max_length] for i in range(0, len(tokens), max_length)]

def classify_texts(df_batch):
    assigned_categories = []
    sub_categories = []
    specific_processes = []
    account_opening_flags = []

    # Updated prompt with correct structure from your images
    prompt_template = """
    You are an AI tasked with classifying financial and business process texts. For each text, understand the context and classify it into one of the following main categories, or select "Other" if none of the categories fit. Only use "Not available" if the text is unclear or has insufficient details.

    **Main Categories:**
    1. **Product Capability**: Services like ACH Direct Send, BAI Reporting, Treasury Services, Financial Services
       - Subcategories: ACH Direct Send, BAI Reporting, Treasury Services, Cash Management Solutions, Reporting Enhancements, Other Product Capability, Financial Services (e.g., Transaction Processing, Treasury Solutions, SWIFT Connectivity)

    2. **Client-Specific Solutions**: Custom solutions like account setup, currency accounts, or account modifications
       - Subcategories: New Account Opening, Account Setup, Currency Accounts, Customized Solutions, Account Transfer, Client Onboarding, Client Support, Client Mandate, Client Deposit, Client Request, Client Interest

    3. **Market Disruption**: Instances where clients switch from competitors due to market shifts (e.g., clients switching from Wells Fargo)
       - Subcategories: Competitor Switch, Market Disruption, Stability Offering

    4. **Relationship and Wallet Share**: Expanding client relationships or wallet share through new services
       - Subcategories: Relationship Expansion, Additional Services, Increased Wallet Share

    5. **Geographical Expansion**: Opening accounts in new regions or international entities
       - Subcategories: New Region Account, International Expansion, Global Growth

    6. **Competitor Comparison**: Comparing JPM services against competitors, highlighting advantages
       - Subcategories: Pricing Advantages, Service Comparison, Competitor Analysis

    7. **Client Onboarding and Implementation**: Streamlined onboarding or delivery of services
       - Subcategories: Onboarding Efficiency, Smooth Transitions, Delivery Effectiveness

    8. **Other Processes**: Processes like regulatory compliance, tax reporting, or risk management not covered by the above categories
       - Subcategories: Regulatory Compliance, Tax Reporting, Risk Management

    9. **Not available**: Use this only if the text is incomplete or lacks sufficient detail to classify.

    For each text, provide the main category, the sub-category, and a 2-3 word description based on the context.

    Text: "{input_text}"

    ### Response Format:
    1. Main Category: [Product Capability / Client-Specific Solutions / Market Disruption / Relationship and Wallet Share / Geographical Expansion / Competitor Comparison / Client Onboarding and Implementation / Other Processes / Not available]
    2. Sub-Category: [Assigned Sub-Category or "Other"]
    3. Specific Process: [2-3 word description]
    """

    for idx, row in df_batch.iterrows():
        input_text = row["combined_text"]
        # **Chunk the input text**
        text_chunks = chunk_text(input_text)  # Use input_text, not the prompt
        
        # Variables to collect results from chunks
        combined_category = []
        combined_sub_category = []
        combined_process = []

        for chunk in text_chunks:
            # Convert chunk back to text
            chunk_text_decoded = tokenizer.decode(chunk)

            # Create prompt for each chunk
            prompt = prompt_template.format(input_text=chunk_text_decoded)

            # Tokenization and model inference for each chunk
            input_tokens = tokenizer(prompt, return_tensors="pt", truncation=True, padding=True, max_length=512).to(model.device)
            output = model.generate(
                input_tokens['input_ids'], 
                max_new_tokens=200, 
                pad_token_id=tokenizer.eos_token_id, 
                temperature=0.7  # Adjusting the temperature
            )

            # Decode and extract results
            generated_text = tokenizer.decode(output[0], skip_special_tokens=True)

            # Debugging: Check what the model has generated
            print(f"Generated text for chunk in row {idx}: {generated_text}")

            # Extract classification results from each chunk
            category, sub_category, process = extract_classification(generated_text)

            # Collect results from each chunk
            combined_category.append(category)
            combined_sub_category.append(sub_category)
            combined_process.append(process)

        # **Aggregate chunk results**
        # For simplicity, you can take the most frequent category and process across chunks.
        final_category = max(set(combined_category), key=combined_category.count)
        final_sub_category = max(set(combined_sub_category), key=combined_sub_category.count)
        final_process = " ".join(combined_process)  # Combine processes from chunks

        # Save the classification results for each row
        assigned_categories.append(final_category)
        sub_categories.append(final_sub_category)
        specific_processes.append(final_process)

        # Set flag for account opening
        if final_category == "Account Management" and "New Account" in final_sub_category:
            account_opening_flags.append("Yes")
        else:
            account_opening_flags.append("No")

    df_batch['assigned_category'] = assigned_categories
    df_batch['sub_category'] = sub_categories
    df_batch['specific_process'] = specific_processes
    df_batch['is_related_to_account_opening'] = account_opening_flags

    return df_batch


# Function to extract categories, sub-categories, and specific processes
def extract_classification(text):
    # Initialize default values
    high_level_category = "Not available"
    sub_category = "Not available"
    specific_process = "Not available"

    # Log full generated text for debugging
    print(f"Generated text:\n{text}\n")

    # Normalize text to handle extra spaces or inconsistencies
    text = text.replace("**", "").strip()  # Remove any Markdown formatting (**bold**)

    # Split the generated text by newlines
    lines = text.split("\n")
    
    # Iterate over each line to extract the relevant fields
    for line in lines:
        line = line.strip()  # Remove leading/trailing whitespace
        
        # Handle potential cases where the text might not follow the exact format
        if "Main Category:" in line:
            high_level_category = line.split("Main Category:")[1].strip() if "Main Category:" in line else high_level_category
        elif "Sub-Category:" in line:
            sub_category = line.split("Sub-Category:")[1].strip() if "Sub-Category:" in line else sub_category
        elif "Specific Process:" in line:
            specific_process = line.split("Specific Process:")[1].strip() if "Specific Process:" in line else specific_process
    
    # Log the extracted categories for debugging
    print(f"Extracted Category: {high_level_category}, Sub-Category: {sub_category}, Specific Process: {specific_process}")
    
    return high_level_category, sub_category, specific_process

# Process data in batches
def process_batches(df, batch_size):
    all_results = pd.DataFrame()

    start_time = time.time()

    for start in tqdm(range(0, total_rows, batch_size), desc="Processing Batches"):
        end = min(start + batch_size, total_rows)

        df_batch = df.iloc[start:end].copy()

        # Run classification task for the batch
        df_batch = classify_texts(df_batch)

        # Concatenate results into the full dataframe
        all_results = pd.concat([all_results, df_batch], ignore_index=True)

    # Measure total time taken
    end_time = time.time()
    elapsed_time = end_time - start_time
    print(f"Time taken to process {len(df)} records: {elapsed_time:.2f} seconds")

    # Save final combined results to a CSV file
    all_results.to_csv('final_classification_output.csv', index=False)
    print("Final results saved to final_classification_output.csv.")

# Running the batch process with batch size of 100
process_batches(df, batch_size=1)

# Display the first few rows of the final output
df_final = pd.read_csv('final_classification_output.csv')
print("First few rows of the final saved output:")
print(df_final.head())
