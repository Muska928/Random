1. 4-bit Quantization:
Explanation: The model is loaded using 4-bit quantization, which reduces the precision of computations to save memory and speed up inference.
Implementation:
python
Copy code
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,  # Enabling 4-bit quantization
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",  # Using "nf4" as quantization type
    bnb_4bit_compute_dtype=torch.bfloat16  # Further optimizing memory with bfloat16
)
Benefit: Reduces memory usage and computational load by using lower precision, while still maintaining sufficient model accuracy.
2. Device Mapping:
Explanation: The model is automatically mapped to the most efficient device (CPU or GPU), based on availability. This ensures optimal hardware utilization.
Implementation:
python
Copy code
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    quantization_config=bnb_config,
    torch_dtype=torch.bfloat16,  # Optimizing precision
    device_map="auto",  # Automatically maps to available hardware (GPU if available)
    trust_remote_code=True
)
Benefit: Allows the model to run on GPU when available, leading to faster processing with minimal manual configuration. If GPU is unavailable, it defaults to CPU.
3. Disabling Cache:
Explanation: The cache is disabled to reduce memory consumption during model inference, especially when working with large inputs.
Implementation:
python
Copy code
model.config.use_cache = False  # Disable caching to save memory
Benefit: Reduces memory overhead during token generation by disabling cache, particularly useful for long sequences or batch processing.
4. Gradient Checkpointing:
Explanation: Gradient checkpointing is enabled to trade off computation for memory by storing fewer intermediate activations, saving memory during backpropagation.
Implementation:
python
Copy code
model.gradient_checkpointing_enable()  # Enable gradient checkpointing
Benefit: Saves memory during training or inference by not storing intermediate states, reducing the overall memory consumption in exchange for slightly longer runtime.
5. Chunking of Input Text:
Explanation: Long input texts are split into smaller manageable chunks, ensuring that the input does not exceed the model's token limit, reducing the memory footprint of the input during processing.
Implementation:
python
Copy code
def chunk_text(text, max_length=128):
    tokens = tokenizer.encode(text, truncation=False)
    return [tokens[i:i+max_length] for i in range(0, len(tokens), max_length)]
Benefit: Prevents memory overload by breaking down large texts into smaller chunks that fit within the model's maximum token length limit. This also helps avoid out-of-memory errors during inference.
6. Batch Processing:
Explanation: The data is processed in batches rather than loading everything into memory at once.
Implementation:
python
Copy code
def process_batches(df, batch_size):
    for start in tqdm(range(0, total_rows, batch_size), desc="Processing Batches"):
        end = min(start + batch_size, total_rows)
        df_batch = df.iloc[start:end].copy()
Benefit: Reduces memory consumption by processing smaller portions of data at a time instead of loading the entire dataset into memory. This allows for more efficient resource utilization, especially when dealing with large datasets.
Summary of Memory Optimization Techniques:
4-bit quantization: Reduces memory usage while maintaining good performance.
Device mapping (GPU/CPU): Automatically utilizes the best available hardware, improving memory efficiency.
Disabling cache: Saves memory during inference by not caching intermediate tokens.
Gradient checkpointing: Further memory saving by reducing intermediate storage during model processing.
Chunking: Splits long texts into manageable pieces, preventing memory overload.
Batch processing: Handles data in smaller chunks, reducing overall memory footprint during processing.
