import pandas as pd
import re
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from tqdm import tqdm
import torch
from transformers import BitsAndBytesConfig
import matplotlib.pyplot as plt

# Load data from Salesforce analytics
df_salesforce = pd.read_excel('/mnt/data/Salesforce_Deals_Text_Analytics.xlsx')

# Concatenate relevant text columns into a single string variable
df_salesforce['combined_text'] = df_salesforce[
    ['product_family_name', 'product_group_name', 'description_text', 'win_loss_reason_text', 'win_loss_comments_text']
].apply(lambda x: ' '.join(x.dropna()), axis=1)

# Preprocess the text
def preprocess_text(text):
    text = text.lower()  # Convert text to lowercase
    text = re.sub(r'[^a-zA-Z\s]', '', text)  # Remove special characters and numbers
    text = re.sub(r'\s+', ' ', text).strip()  # Remove extra whitespace
    return text

# Apply preprocessing
df_salesforce['combined_text'] = df_salesforce['combined_text'].apply(preprocess_text)

# Sample 20 records for testing
df_sample = df_salesforce.sample(n=20, random_state=42)

# Define prompt template for theme extraction
one_shot_template_theme = """
You are an information detector for a Fintech company. Your job is to analyze the provided product descriptions and identify the key reasons or themes associated with each deal. Categorize the main reasons or themes under the appropriate headings.

- **Product Group:** Identify the main product group mentioned.
- **Key Features:** List up to 5 specific features or aspects highlighted in the description.
- **Closed Deal Reasons / Themes:** Summarize the reasons for closing the deal, focusing on key themes such as pricing, product features, service quality, etc.

Here is the text:
{input_text}

Provide your response in the following format:
Product Group: [Product Group]
Key Features: [Key Features]
Closed Deal Reasons / Themes: [Closed Deal Reasons / Themes]
"""

# Maximum length of input tokens
max_input_length = 200

# Batch processing size
batch_size = 10

# Define the parse_response function to extract structured information without regular expressions
def parse_response(response):
    lines = response.split('\n')
    product_group = ""
    key_features = []
    closed_deal_reasons = ""

    for line in lines:
        if line.startswith("Product Group:"):
            product_group = line[len("Product Group:"):].strip()[1:-1]  # Remove leading and trailing brackets
        elif line.startswith("Key Features:"):
            features_str = line[len("Key Features:"):].strip()[1:-1]  # Remove leading and trailing brackets
            key_features = features_str.split(', ')
        elif line.startswith("Closed Deal Reasons / Themes:"):
            closed_deal_reasons = line[len("Closed Deal Reasons / Themes:"):].strip()[1:-1]  # Remove leading and trailing brackets

    return {
        "Product Group": product_group,
        "Key Features": key_features,
        "Closed Deal Reasons / Themes": closed_deal_reasons
    }

# Function to process and parse results
def process_and_parse(df_sample, one_shot_template, batch_size=10):
    results = []
    for i in tqdm(range(0, len(df_sample), batch_size), desc="Processing"):
        batch_texts = df_sample['combined_text'].iloc[i:i+batch_size].tolist()
        truncated_texts = [tokenizer.decode(tokenizer.encode(text, max_length=max_input_length, truncation=True), skip_special_tokens=True) for text in batch_texts]
        prompts = [one_shot_template.format(input_text=text) for text in truncated_texts]

        batch_responses = generation_pipeline(prompts, max_new_tokens=50, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)
        for response in batch_responses:
            generated_text = response[0]['generated_text'] if isinstance(response, list) and 'generated_text' in response[0] else response['generated_text']
            results.append(generated_text)
    
    df_sample['final_response'] = results
    df_sample['all_information'] = df_sample['final_response'].apply(parse_response)
    return df_sample

# Process and parse the data
df_processed = process_and_parse(df_sample, one_shot_template_theme)
df_processed.to_csv('/mnt/data/Theme_Analysis.csv', index=False)

# Extract the closed deal reasons/themes for charting
df_processed['Closed Deal Reasons / Themes'] = df_processed['all_information'].apply(lambda x: x['Closed Deal Reasons / Themes'])

# Count the occurrences of each theme
theme_counts = df_processed['Closed Deal Reasons / Themes'].value_counts()

# Plot the bar chart
plt.figure(figsize=(10, 6))
theme_counts.plot(kind='bar')
plt.title('Distribution of Closed Deal Reasons / Themes')
plt.xlabel('Themes')
plt.ylabel('Number of Deals')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()
