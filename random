def classify_texts(df_batch):
    assigned_categories = []
    sub_categories = []
    specific_processes = []
    account_opening_flags = []

    # Updated prompt with categories from the image
    prompt_template = """
    You are an advanced AI tasked with classifying business process and financial texts. Follow the instructions carefully for each text provided.

    ### Step 1: Understand the Text
    - Comprehend the content and context of the text, focusing on business processes.

    ### Step 2: Categorize the Text
    - Assign the text to one of the **Main Categories** below. If none of the categories fit, choose **"Other"**.

    #### Main Categories:
    1. **Product Capability:** Describes strong product offerings (e.g., ACH Direct Send, BAI Reporting, Treasury Services).
       - **Subcategories:** ACH Direct Send, BAI Reporting, Treasury Services, Other Product Capability.

    2. **Client-Specific Solutions:** Describes custom solutions for specific client needs (e.g., account setup, currency accounts).
       - **Subcategories:** Account Setup, Currency Accounts, Customized Solutions.

    3. **Market Disruption:** Relates to market changes leading to client movement or service adoption.
       - **Subcategories:** Competitor Switch, Market Disruption, Stability Offering.

    4. **Relationship and Wallet Share:** Expands client relationships or increases client engagement.
       - **Subcategories:** Relationship Expansion, Additional Services, Increased Wallet Share.

    5. **Geographical Expansion:** Describes growth into new regions or international markets.
       - **Subcategories:** New Region Account, International Expansion, Global Growth.

    6. **Competitor Comparison:** Compares company offerings with competitors, usually related to pricing or service.
       - **Subcategories:** Pricing Advantages, Service Comparison, Competitor Analysis.

    7. **Client Onboarding and Implementation:** Describes the process of onboarding new clients or delivering solutions.
       - **Subcategories:** Onboarding Efficiency, Smooth Transitions, Delivery Effectiveness.

    8. **Other:** If no category applies, classify the text under "Other" and provide a relevant sub-category or explanation.

    ### Step 3: Provide a Short Process Description
    - Summarize the key business process in **2-3 words**.

    ### Input Text:
    "{input_text}"

    ### Response Format:
    1. **Main Category:** [Product Capability / Client-Specific Solutions / Market Disruption / Relationship and Wallet Share / Geographical Expansion / Competitor Comparison / Client Onboarding and Implementation / Other]
    2. **Sub-Category:** [Relevant Sub-Category or "Other"]
    3. **Specific Process:** [2-3 word summary]
    """

    for idx, row in df_batch.iterrows():
        input_text = row["combined_text"]
        # **Chunk the input text**
        text_chunks = chunk_text(input_text)  # Use input_text, not the prompt
        
        # Variables to collect results from chunks
        combined_category = []
        combined_sub_category = []
        combined_process = []

        for chunk in text_chunks:
            # Convert chunk back to text
            chunk_text_decoded = tokenizer.decode(chunk)

            # Create prompt for each chunk
            prompt = prompt_template.format(input_text=chunk_text_decoded)

            # Tokenization and model inference for each chunk
            input_tokens = tokenizer(prompt, return_tensors="pt", truncation=True, padding=True, max_length=512).to(model.device)
            output = model.generate(
                input_tokens['input_ids'], 
                max_new_tokens=200, 
                pad_token_id=tokenizer.eos_token_id, 
                temperature=0.7  # Adjusting the temperature
            )

            # Decode and extract results
            generated_text = tokenizer.decode(output[0], skip_special_tokens=True)

            # Debugging: Check what the model has generated
            print(f"Generated text for chunk in row {idx}: {generated_text}")

            # Extract classification results from each chunk
            category, sub_category, process = extract_classification(generated_text)

            # Collect results from each chunk
            combined_category.append(category)
            combined_sub_category.append(sub_category)
            combined_process.append(process)

        # **Aggregate chunk results**
        final_category = max(set(combined_category), key=combined_category.count)
        final_sub_category = max(set(combined_sub_category), key=combined_sub_category.count)
        final_process = " ".join(combined_process)  # Combine processes from chunks

        # Save the classification results for each row
        assigned_categories.append(final_category)
        sub_categories.append(final_sub_category)
        specific_processes.append(final_process)

        # Set flag for account opening
        if final_category == "Client Onboarding and Implementation" and "New Account" in final_sub_category:
            account_opening_flags.append("Yes")
        else:
            account_opening_flags.append("No")

    df_batch['assigned_category'] = assigned_categories
    df_batch['sub_category'] = sub_categories
    df_batch['specific_process'] = specific_processes
    df_batch['is_related_to_account_opening'] = account_opening_flags

    return df_batch

# Function to extract categories, sub-categories, and specific processes
def extract_classification(text):
    # Initialize default values
    high_level_category = "Not available"
    sub_category = "Not available"
    specific_process = "Not available"

    # Normalize text to handle extra spaces or inconsistencies
    text = text.replace("**", "").strip()  # Remove any Markdown formatting (**bold**)

    # Split the generated text by newlines
    lines = text.split("\n")
    
    # Iterate over each line to extract the relevant fields
    for line in lines:
        line = line.strip()  # Remove leading/trailing whitespace
        
        # Handle potential cases where the text might not follow the exact format
        if "Main Category:" in line:
            high_level_category = line.split("Main Category:")[1].strip() 
        elif "Sub-Category:" in line:
            sub_category = line.split("Sub-Category:")[1].strip() 
        elif "Specific Process:" in line:
            specific_process = line.split("Specific Process:")[1].strip() 
    
    return high_level_category, sub_category, specific_process

# Function to chunk text based on token length
def chunk_text(text, max_length=128):
    tokens = tokenizer.encode(text, truncation=False)
    # Split tokens into chunks of size max_length
    return [tokens[i:i + max_length] for i in range(0, len(tokens), max_length)]
