
# Load Data
df = pd.read_csv('closed_won_final.csv')
df = df.head(1)  # Limiting to 1000 rows for demonstration
total_rows = len(df)

# Improved Prompt Template with Examples
prompt_template = """
You are an AI tasked with classifying financial and business process texts. Your goal is to classify each text into one of the high-level categories and relevant subcategories based on the context. If none of the categories fit, assign it to "Other" with an appropriate sub-category. Below are examples for each category to help you better understand the classification.

**High-Level Categories and Subcategories with Examples:**

1. **Product Capability**: Classify here if the text mentions product offerings or features related to banking, treasury services, payments, or product performance.
   - **Subcategories**:
     - ACH Direct Send (e.g., "Client requested an ACH Direct Send setup.")
     - BAI Reporting (e.g., "BAI Reporting for treasury services requested by client.")
     - Treasury Services (e.g., "Comprehensive treasury services provided.")
     - ACH Capabilities (e.g., "Client is utilizing ACH for transactions.")
     - EFT Processing (e.g., "EFT processing is enabled for client payments.")

2. **Client-Specific Solutions**: Classify here if the text discusses solutions tailored to specific clients, such as customization or specific account setups.
   - **Subcategories**:
     - Customization (e.g., "Tailored reporting solution delivered for client’s unique needs.")
     - Account Setup (e.g., "New international account setup for client.")
     - Currency Accounts (e.g., "Multi-currency account established for the client.")
     - Tailored Solutions (e.g., "Customized treasury solution provided for specific client requests.")

3. **Market Disruption**: Classify here if the text is about market fluctuations, competitor actions, or general instability in the market.
   - **Subcategories**:
     - Market Stability (e.g., "Client requested market stability measures during volatility.")
     - Competitor Switching (e.g., "Client switched to us due to competitor service disruptions.")
     - PEO Clients (e.g., "PEO client services affected by competitor’s product issues.")
     - Market Volatility (e.g., "Market volatility impacted client's treasury services.")

4. **Relationship and Wallet Share**: Classify here if the text talks about strengthening relationships with clients or growing the client’s business with new products/services.
   - **Subcategories**:
     - Relationship Expansion (e.g., "Client has agreed to expand relationship with new services.")
     - Wallet Share Growth (e.g., "Achieved wallet share growth through cross-selling products.")
     - Additional Services (e.g., "Client requested additional treasury services.")
     - Revenue Growth (e.g., "Client relationship resulted in significant revenue growth.")

5. **Geographical Expansion**: Classify here if the text mentions expanding into new regions, international account setups, or cross-border transactions.
   - **Subcategories**:
     - New Regions (e.g., "Expansion into Asia Pacific region with new client accounts.")
     - International Accounts (e.g., "Opening of international accounts for client operations.")
     - Cross-border Transactions (e.g., "Enabled cross-border transactions for global operations.")

6. **Competitor Comparison**: Classify here if the text mentions competitive analysis, comparing pricing, or service advantages over competitors.
   - **Subcategories**:
     - Service Advantages (e.g., "Client chose us for service advantages over competitors.")
     - Pricing Comparisons (e.g., "Pricing comparison shows favorable rates versus competitors.")
     - Competitive Analysis (e.g., "Analysis of competitors’ offerings revealed gaps in their services.")

7. **Client Onboarding and Implementation**: Classify here if the text discusses the process of onboarding clients or implementing services.
   - **Subcategories**:
     - Smooth Transition (e.g., "Client experienced a smooth transition during onboarding.")
     - Service Implementation (e.g., "Successful implementation of client’s requested services.")
     - Client Onboarding (e.g., "New client onboarding completed for treasury services.")

8. **Other**: If the text does not match any of the above categories, classify it as "Other" with a relevant sub-category (e.g., "Not available").

For each text, provide a 2-3 word process description along with the Category and Subcategory.
Text: "{input_text}"

### Response Format:
1. Category: [Geographical Expansion, etc.]
2. Subcategory: [New Accounts, etc.]
3. Process Description: [New Account Setup, etc.]
"""

# Function to chunk text based on token length
def chunk_text(text, max_length=128):
    tokens = tokenizer.encode(text, truncation=False)
    # Split tokens into chunks of size max_length
    return [tokens[i:i+max_length] for i in range(0, len(tokens), max_length)]

# Function to classify texts
def classify_texts(df_batch):
    assigned_categories = []
    sub_categories = []
    specific_processes = []

    for idx, row in df_batch.iterrows():
        input_text = row["combined_text"]
        # **Chunk the input text**
        text_chunks = chunk_text(input_text)

        # Variables to collect results from chunks
        combined_category = []
        combined_sub_category = []
        combined_process = []

        for chunk in text_chunks:
            # Convert chunk back to text
            chunk_text_decoded = tokenizer.decode(chunk)

            # Create prompt for each chunk
            prompt = prompt_template.format(input_text=chunk_text_decoded)

            # Tokenization and model inference for each chunk
            input_tokens = tokenizer(prompt, return_tensors="pt", truncation=True, padding=True, max_length=512).to(model.device)
            output = model.generate(input_tokens['input_ids'], max_new_tokens=200, pad_token_id=tokenizer.eos_token_id)

            # Decode and extract results
            generated_text = tokenizer.decode(output[0], skip_special_tokens=True)

            # Debugging: Check what the model has generated
            print(f"Generated text for chunk in row {idx}: {generated_text}")

            # Extract classification results from each chunk
            category, sub_category, process = extract_classification(generated_text)

            # Collect results from each chunk
            combined_category.append(category)
            combined_sub_category.append(sub_category)
            combined_process.append(process)

        # **Aggregate chunk results**
        final_category = max(set(combined_category), key=combined_category.count)
        final_sub_category = max(set(combined_sub_category), key=combined_sub_category.count)
        final_process = " ".join(combined_process)  # Combine processes from chunks

        # Save the classification results for each row
        assigned_categories.append(final_category)
        sub_categories.append(final_sub_category)
        specific_processes.append(final_process)

    df_batch['assigned_category'] = assigned_categories
    df_batch['sub_category'] = sub_categories
    df_batch['specific_process'] = specific_processes

    return df_batch

# Function to extract categories, sub-categories, and specific processes
def extract_classification(text):
    # Split the text into lines
    lines = text.split("\n")
    high_level_category = "Not available"
    sub_category = "Not available"
    specific_process = "Not available"

    for line in lines:
        line = line.strip()  # Clean the line

        # Look for high-level category
        if "Category:" in line:
            high_level_category = line.split("Category:", 1)[1].strip()

        # Look for sub-category
        elif "Subcategory:" in line:
            sub_category = line.split("Subcategory:", 1)[1].strip()

        # Look for specific process
        elif "Process Description:" in line:
            specific_process = line.split("Process Description:", 1)[1].strip()

    # Debugging: Print extracted values to verify
    print(f"Extracted - Category: {high_level_category}, Sub-Category: {sub_category}, Specific Process: {specific_process}")

    return high_level_category, sub_category, specific_process

# Process data in batches
def process_batches(df, batch_size):
    all_results = pd.DataFrame()

    start_time = time.time()

    for start in tqdm(range(0, total_rows, batch_size), desc="Processing Batches"):
        end = min(start + batch_size, total_rows)

        df_batch = df.iloc[start:end].copy()

        # Run classification task for the batch
        df_batch = classify_texts(df_batch)

        # Concatenate results into the full dataframe
        all_results = pd.concat([all_results, df_batch], ignore_index=True)

    # Measure total time taken
    end_time = time.time()
    elapsed_time = end_time - start_time
    print(f"Time taken to process {len(df)} records: {elapsed_time:.2f} seconds")

    # Save final combined results to a CSV file
    all_results.to_csv('final_classification_output.csv', index=False)
    print("Final results saved to final_classification_output.csv.")

# Running the batch process with batch size of 100
process_batches(df, batch_size=1)

# Display the first few rows of the final output
df_final = pd.read_csv('final_classification_output.csv')
print("First few rows of the final saved output:")
print(df_final.head())
