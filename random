import pandas as pd
import spacy
import gensim
import matplotlib.pyplot as plt
import seaborn as sns
from textblob import TextBlob
import re
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.manifold import TSNE
from sklearn.metrics import silhouette_score
import umap

# Load data from Salesforce analytics
df_salesforce = pd.read_excel('Salesforce_Deals_Text_Analytics.xlsx')

# Concatenate text columns into a single string variable
df_salesforce['combined_text'] = df_salesforce[['record_comment_text', 'description_text', 'executive_summary_text', 'win_loss_reason_text', 'win_loss_comments_text']].apply(lambda x: ' '.join(x.dropna()), axis=1)

# Load Spacy model
nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])

# Function to preprocess text
def preprocess(document):
    document = document.lower()  # Lowercase the text
    document = re.sub(r'[^a-zA-Z\s]', '', document)  # Remove special characters and numbers
    document = re.sub(r'\s+', ' ', document).strip()  # Remove extra whitespace
    doc = nlp(document)  # Tokenize and lemmatize
    words = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct and len(token.lemma_) > 2]
    return words

# Preprocessing text columns
df_salesforce['processed_text'] = df_salesforce['combined_text'].map(preprocess)
processed_texts = df_salesforce['processed_text'].tolist()

# Define Middle Office related keywords
keywords = ['middle office', 'service', 'onboarding', 'relationship']

# Add Middle Office flag to the dataframe
df_salesforce['middle_office_related'] = df_salesforce['combined_text'].apply(lambda x: 'yes' if any(keyword in x.lower() for keyword in keywords) else 'no')

# Create a new DataFrame for Middle Office related deals
df_middle_office = df_salesforce[df_salesforce['middle_office_related'] == 'yes']

# Prepare the data for topic modeling
processed_texts_mo = df_middle_office['processed_text'].tolist()

# Create bigram and trigram models
bigram_mo = gensim.models.Phrases(processed_texts_mo, min_count=5, threshold=100)
trigram_mo = gensim.models.Phrases(bigram_mo[processed_texts_mo], threshold=100)
bigram_mod_mo = gensim.models.phrases.Phraser(bigram_mo)
trigram_mod_mo = gensim.models.phrases.Phraser(trigram_mo)

# Applying bigrams and trigrams to the preprocessed texts
texts_bigrams_mo = [bigram_mod_mo[doc] for doc in processed_texts_mo]
texts_trigrams_mo = [trigram_mod_mo[bigram_mod_mo[doc]] for doc in texts_bigrams_mo]

# Creating dictionary
dictionary_mo = gensim.corpora.Dictionary(texts_trigrams_mo)

# Filtering dictionary
dictionary_mo.filter_extremes(no_below=int(len(texts_trigrams_mo) * 0.01), no_above=0.5)

# Creating the corpus
corpus_mo = [dictionary_mo.doc2bow(text) for text in texts_trigrams_mo]

# Train the LDA model
num_topics = 10
lda_model_mo = gensim.models.LdaMulticore(corpus=corpus_mo, num_topics=num_topics, id2word=dictionary_mo, passes=10, workers=2, random_state=5)

# Extracting topic distributions for each document
topic_distributions = [dict(lda_model_mo.get_document_topics(bow)) for bow in corpus_mo]

# Converting topic distributions to DataFrame
topic_df = pd.DataFrame(topic_distributions).fillna(0)

# Dimensionality reduction using UMAP
umap_model = umap.UMAP(n_neighbors=15, n_components=2, metric='cosine', random_state=5)
umap_result = umap_model.fit_transform(topic_df)

# Clustering topics using KMeans and optimizing the number of clusters
silhouette_scores = []
k_range = range(2, 11)

for k in k_range:
    kmeans = KMeans(n_clusters=k, random_state=5)
    labels = kmeans.fit_predict(umap_result)
    silhouette_avg = silhouette_score(umap_result, labels)
    silhouette_scores.append(silhouette_avg)

optimal_k = k_range[silhouette_scores.index(max(silhouette_scores))]
kmeans = KMeans(n_clusters=optimal_k, random_state=5)
topic_df['cluster'] = kmeans.fit_predict(umap_result)

# Adding the cluster labels to the Middle Office dataframe
df_middle_office['topic_cluster'] = topic_df['cluster']

# Plotting the distribution of topics clusters
cluster_counts = df_middle_office['topic_cluster'].value_counts().sort_index()
plt.style.use('fivethirtyeight')
plt.figure(figsize=(12, 6))
sns.barplot(x=cluster_counts.index, y=cluster_counts.values)
plt.xlabel('Topic Cluster')
plt.ylabel('Number of Deals')
plt.title('Distribution of Topic Clusters in Middle Office Deals')
plt.xticks(rotation=45, ha='right')
plt.show()

# Export the Middle Office dataframe with topic clusters to Excel
df_middle_office.to_excel('Middle_Office_Topic_Clusters.xlsx', index=False)

# Plotting silhouette scores to visualize the optimal number of clusters
plt.figure(figsize=(10, 6))
plt.plot(k_range, silhouette_scores, marker='o')
plt.xlabel('Number of Clusters')
plt.ylabel('Silhouette Score')
plt.title('Silhouette Scores for Different Numbers of Clusters')
plt.show()

# Add a column for the number of deals per topic
df_topic_counts = df_middle_office['main_topic_name'].value_counts().reset_index()
df_topic_counts.columns = ['Topic', 'Deal_Count']

# Save the DataFrame with deal counts to a new sheet in the same Excel file or a new file
with pd.ExcelWriter('Middle_Office_Topics_with_Sentiment.xlsx', mode='a', engine='openpyxl') as writer:
    df_topic_counts.to_excel(writer, sheet_name='Topic_Counts', index=False)
                       

                       
