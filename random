import pandas as pd
import spacy
from collections import Counter
import matplotlib.pyplot as plt
import seaborn as sns
import re
import gensim
from datetime import datetime
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import string
from spacy.lang.en.stop_words import STOP_WORDS

# Load Spacy model
nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])
nlp.max_length = 2000000  # Increase SpaCy's max_length to handle larger texts

# Load data
df_salesforce = pd.read_excel('Data/Closed_Won Reasons/Closed_Won_Product_Capability.xlsx')

# Concatenate text columns into a single string variable
df_salesforce['combined_text'] = df_salesforce[['record_comment_text', 
                                                'executive_summary_text', 
                                                'win_loss_comments_text']].apply(lambda x: ' '.join(x.dropna()), axis=1)

# Import custom stopwords
df_stopwords = pd.read_csv('list_stopwords.csv')
stopword_list = df_stopwords['Words'].to_list()

# Adding stopwords to spacy module
for word in stopword_list:
    STOP_WORDS.add(word)
    nlp.vocab[word].is_stop = True

# Contractions and abbreviations mapping
contractions_dict = {
    "ain't": "are not", "'s": " is", "aren't": "are not", "can't": "cannot", "can't've": "cannot have",
    "'cause": "because", "could've": "could have", "couldn't": "could not", "couldn't've": "could not have",
    "didn't": "did not", "doesn't": "does not", "don't": "do not", "hadn't": "had not", "hadn't've": "had not have",
    "hasn't": "has not", "haven't": "have not", "he'd": "he would", "he'd've": "he would have", "he'll": "he will",
    "he'll've": "he will have", "how'd": "how did", "how'd'y": "how do you", "how'll": "how will", "I'd": "I would",
    "I'd've": "I would have", "I'll": "I will", "I'll've": "I will have", "I'm": "I am", "I've": "I have", "isn't": "is not",
    "it'd": "it would", "it'd've": "it would have", "it'll": "it will", "it'll've": "it will have", "let's": "let us",
    # Additional contraction mappings as needed...
}

# Function to expand contractions
contractions_re = re.compile('(%s)' % '|'.join(contractions_dict.keys()))
def expand_contractions(text):
    def replace(match):
        return contractions_dict[match.group(0)]
    return contractions_re.sub(replace, text)

# Function to remove digits
def remove_digits(text):
    return re.sub(r'\d+', '', text)

# Function to remove extra punctuation/marks
def remove_extra_marks(text):
    return re.sub(r'[^\w\s]', '', text)

# Function to remove stopwords
def remove_stopwords(text):
    doc = nlp(text)
    return ' '.join([token.text for token in doc if not token.is_stop])

# **Additional Cleanup Function** to handle erroneous stopwords like 'th'
def remove_invalid_tokens(text):
    invalid_tokens = ['th']  # Add more if necessary
    tokens = text.split()
    return ' '.join([word for word in tokens if word not in invalid_tokens])

# Function to lemmatize
def lemmatize(text):
    doc = nlp(text)
    return ' '.join([token.lemma_ for token in doc])

# Full preprocessing pipeline
def full_preprocess(text):
    text = expand_contractions(text)  # Step 1: Expand contractions
    text = text.lower()  # Step 2: Convert to lowercase
    text = remove_digits(text)  # Step 3: Remove digits
    text = remove_extra_marks(text)  # Step 4: Remove extra marks
    text = remove_stopwords(text)  # Step 5: Remove stopwords
    text = lemmatize(text)  # Step 6: Lemmatization
    text = remove_invalid_tokens(text)  # Step 7: Remove invalid tokens (like 'th')
    return text

# Example text
first_row_text = "I'll buy 20% of Union Bank's shares on 5th May! It's a great 5-year opportunity, isn't it?"

# Print the original text
print("Original Text:\n", first_row_text)

# Apply full preprocessing on the first row
processed_first_row = full_preprocess(first_row_text)

# Print the processed text
print("\nProcessed Text:\n", processed_first_row)

# ----- Below is the additional test part to process the first row from DataFrame -----

# Take the first row to test preprocessing from the dataframe
first_row_text = df_salesforce['combined_text'].iloc[0]
print("Original Text:\n", first_row_text)

# Apply full preprocessing on the first row from the dataframe
processed_first_row = full_preprocess(first_row_text)

# Print the processed text from the dataframe
print("\nProcessed Text:\n", processed_first_row)
