import pandas as pd
import gensim
from gensim import corpora
import pyLDAvis.gensim_models as gensimvis
import pyLDAvis
import spacy
import re

# Load your short texts dataset
short_texts = pd.read_csv('short_texts.csv')  # Assuming 'combined_text' column exists

# Drop any NaN values in 'combined_text' if they exist
short_texts = short_texts.dropna(subset=['combined_text'])

# Load Spacy model for tokenization and lemmatization
nlp = spacy.load('en_core_web_md', disable=['parser', 'ner'])

# Preprocessing function to clean the text
def preprocess(text):
    text = text.lower()  # Lowercase the text
    text = re.sub(r'[^a-zA-Z\s]', '', text)  # Remove special characters and numbers
    text = re.sub(r'\s+', ' ', text).strip()  # Remove extra whitespace
    doc = nlp(text)
    words = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct and len(token.lemma_) > 2]
    return words

# Apply preprocessing to each row in the 'combined_text' column
short_texts['processed_text'] = short_texts['combined_text'].apply(preprocess)

# Prepare the data for LDA
processed_texts = short_texts['processed_text'].tolist()
dictionary = corpora.Dictionary(processed_texts)
corpus = [dictionary.doc2bow(text) for text in processed_texts]

# Build LDA Model
lda_model = gensim.models.LdaModel(corpus=corpus, 
                                   id2word=dictionary, 
                                   num_topics=5,  # Adjust the number of topics as needed
                                   random_state=42,
                                   passes=15,
                                   alpha='auto',
                                   per_word_topics=True)

# Visualize the topics using pyLDAvis
lda_vis = gensimvis.prepare(lda_model, corpus, dictionary)
pyLDAvis.display(lda_vis)

# If you're running this code locally (not in a notebook), you can save the visualization as an HTML file:
pyLDAvis.save_html(lda_vis, 'lda_visualization.html')
-----
          import pandas as pd
import spacy
from collections import Counter
import matplotlib.pyplot as plt
import seaborn as sns
import gensim
import re
import pyLDAvis.gensim_models as gensimvis
import pyLDAvis
from matplotlib.pyplot import imread

# Load Spacy model (without custom stopwords)
nlp = spacy.load('en_core_web_md', disable=['parser', 'ner'])

# Function to preprocess text
def preprocess(document):
    # Lowercase the text
    document = document.lower()
    # Remove special characters and numbers
    document = re.sub(r'[^a-zA-Z\s]', '', document)
    # Remove extra whitespace
    document = re.sub(r'\s+', ' ', document).strip()
    # Tokenize and Lemmatize
    doc = nlp(document)
    # Only default stopwords from Spacy, no custom stopwords
    words = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct and len(token.lemma_) > 2]
    return words

# Read data from Salesforce analytics
df_salesforce = pd.read_excel('/mnt/data/exclude_won_rst.xlsx')

# Concatenate text columns into a single string variable
df_salesforce['combined_text'] = df_salesforce[['record_comment_text', 
                                                'executive_summary_text', 
                                                'win_loss_reason_text', 
                                                'win_loss_comments_text']].apply(lambda x: ' '.join(x.dropna()), axis=1)

# 1. Print total deal count
total_deal_count = len(df_salesforce)
print(f"Total Deal Count: {total_deal_count}")

# 2. Check how many rows are missing after combining the data
missing_rows = df_salesforce['combined_text'].isna().sum()
print(f"Number of missing rows in 'combined_text': {missing_rows}")

# 3. Separate the data based on 'account_eci' column
rst_eci_dataset = df_salesforce[df_salesforce['account_eci'] == 'RST']  # Rows where account_eci is "RST"
non_rst_dataset = df_salesforce[df_salesforce['account_eci'] != 'RST']  # All other rows (non-RST)

# Print the count of each category
print(f"RST ECI Count: {len(rst_eci_dataset)}")
print(f"Non-RST Count: {len(non_rst_dataset)}")

# 4. Save the datasets to separate files
rst_eci_dataset.to_csv('rst_eci_dataset.csv', index=False)  # Save RST data
non_rst_dataset.to_csv('non_rst_eci_dataset.csv', index=False)  # Save non-RST data

# 5. Count word tokens for each row before processing and append this to non-RST dataset
non_rst_dataset.loc[:, 'token_count'] = non_rst_dataset['combined_text'].apply(lambda x: len(x.split()))

# Save the updated non-RST dataset with token count
non_rst_dataset.to_csv('non_rst_eci_with_token_count.csv', index=False)

# 6. Perform topic modeling on rows with more than 3 word tokens in non-RST data
non_rst_gt_3_tokens = non_rst_dataset.loc[non_rst_dataset['token_count'] > 3].copy()

# Preprocessing text columns for topic modeling
non_rst_gt_3_tokens['processed_text'] = non_rst_gt_3_tokens['combined_text'].map(preprocess)
processed_texts = non_rst_gt_3_tokens['processed_text'].tolist()

# Building bi-gram and tri-gram models
bigram = gensim.models.Phrases(processed_texts, min_count=10)  # Min count increased for bigrams
trigram = gensim.models.Phrases(bigram[processed_texts], min_count=10)  # Add trigram modeling

# Add bigrams and trigrams to the processed texts
for idx in range(len(processed_texts)):
    for token in trigram[bigram[processed_texts[idx]]]:
        if '_' in token:
            processed_texts[idx].append(token)

# Creating dictionary
dictionary = gensim.corpora.Dictionary(processed_texts)

# Filtering dictionary
dictionary.filter_extremes(no_below=int(len(processed_texts) * 0.01), no_above=0.5)

# Creating the corpus
corpus = [dictionary.doc2bow(text) for text in processed_texts]

# Compute coherence values for different numbers of topics using grid search
def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=1):
    coherence_values = []
    model_list = []
    for num_topics in range(start, limit, step):
        # Fine-tuning LDA parameters with more passes and auto alpha/eta
        model = gensim.models.LdaMulticore(
            corpus=corpus,
            num_topics=num_topics,
            id2word=dictionary,
            passes=20,  # Increased number of passes
            alpha='symmetric',  # Changed to 'symmetric'
            eta='symmetric',  # Changed to 'symmetric'
            workers=2,
            random_state=5
        )
        model_list.append(model)
        coherence_model = gensim.models.CoherenceModel(
            model=model,
            texts=texts,
            dictionary=dictionary,
            coherence='c_v'
        )
        coherence_values.append(coherence_model.get_coherence())
    return model_list, coherence_values

# Run the grid search for optimal number of topics
model_list, coherence_values = compute_coherence_values(dictionary, corpus, processed_texts, limit=20, start=2, step=2)

# Find the model with the highest coherence value
optimal_model = model_list[coherence_values.index(max(coherence_values))]

# Extracting top words from each topic
def extract_top_words(model, num_words):
    top_words_per_topic = []
    for topic_id in range(model.num_topics):
        top_words = model.show_topic(topic_id, num_words)
        top_words_per_topic.append(" ".join([word for word, _ in top_words]))
    return top_words_per_topic

top_words_per_topic = extract_top_words(optimal_model, 5)

# Assigning main topic column to dataframe
def get_main_topic(corpus):
    topic_weights = optimal_model[corpus]
    main_topic = max(topic_weights, key=lambda x: x[1])
    return main_topic[0]

non_rst_gt_3_tokens['main_topic'] = [get_main_topic(corp) for corp in corpus]

# Adding the topic name correlated with main topic rank
non_rst_gt_3_tokens['main_topic_name'] = non_rst_gt_3_tokens['main_topic'].apply(lambda x: top_words_per_topic[x])

# Export the main topic information to a CSV file
non_rst_gt_3_tokens.to_csv('non_rst_eci_topic_modeling.csv', index=False)

# 7. Save rows with fewer than 3 tokens separately
non_rst_lt_3_tokens = non_rst_dataset[non_rst_dataset['token_count'] <= 3]
non_rst_lt_3_tokens.to_csv('non_rst_lt_3_tokens.csv', index=False)
print(f"Rows with less than or equal to 3 tokens: {len(non_rst_lt_3_tokens)}")

# Plotting the distribution of topics with top words as labels
topic_counts = non_rst_gt_3_tokens['main_topic'].value_counts().sort_index()

plt.style.use('fivethirtyeight')
plt.figure(figsize=(12, 6))
sns.barplot(x=topic_counts.index, y=topic_counts.values)
plt.xticks(range(len(top_words_per_topic)), [top_words_per_topic[i] for i in topic_counts.index], rotation=45, ha='right')
plt.xlabel('Topic')
plt.ylabel('Number of Deals')
plt.title("Distribution of Topics in Deals")
plt.show()

# Visualize topics with pyLDAvis
vis = gensimvis.prepare(optimal_model, corpus, dictionary)
pyLDAvis.display(vis)

# ------------ N-gram Frequency Analysis --------------

# Flatten list of tokens
all_tokens = [token for sublist in processed_texts for token in sublist]

# Get the frequency of n-grams
ngram_freq = Counter(all_tokens)

# Select top 20 most frequent n-grams
top_20_ngrams = ngram_freq.most_common(20)

# Create DataFrame for visualization
ngram_df = pd.DataFrame(top_20_ngrams, columns=['ngram', 'frequency'])

# Plot the top 20 n-grams
plt.figure(figsize=(10, 6))
sns.barplot(x='ngram', y='frequency', data=ngram_df, palette='husl')
plt.xticks(rotation=45, ha='right')
plt.title('Top 20 Most Frequent N-grams (Bigrams and Trigrams)')
plt.xlabel('N-gram')
plt.ylabel('Frequency')
plt.show()

# ------------ Display the Uploaded Image --------------

# Path to the uploaded image
img = imread('/mnt/data/Screenshot 2024-09-17 at 3.36.31â€¯PM.png')

# Show the uploaded image
plt.figure(figsize=(10, 6))
plt.imshow(img)
plt.axis('off')
plt.title('Uploaded N-gram Frequency Plot')
plt.show()
