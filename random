import pandas as pd
import spacy
from collections import Counter
import matplotlib.pyplot as plt
import seaborn as sns
import re
import string
import gensim
import pyLDAvis.gensim_models as gensimvis
import pyLDAvis
from spacy.lang.en.stop_words import STOP_WORDS

# Load Spacy model
nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])
nlp.max_length = 2000000  # Increase SpaCy's max_length to handle larger texts

# Load data
df_salesforce = pd.read_excel('Data/Closed_Won Reasons/Closed_Won_Product_Capability.xlsx')

# Concatenate text columns into a single string variable
df_salesforce['combined_text'] = df_salesforce[['record_comment_text', 
                                                'executive_summary_text', 
                                                'win_loss_comments_text']].apply(lambda x: ' '.join(x.dropna()), axis=1)

# Import custom stopwords
df_stopwords = pd.read_csv('list_stopwords.csv')
stopword_list = df_stopwords['Words'].to_list()

# Adding stopwords to spacy module
for word in stopword_list:
    STOP_WORDS.add(word)
    nlp.vocab[word].is_stop = True

# Additional stopwords to filter out single characters and unwanted terms
extra_stopwords = set(['d', 'ir', '--', '---', '-', 'new', 'account', 't', 'deal', 'represent'])
for word in extra_stopwords:
    STOP_WORDS.add(word)
    nlp.vocab[word].is_stop = True

# Contractions and abbreviations mapping
contractions_dict = {
    "ain't": "are not", "'s": " is", "aren't": "are not", "can't": "cannot",
    "'cause": "because", "could've": "could have", "couldn't": "could not", "didn't": "did not", "doesn't": "does not", 
    "don't": "do not", "hadn't": "had not", "hasn't": "has not", "haven't": "have not", "he'd": "he would",
}

# Function to expand contractions
contractions_re = re.compile('(%s)' % '|'.join(contractions_dict.keys()))
def expand_contractions(text):
    def replace(match):
        return contractions_dict[match.group(0)]
    return contractions_re.sub(replace, text)

# **Function to remove unwanted dashes and special characters**
def remove_hyphens_and_special_chars(text):
    return re.sub(r'\s*[-–—]+\s*', ' ', text)  # Replace multiple dashes (e.g., --, ---, -) with a space

# Function to normalize multi-word phrases
multi_word_phrases = {
    "account open": ["open account", "account opening"],
    "new client": ["client new"],
    "market disruption": ["disruption market"],
    "client acquisition": ["acquired new client", "acquiring new clients"],
}

def normalize_phrases(text):
    for normalized, variants in multi_word_phrases.items():
        for variant in variants:
            text = text.replace(variant, normalized)
    return text

# Function to remove digits
def remove_digits(text):
    return re.sub(r'\d+', '', text)

# Function to remove extra punctuation/marks
def remove_extra_marks(text):
    return re.sub(r'[^\w\s]', '', text)

# **Function to remove single-character tokens**
def remove_single_char_tokens(text):
    return ' '.join([word for word in text.split() if len(word) > 1])

# Function to remove stopwords
def remove_stopwords(text):
    doc = nlp(text)
    return ' '.join([token.text for token in doc if not token.is_stop])

# Function to lemmatize
def lemmatize(text):
    doc = nlp(text)
    return ' '.join([token.lemma_ for token in doc])

# **Rare word filtering**
def remove_rare_words(text, word_counter, min_freq=2):
    tokens = text.split()
    return ' '.join([word for word in tokens if word_counter[word] >= min_freq])

# Full preprocessing pipeline
def full_preprocess(text, word_counter=None, min_freq=2):
    text = expand_contractions(text)  # Step 1: Expand contractions
    text = remove_hyphens_and_special_chars(text)  # Step 2: Remove hyphens and unwanted characters
    text = text.lower()  # Step 3: Convert to lowercase
    text = remove_digits(text)  # Step 4: Remove digits
    text = remove_extra_marks(text)  # Step 5: Remove extra marks
    text = normalize_phrases(text)  # Step 6: Normalize multi-word phrases
    text = remove_stopwords(text)  # Step 7: Remove stopwords
    text = lemmatize(text)  # Step 8: Lemmatization
    text = remove_single_char_tokens(text)  # Step 9: Remove single-character tokens
    
    # Remove rare words based on frequency
    if word_counter is not None:
        text = remove_rare_words(text, word_counter, min_freq=min_freq)
    
    return text

# ---- N-gram Frequency and Plotting (Non-RST Data) ----

# Filter Non-RST data
non_rst_data = df_salesforce[df_salesforce['account_eci'] != 'RST']

# Step to create word frequency counter across the entire dataset for rare word removal
all_words = ' '.join(non_rst_data['combined_text']).split()
word_counter = Counter(all_words)

# Apply full preprocessing to Non-RST data with rare word removal
non_rst_data['processed_text'] = non_rst_data['combined_text'].apply(lambda x: full_preprocess(x, word_counter=word_counter, min_freq=2))

# Tokenize the processed text
def tokenize_text(text):
    return text.split()

# Get all tokens from the processed Non-RST deals
all_tokens_non_rst = [token for text in non_rst_data['processed_text'] for token in tokenize_text(text)]

# Function to get n-gram frequency for Non-RST data
def get_ngram_freqs(tokens_list, n=1):
    ngrams = zip(*[tokens_list[i:] for i in range(n)])
    return Counter([' '.join(ngram) for ngram in ngrams]).most_common(30)

# **Function to save n-gram frequency tables to Excel**
def save_ngram_freqs_to_excel(ngram_freqs, sheet_name, writer):
    ngram_df = pd.DataFrame(ngram_freqs, columns=['ngram', 'frequency'])
    ngram_df.to_excel(writer, sheet_name=sheet_name, index=False)

# Save n-grams to an Excel file
with pd.ExcelWriter('ngram_frequencies_non_rst.xlsx') as writer:
    unigram_freq_non_rst = get_ngram_freqs(all_tokens_non_rst, n=1)
    bigram_freq_non_rst = get_ngram_freqs(all_tokens_non_rst, n=2)
    trigram_freq_non_rst = get_ngram_freqs(all_tokens_non_rst, n=3)

    # Save each frequency table to a separate sheet
    save_ngram_freqs_to_excel(unigram_freq_non_rst, 'Unigrams', writer)
    save_ngram_freqs_to_excel(bigram_freq_non_rst, 'Bigrams', writer)
    save_ngram_freqs_to_excel(trigram_freq_non_rst, 'Trigrams', writer)

# Function to display and plot n-grams for Non-RST data with sum of frequencies
def display_ngrams(ngram_freq, title):
    ngram_df = pd.DataFrame(ngram_freq, columns=['ngram', 'frequency'])
    ngram_sum = ngram_df['frequency'].sum()  # Calculate sum of frequencies
    
    print(f"\n{title} Frequency Table (Non-RST):")
    print(ngram_df.to_string(index=False))
    print(f"\nTotal Frequency Sum: {ngram_sum}")

    plt.figure(figsize=(12, 8))
    sns.barplot(x='ngram', y='frequency', data=ngram_df, palette='coolwarm')

    # Add frequency numbers on top of bars
    for index, value in enumerate(ngram_df['frequency']):
        plt.text(index, value, str(value), ha='center', va='bottom', fontsize=10)

    plt.xticks(rotation=45, ha='right', fontsize=12)
    plt.title(f'Top 30 {title} (Non-RST)', fontsize=16, weight='bold')
    plt.tight_layout()
    plt.show()

# Display and plot unigrams, bigrams, trigrams for Non-RST deals
display_ngrams(unigram_freq_non_rst, 'Unigrams')
display_ngrams(bigram_freq_non_rst, 'Bigrams')
display_ngrams(trigram_freq_non_rst, 'Trigrams')

# ---- LDA Topic Modeling (Non-RST Data) ----

# Preprocess data for LDA
non_rst_processed_texts = [tokenize_text(text) for text in non_rst_data['processed_text']]

# Build bigram and trigram models
bigram = gensim.models.Phrases(non_rst_processed_texts, min_count=10)
trigram = gensim.models.Phrases(bigram[non_rst_processed_texts], min_count=10)

# Add bigrams and trigrams to the processed texts
non_rst_processed_texts = [trigram[bigram[text]] for text in non_rst_processed_texts]

# Create a dictionary and corpus
dictionary = gensim.corpora.Dictionary(non_rst_processed_texts)
dictionary.filter_extremes(no_below=10, no_above=0.5)
corpus = [dictionary.doc2bow(text) for text in non_rst_processed_texts]

# Train LDA model
lda_model = gensim.models.LdaMulticore(corpus=corpus, num_topics=6, id2word=dictionary, passes=10, workers=2)

# Visualize topics with pyLDAvis
lda_vis = gensimvis.prepare(lda_model, corpus, dictionary)
pyLDAvis.save_html(lda_vis, 'lda_topic_modeling_non_rst.html')

# Extract top words per topic and plot
def extract_top_words_per_topic(model, num_words=5):
    topics = model.print_topics(num_topics=model.num_topics, num_words=num_words)
    return {topic[0]: re.findall(r'"(.*?)"', topic[1]) for topic in topics}

# Plot topic distribution
def plot_topic_distribution(model, corpus):
    topic_counts = [max(model.get_document_topics(corp), key=lambda x: x[1])[0] for corp in corpus]
    topic_freq = Counter(topic_counts)

    # Plot using coolwarm palette and add frequency numbers on top of bars
    plt.figure(figsize=(12, 6))
    sns.barplot(x=list(topic_freq.keys()), y=list(topic_freq.values()), palette='coolwarm')
    
    # Add frequencies on top of bars
    for index, value in enumerate(topic_freq.values()):
        plt.text(index, value, str(value), ha='center', va='bottom', fontsize=10)
    
    plt.xlabel('Topic')
    plt.ylabel('Number of Deals')
    plt.title('LDA Topic Distribution for Non-RST Data')
    plt.show()

# Plot the topic distribution with frequency on top
plot_topic_distribution(lda_model, corpus)
