

# Load Data
df = pd.read_csv('Data/closed_won_final.csv')
df = df.head(1000)  # Limiting to 1000 rows for demonstration
total_rows = len(df)

# Updated Prompt Template
prompt_template = """
    You are an AI tasked with classifying financial and business process texts. First, understand the context of the text and classify it into one of the following categories and subcategories. If none of the categories fit, assign it to "Other" with a relevant sub-category.

    Categories:
    1. Product Capability (e.g., Strong product offerings such as ACH Direct Send, BAI reporting, comprehensive treasury services)
       - Subcategories: ACH Direct Send, BAI Reporting, Treasury Services

    2. Client-Specific Solutions (e.g., Customization, Responsiveness to client needs, New Account Setups, Specific Currency Accounts)
       - Subcategories: Customization, Account Setup, Currency Accounts

    3. Market Disruption (e.g., Clients seeking stability during market disruptions, PEO clients switching from competitors)
       - Subcategories: Market Stability, Competitor Switching, PEO Clients

    4. Relationship and Wallet Share (e.g., Expanding existing relationships, Increasing wallet share through additional services)
       - Subcategories: Relationship Expansion, Wallet Share Growth, Additional Services

    5. Geographical Expansion (e.g., Opening accounts in new regions or international entities)
       - Subcategories: New Regions, International Accounts

    6. Competitor Comparison (e.g., Highlighting advantages over competitors in service offerings or pricing)
       - Subcategories: Service Advantages, Pricing Comparisons

    7. Client Onboarding and Implementation (e.g., Efficient onboarding, Successful implementation of services)
       - Subcategories: Smooth Transition, Service Implementation, Client Onboarding

    8. Other: If the text does not match any of the above categories, classify it as "Other" with a relevant sub-category.

    For each text, provide one main category, one relevant sub-category, and a 2-3 word process description based on the context.

    Text: "{input_text}"

    ### Response Format:
    1. Category: [Choose from the list above]
    2. Subcategory: [Choose from the list above]
    3. Process Description: [Provide 2-3 words based on context]
"""

# Function to chunk text based on token length
def chunk_text(text, max_length=128):
    tokens = tokenizer.encode(text, truncation=False)
    # Split tokens into chunks of size max_length
    return [tokens[i:i+max_length] for i in range(0, len(tokens), max_length)]

# Function to classify texts
def classify_texts(df_batch):
    assigned_categories = []
    sub_categories = []
    specific_processes = []

    for idx, row in df_batch.iterrows():
        input_text = row["combined_text"]
        # **Chunk the input text**
        text_chunks = chunk_text(input_text)

        # Variables to collect results from chunks
        combined_category = []
        combined_sub_category = []
        combined_process = []

        for chunk in text_chunks:
            # Convert chunk back to text
            chunk_text_decoded = tokenizer.decode(chunk)

            # Create prompt for each chunk
            prompt = prompt_template.format(input_text=chunk_text_decoded)

            # Tokenization and model inference for each chunk
            input_tokens = tokenizer(prompt, return_tensors="pt", truncation=True, padding=True, max_length=512).to(model.device)
            output = model.generate(input_tokens['input_ids'], max_new_tokens=200, pad_token_id=tokenizer.eos_token_id)

            # Decode and extract results
            generated_text = tokenizer.decode(output[0], skip_special_tokens=True)

            # Debugging: Check what the model has generated
            print(f"Generated text for chunk in row {idx}: {generated_text}")

            # Extract classification results from each chunk
            category, sub_category, process = extract_classification(generated_text)

            # Collect results from each chunk
            combined_category.append(category)
            combined_sub_category.append(sub_category)
            combined_process.append(process)

        # **Aggregate chunk results**
        final_category = max(set(combined_category), key=combined_category.count)
        final_sub_category = max(set(combined_sub_category), key=combined_sub_category.count)
        final_process = " ".join(combined_process)  # Combine processes from chunks

        # Save the classification results for each row
        assigned_categories.append(final_category)
        sub_categories.append(final_sub_category)
        specific_processes.append(final_process)

    df_batch['assigned_category'] = assigned_categories
    df_batch['sub_category'] = sub_categories
    df_batch['specific_process'] = specific_processes

    return df_batch

# Function to extract categories, sub-categories, and specific processes
def extract_classification(text):
    # Split the text into lines
    lines = text.split("\n")
    high_level_category = "Not available"
    sub_category = "Not available"
    specific_process = "Not available"

    for line in lines:
        line = line.strip()  # Clean the line

        # Look for high-level category
        if "Category:" in line:
            high_level_category = line.split("Category:", 1)[1].strip()

        # Look for sub-category
        elif "Subcategory:" in line:
            sub_category = line.split("Subcategory:", 1)[1].strip()

        # Look for specific process
        elif "Process Description:" in line:
            specific_process = line.split("Process Description:", 1)[1].strip()

    # Debugging: Print extracted values to verify
    print(f"Extracted - Category: {high_level_category}, Sub-Category: {sub_category}, Specific Process: {specific_process}")

    return high_level_category, sub_category, specific_process

# Process data in batches
def process_batches(df, batch_size):
    all_results = pd.DataFrame()

    start_time = time.time()

    for start in tqdm(range(0, total_rows, batch_size), desc="Processing Batches"):
        end = min(start + batch_size, total_rows)

        df_batch = df.iloc[start:end].copy()

        # Run classification task for the batch
        df_batch = classify_texts(df_batch)

        # Concatenate results into the full dataframe
        all_results = pd.concat([all_results, df_batch], ignore_index=True)

    # Measure total time taken
    end_time = time.time()
    elapsed_time = end_time - start_time
    print(f"Time taken to process {len(df)} records: {elapsed_time:.2f} seconds")

    # Save final combined results to a CSV file
    all_results.to_csv('final_classification_output.csv', index=False)
    print("Final results saved to final_classification_output.csv.")

# Running the batch process with batch size of 100
process_batches(df, batch_size=100)

# Display the first few rows of the final output
df_final = pd.read_csv('final_classification_output.csv')
print("First few rows of the final saved output:")
print(df_final.head())
