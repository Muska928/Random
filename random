

# Define custom stopwords and multi-word phrases
df_stopwords = pd.read_csv('list_stopwords.csv')
stopword_list = df_stopwords['Words'].to_list()

# Adding stopwords to spacy module
for word in stopword_list:
    nlp.vocab[word].is_stop = True

extra_stopwords = set(['d', 'ir', '--', '---', '-', 'new', 'account', 't', 'deal', 'represent'])
for word in extra_stopwords:
    nlp.vocab[word].is_stop = True

# Define multi-word phrases for normalization
multi_word_phrases = {
    "account opening": ["account open", "open account", "account opening", "open accounts"],
    "client acquisition": ["acquired new client", "acquiring new clients"],
    "new account": ["account new", "open new account"],
    "client request": ["request client"],
    "market disruption": ["disruption market"],
    "new client": ["client new"],
    "commercial card": ["card commercial"],
    "bb transfer": ["transfer bb"],
    "core cash": ["cash core"],
    "cash liquidity": ["liquidity cash"],
}

# Function to expand contractions
contractions_dict = {
    "ain't": "are not", "'s": " is", "aren't": "are not", "can't": "cannot",
    "couldn't": "could not", "didn't": "did not", "doesn't": "does not",
    "don't": "do not", "hadn't": "had not", "hasn't": "has not", "haven't": "have not",
    "he'd": "he would", "he'll": "he will", "I'd": "I would", "I'll": "I will",
    "I'm": "I am", "I've": "I have", "isn't": "is not", "it'd": "it would", "it'll": "it will",
    "let's": "let us", "ma'am": "madam", "might've": "might have", "must've": "must have",
    "needn't": "need not", "shan't": "shall not", "she'd": "she would", "she'll": "she will",
    "should've": "should have", "shouldn't": "should not", "that's": "that is",
    "there'd": "there would", "they'd": "they would", "they'll": "they will",
    "they're": "they are", "they've": "they have", "won't": "will not",
    "would've": "would have", "wouldn't": "would not", "y'all": "you all"
}

# Function to expand contractions
def expand_contractions(text):
    contractions_re = re.compile('(%s)' % '|'.join(contractions_dict.keys()))
    def replace(match):
        return contractions_dict[match.group(0)]
    return contractions_re.sub(replace, text)

# Function to normalize multi-word phrases
def normalize_phrases(text):
    for normalized, variants in multi_word_phrases.items():
        for variant in variants:
            text = text.replace(variant, normalized)
    return text

# Function to remove hyphens and unwanted special characters
def remove_hyphens_and_special_chars(text):
    return re.sub(r'\s*[-–—]+\s*', ' ', text)

# Function to remove digits
def remove_digits(text):
    return re.sub(r'\d+', '', text)

# Function to remove extra punctuation
def remove_extra_marks(text):
    return re.sub(r'[^\w\s]', '', text)

# Function to remove single-character tokens
def remove_single_char_tokens(text):
    return ' '.join([word for word in text.split() if len(word) > 1])

# Function to remove stopwords
def remove_stopwords(text):
    doc = nlp(text)
    return ' '.join([token.text for token in doc if not token.is_stop])

# Function to lemmatize text
def lemmatize(text):
    doc = nlp(text)
    return ' '.join([token.lemma_ for token in doc])

# Function to remove rare words based on frequency
def remove_rare_words(text, word_counter, min_freq=2):
    tokens = text.split()
    return ' '.join([word for word in tokens if word_counter[word] >= min_freq])

# Full preprocessing pipeline
def full_preprocess(text, word_counter=None, min_freq=2):
    text = expand_contractions(text)  # Step 1: Expand contractions
    text = remove_hyphens_and_special_chars(text)  # Step 2: Remove hyphens and unwanted characters
    text = text.lower()  # Step 3: Convert to lowercase
    text = normalize_phrases(text)  # Step 4: Normalize multi-word phrases
    text = remove_digits(text)  # Step 5: Remove digits
    text = remove_extra_marks(text)  # Step 6: Remove extra marks
    text = remove_stopwords(text)  # Step 7: Remove stopwords
    text = lemmatize(text)  # Step 8: Lemmatization
    text = remove_single_char_tokens(text)  # Step 9: Remove single-character tokens
    
    # Remove rare words based on frequency
    if word_counter is not None:
        text = remove_rare_words(text, word_counter, min_freq=min_freq)
    
    return text

# Test sentence to validate preprocessing
test_sentence = """
    The account opening process was tedious. Client acquisition was smooth for the Union Bank.
    The market disruption caused by core cash management issues affected the overall performance.
"""

# Simulate word frequencies (in a real case, this should be built from the dataset)
test_word_counter = Counter(test_sentence.lower().split())

# Apply preprocessing to test sentence
processed_test_sentence = full_preprocess(test_sentence, word_counter=test_word_counter, min_freq=1)

# Print the processed test sentence
print("Original Test Sentence:")
print(test_sentence)

print("\nProcessed Test Sentence:")
print(processed_test_sentence)

# Step 3: Apply Preprocessing to Non-RST Data

# Assuming non_rst_cleaned is your cleaned dataframe with the 'combined_text' column

# Apply preprocessing to the cleaned Non-RST data
non_rst_cleaned['processed_text'] = non_rst_cleaned['combined_text'].apply(lambda x: full_preprocess(x))

# Print a sample of the processed Non-RST dataset
print(non_rst_cleaned[['combined_text', 'processed_text']].head())

# Save the processed data for future analysis
non_rst_cleaned.to_csv('non_rst_cleaned_processed.csv', index=False)
