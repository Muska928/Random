import pandas as pd
import spacy
from collections import Counter
import matplotlib.pyplot as plt
import seaborn as sns
import re
from nltk import ngrams
import gensim
import pyLDAvis.gensim_models as gensimvis
import pyLDAvis

# Load Spacy model (without custom stopwords)
nlp = spacy.load('en_core_web_md', disable=['parser', 'ner'])

# Function to preprocess text
def preprocess(document):
    document = document.lower()
    document = re.sub(r'[^a-zA-Z\s]', '', document)
    document = re.sub(r'\s+', ' ', document).strip()
    doc = nlp(document)
    words = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct and len(token.lemma_) > 2]
    return words

# Function to get n-grams (unigram, bigram, trigram, four-gram)
def get_ngrams(tokens, n):
    return list(ngrams(tokens, n))

# Function to analyze and visualize n-grams
def analyze_ngrams(df_subset, title, ngram_type):
    all_tokens = [token for sublist in df_subset['processed_text'] for token in sublist]

    # Get the frequency of n-grams
    ngram_freq = Counter(get_ngrams(all_tokens, ngram_type))

    # Select top 20 most frequent n-grams
    top_20_ngrams = ngram_freq.most_common(20)

    # Create DataFrame for visualization and saving to CSV
    ngram_df = pd.DataFrame(top_20_ngrams, columns=['ngram', 'frequency'])
    csv_filename = f'{title}_top_{ngram_type}gram.csv'
    ngram_df.to_csv(csv_filename, index=False)

    # Plot the top 20 n-grams
    plt.figure(figsize=(12, 8))
    sns.barplot(x='ngram', y='frequency', data=ngram_df, palette='coolwarm')
    plt.xticks(rotation=45, ha='right', fontsize=12)
    plt.yticks(fontsize=12)
    plt.title(f'Top 20 Most Frequent {ngram_type}-grams ({title})', fontsize=16, weight='bold')
    plt.xlabel(f'{ngram_type}-gram', fontsize=14, weight='bold')
    plt.ylabel('Frequency', fontsize=14, weight='bold')

    # Add data labels for each bar
    for index, value in enumerate(ngram_df['frequency']):
        plt.text(index, value + 500, str(value), ha='center', fontsize=12)

    plt.tight_layout()
    plt.show()

# Read data from Salesforce analytics
df_salesforce = pd.read_excel('Data/exclude_won_rst.xlsx')

# Concatenate text columns into a single string variable
df_salesforce['combined_text'] = df_salesforce[['record_comment_text', 
                                                'executive_summary_text', 
                                                'win_loss_reason_text', 
                                                'win_loss_comments_text']].apply(lambda x: ' '.join(x.dropna()), axis=1)

# Preprocess the combined text
df_salesforce['processed_text'] = df_salesforce['combined_text'].apply(preprocess)

# Token count for each row
df_salesforce['token_count'] = df_salesforce['processed_text'].apply(len)

# Analyzing and printing the categories
# 1) RST Data
rst_data = df_salesforce[df_salesforce['account_eci'] == 'RST']
print(f"Total RST Data Count: {len(rst_data)}")

# Analyze unigrams, bigrams, trigrams, and four-grams for RST Data
for ngram_type in [1, 2, 3, 4]:
    analyze_ngrams(rst_data, 'RST_Data', ngram_type)

# 2) Non-RST Data
non_rst_data = df_salesforce[df_salesforce['account_eci'] != 'RST']
print(f"Total Non-RST Data Count: {len(non_rst_data)}")

# Analyze unigrams, bigrams, trigrams, and four-grams for Non-RST Data
for ngram_type in [1, 2, 3, 4]:
    analyze_ngrams(non_rst_data, 'Non_RST_Data', ngram_type)

# 3) Token length <= 3
token_len_le_3_data = non_rst_data[non_rst_data['token_count'] <= 3]
print(f"Total Non-RST Data with Token Length <= 3: {len(token_len_le_3_data)}")

# Analyze unigrams, bigrams, trigrams, and four-grams for Non-RST Data with token length <= 3
for ngram_type in [1, 2, 3, 4]:
    analyze_ngrams(token_len_le_3_data, 'Non_RST_Data_Token_Len_<=_3', ngram_type)

# 4) Token length >= 4
token_len_ge_4_data = non_rst_data[non_rst_data['token_count'] >= 4]
print(f"Total Non-RST Data with Token Length >= 4: {len(token_len_ge_4_data)}")

# Analyze unigrams, bigrams, trigrams, and four-grams for Non-RST Data with token length >= 4
for ngram_type in [1, 2, 3, 4]:
    analyze_ngrams(token_len_ge_4_data, 'Non_RST_Data_Token_Len_>=_4', ngram_type)

# LDA topic modeling only for Non-RST data with token length >= 4
# Preprocessing text columns for topic modeling
non_rst_ge_4_data['processed_text'] = non_rst_ge_4_data['combined_text'].map(preprocess)
processed_texts = non_rst_ge_4_data['processed_text'].tolist()

# Building bigram and trigram models
bigram = gensim.models.Phrases(processed_texts, min_count=10)
trigram = gensim.models.Phrases(bigram[processed_texts], min_count=10)

# Add bigrams and trigrams to the processed texts
for idx in range(len(processed_texts)):
    for token in trigram[bigram[processed_texts[idx]]]:
        if '_' in token:
            processed_texts[idx].append(token)

# Creating dictionary
dictionary = gensim.corpora.Dictionary(processed_texts)

# Filtering dictionary
dictionary.filter_extremes(no_below=int(len(processed_texts) * 0.01), no_above=0.5)

# Creating the corpus
corpus = [dictionary.doc2bow(text) for text in processed_texts]

# Compute coherence values for different numbers of topics using grid search
def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=1):
    coherence_values = []
    model_list = []
    for num_topics in range(start, limit, step):
        model = gensim.models.LdaMulticore(
            corpus=corpus,
            num_topics=num_topics,
            id2word=dictionary,
            passes=20,
            alpha='symmetric',
            eta='symmetric',
            workers=2,
            random_state=5
        )
        model_list.append(model)
        coherence_model = gensim.models.CoherenceModel(
            model=model,
            texts=texts,
            dictionary=dictionary,
            coherence='c_v'
        )
        coherence_values.append(coherence_model.get_coherence())
    return model_list, coherence_values

# Run the grid search for optimal number of topics
model_list, coherence_values = compute_coherence_values(dictionary, corpus, processed_texts, limit=20, start=2, step=2)

# Find the model with the highest coherence value
optimal_model = model_list[coherence_values.index(max(coherence_values))]

# Extracting top words from each topic
def extract_top_words(model, num_words):
    top_words_per_topic = []
    for topic_id in range(model.num_topics):
        top_words = model.show_topic(topic_id, num_words)
        top_words_per_topic.append(" ".join([word for word, _ in top_words]))
    return top_words_per_topic

top_words_per_topic = extract_top_words(optimal_model, 5)

# Assigning main topic column to dataframe
def get_main_topic(corpus):
    topic_weights = optimal_model[corpus]
    main_topic = max(topic_weights, key=lambda x: x[1])
    return main_topic[0]

non_rst_ge_4_data['main_topic'] = [get_main_topic(corp) for corp in corpus]

# Adding the topic name correlated with main topic rank
non_rst_ge_4_data['main_topic_name'] = non_rst_ge_4_data['main_topic'].apply(lambda x: top_words_per_topic[x])

# Export the main topic information to a CSV file
non_rst_ge_4_data.to_csv('non_rst_eci_topic_modeling.csv', index=False)

# Visualize topics with pyLDAvis
vis = gensimvis.prepare(optimal_model, corpus, dictionary)
pyLDAvis.display(vis)
