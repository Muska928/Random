import pandas as pd
import spacy
import gensim
import matplotlib.pyplot as plt
import seaborn as sns
from textblob import TextBlob
import re

# Load data from Salesforce analytics
df_salesforce = pd.read_excel('Salesforce_Deals_Text_Analytics.xlsx')

# Concatenate text columns into a single string variable
df_salesforce['combined_text'] = df_salesforce[['record_comment_text', 'description_text', 'executive_summary_text', 'win_loss_reason_text', 'win_loss_comments_text']].apply(lambda x: ' '.join(x.dropna()), axis=1)

# Load Spacy model
nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])

# Function to preprocess text
def preprocess(document):
    document = document.lower()  # Lowercase the text
    document = re.sub(r'[^a-zA-Z\s]', '', document)  # Remove special characters and numbers
    document = re.sub(r'\s+', ' ', document).strip()  # Remove extra whitespace
    doc = nlp(document)  # Tokenize and lemmatize
    words = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct and len(token.lemma_) > 2]
    return words

# Preprocessing text columns
df_salesforce['processed_text'] = df_salesforce['combined_text'].map(preprocess)
processed_texts = df_salesforce['processed_text'].tolist()

# Define Middle Office related keywords
keywords = ['middle office', 'service', 'onboarding', 'relationship']

# Add Middle Office flag to the dataframe
df_salesforce['middle_office_related'] = df_salesforce['combined_text'].apply(lambda x: 'yes' if any(keyword in x.lower() for keyword in keywords) else 'no')

# Create a new DataFrame for Middle Office related deals
df_middle_office = df_salesforce[df_salesforce['middle_office_related'] == 'yes']

# Sentiment Analysis
df_middle_office['sentiment'] = df_middle_office['combined_text'].apply(lambda x: TextBlob(x).sentiment.polarity)

# Prepare the data for topic modeling
processed_texts_mo = df_middle_office['processed_text'].tolist()

# Create bigram and trigram models
bigram_mo = gensim.models.Phrases(processed_texts_mo, min_count=5, threshold=100)
trigram_mo = gensim.models.Phrases(bigram_mo[processed_texts_mo], threshold=100)
bigram_mod_mo = gensim.models.phrases.Phraser(bigram_mo)
trigram_mod_mo = gensim.models.phrases.Phraser(trigram_mo)

# Applying bigrams and trigrams to the preprocessed texts
texts_bigrams_mo = [bigram_mod_mo[doc] for doc in processed_texts_mo]
texts_trigrams_mo = [trigram_mod_mo[bigram_mod_mo[doc]] for doc in texts_bigrams_mo]

# Creating dictionary
dictionary_mo = gensim.corpora.Dictionary(texts_trigrams_mo)

# Filtering dictionary
dictionary_mo.filter_extremes(no_below=int(len(texts_trigrams_mo) * 0.01), no_above=0.5)

# Creating the corpus
corpus_mo = [dictionary_mo.doc2bow(text) for text in texts_trigrams_mo]

# Function to compute coherence values for LDA models
def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=1):
    coherence_values = []
    model_list = []
    for num_topics in range(start, limit, step):
        model = gensim.models.LdaMulticore(corpus=corpus, num_topics=num_topics, id2word=dictionary, passes=10, workers=2, random_state=5)
        model_list.append(model)
        coherence_model = gensim.models.CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')
        coherence_values.append(coherence_model.get_coherence())
    return model_list, coherence_values

# Computing coherence values and selecting optimal model for Middle Office deals
model_list_mo, coherence_values_mo = compute_coherence_values(dictionary=dictionary_mo, corpus=corpus_mo, texts=texts_trigrams_mo, start=2, limit=13, step=1)
optimal_model_mo = model_list_mo[coherence_values_mo.index(max(coherence_values_mo))]

# Extracting top words from each topic for Middle Office deals
def extract_top_words(model, num_words):
    top_words_per_topic = []
    for topic_id in range(model.num_topics):
        top_words = model.show_topic(topic_id, num_words)
        top_words_per_topic.append(' '.join([word for word, _ in top_words]))
    return top_words_per_topic

top_words_per_topic_mo = extract_top_words(optimal_model_mo, 5)

# Assigning main topic column to the Middle Office dataframe
def get_main_topic(corpus):
    topic_weights = optimal_model_mo[corpus]
    main_topic = max(topic_weights, key=lambda x: x[1])[0]
    return main_topic

df_middle_office['main_topic'] = [get_main_topic(corp) for corp in corpus_mo]

# Adding the topic name correlated with main topic rank to the Middle Office dataframe
df_middle_office['main_topic_name'] = df_middle_office['main_topic'].map(lambda x: top_words_per_topic_mo[x])

# Plotting the distribution of topics with top words as labels for Middle Office deals
topic_counts_mo = df_middle_office['main_topic'].value_counts().sort_index()
plt.style.use('fivethirtyeight')
plt.figure(figsize=(12, 6))
sns.barplot(x=topic_counts_mo.index, y=topic_counts_mo.values)
plt.xticks(range(len(top_words_per_topic_mo)), top_words_per_topic_mo, rotation=45, ha='right')
plt.xlabel('Topic')
plt.ylabel('Number of Deals')
plt.title('Distribution of Topics in Middle Office Deals')
plt.show()

# Export the Middle Office dataframe with main topics and sentiment analysis to Excel
df_middle_office.to_excel('Middle_Office_Topics_with_Sentiment.xlsx', index=False)
