import pandas as pd
import spacy
from collections import Counter
import matplotlib.pyplot as plt
import re
import gensim

# Load Spacy model (without custom stopwords)
nlp = spacy.load('en_core_web_md', disable=['parser', 'ner'])

# Contractions and abbreviations mapping
contractions = {
    "won't": "will not",
    "can't": "cannot",
    "i'm": "i am",
    "it's": "it is",
    "we've": "we have",
    "they're": "they are",
    "corp": "corporation",
    "inc": "incorporated",
    # Add more as needed
}

# Function to expand contractions
def expand_contractions(text):
    for contraction, expansion in contractions.items():
        text = re.sub(r"\b" + contraction + r"\b", expansion, text)
    return text

# Function to preprocess text and remove rare words
def preprocess(document, word_freqs, rare_threshold):
    # Convert to lowercase
    document = document.lower()

    # Expand contractions
    document = expand_contractions(document)

    # Remove special characters, numbers, and underscores
    document = re.sub(r'[^a-zA-Z\s]', '', document)

    # Remove extra spaces
    document = re.sub(r'\s+', ' ', document).strip()

    # Tokenize and Lemmatize
    doc = nlp(document)

    # Remove stopwords and rare words, and lemmatize
    words = [
        token.lemma_ for token in doc
        if not token.is_stop and not token.is_punct and len(token.lemma_) > 2
        and word_freqs[token.lemma_] > rare_threshold  # Exclude rare words
    ]

    return words

# Function to get n-gram frequency
def get_ngram_freqs(tokens_list, n=1):
    if n == 1:
        return Counter(tokens_list).most_common(20)
    else:
        ngrams = zip(*[tokens_list[i:] for i in range(n)])
        return Counter([' '.join(ngram) for ngram in ngrams]).most_common(20)

# Function to save n-grams to Excel file
def save_ngrams_to_excel(ngram_freq, sheet_name, writer):
    # Create a DataFrame from n-gram frequency data
    ngram_df = pd.DataFrame(ngram_freq, columns=['ngram', 'frequency'])
    # Save the DataFrame to the respective sheet in the Excel writer
    ngram_df.to_excel(writer, sheet_name=sheet_name, index=False)

# Read data from Salesforce analytics
df_salesforce = pd.read_excel('Data/exclude_won_rst.xlsx')

# Concatenate text columns into a single string variable
df_salesforce['combined_text'] = df_salesforce[['record_comment_text', 
                                                'executive_summary_text', 
                                                'win_loss_reason_text', 
                                                'win_loss_comments_text']].apply(lambda x: ' '.join(x.dropna()), axis=1)

# First pass to calculate word frequencies across the entire dataset
all_text = ' '.join(df_salesforce['combined_text'].tolist())
all_tokens = [token.lemma_ for token in nlp(all_text) if not token.is_stop and not token.is_punct and len(token.lemma_) > 2]
word_freqs = Counter(all_tokens)

# Define a threshold for rare words (e.g., words that occur fewer than 5 times)
rare_threshold = 5

# Identify rare words that will be removed
rare_words = [word for word, count in word_freqs.items() if count <= rare_threshold]

# Print the list of rare words that will be removed
print("\n==== Rare Words to be Removed ====\n")
print(rare_words)

# Preprocess the combined text, removing rare words
df_salesforce['processed_text'] = df_salesforce['combined_text'].apply(lambda doc: preprocess(doc, word_freqs, rare_threshold))

# Token count for each row
df_salesforce['token_count'] = df_salesforce['processed_text'].apply(len)

# Deals Overview
print("\n==== DEALS OVERVIEW ====\n")

# 1. Total Count of Deals
total_deal_count = len(df_salesforce)
print(f"Total Count of Deals: {total_deal_count}")

# 2. Count of RST Deals
rst_deal_count = len(df_salesforce[df_salesforce['account_eci'] == 'RST'])
print(f"Count of RST Deals: {rst_deal_count}")

# 3. Count of Non-RST Deals
non_rst_deal_count = len(df_salesforce[df_salesforce['account_eci'] != 'RST'])
print(f"Count of Non-RST Deals: {non_rst_deal_count}")

# 4. Count of Deals by Win/Loss Reason
win_loss_reason_counts = df_salesforce['win_loss_reason_text'].value_counts()
print("\nCount of Deals by Win/Loss Reason:")
print(win_loss_reason_counts)

# Filter Non-RST data (without token length constraints)
non_rst_data = df_salesforce[df_salesforce['account_eci'] != 'RST']

# Preprocessing the text for analysis
processed_texts = non_rst_data['processed_text'].tolist()

# Building bi-gram and tri-gram models
bigram = gensim.models.Phrases(processed_texts, min_count=10)
trigram = gensim.models.Phrases(bigram[processed_texts], min_count=10)

# Add bigrams and trigrams to the processed texts
for idx in range(len(processed_texts)):
    for token in trigram[bigram[processed_texts[idx]]]:
        if '_' in token:
            processed_texts[idx].append(token)

# Recreate processed text back into the dataframe
non_rst_data['processed_text'] = processed_texts

# Extracting N-Gram frequencies for unigrams, bigrams, trigrams, and four-grams
all_tokens = [token for sublist in non_rst_data['processed_text'] for token in sublist]

# Unigrams, Bigrams, Trigrams, Four-Grams Frequency Analysis
unigram_freq = get_ngram_freqs(all_tokens, n=1)
bigram_freq = get_ngram_freqs(all_tokens, n=2)
trigram_freq = get_ngram_freqs(all_tokens, n=3)
fourgram_freq = get_ngram_freqs(all_tokens, n=4)

# Save the n-gram frequency analysis results to an Excel file
output_file = 'ngram_frequency_analysis_non_rst.xlsx'
with pd.ExcelWriter(output_file) as writer:
    # Save each n-gram frequency to a separate sheet
    save_ngrams_to_excel(unigram_freq, 'Unigrams', writer)
    save_ngrams_to_excel(bigram_freq, 'Bigrams', writer)
    save_ngrams_to_excel(trigram_freq, 'Trigrams', writer)
    save_ngrams_to_excel(fourgram_freq, 'Four-Grams', writer)

print(f"\nN-Gram frequency analysis results saved to {output_file}")
