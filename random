Subject: TIM PLI Complexity Scoring – Regression-Based Weights, Derived Attributes & Feedback Request

Hi [Daniel / Team],

As part of our ongoing enhancement of the complexity scoring framework for TIM PLIs, I’m sharing our latest results from a regression-based model that quantifies complexity using historical cycle time data.

This version incorporates both native and derived attributes, providing a transparent, data-driven view of complexity drivers to support prioritization and effort estimation.

Methodology Overview
A linear regression model was trained to predict pli_active_cycle_time using PLI setup types and flag data.

Log transformation was applied to normalize skewed cycle times.

Predicted values were grouped into four complexity levels using percentile-based binning.

We calculated model-derived weights using Pearson correlation, indicating each attribute’s contribution to predicted cycle time.

Several derived features were engineered to capture complex flag patterns in a simplified and interpretable format.

Cycle Time Ranges by Complexity Score
Complexity Level	Predicted Cycle Time (days)	PLI Count
1 (Low)	10.37 – 54.05	73
2	54.11 – 54.90	248
3	55.09 – 70.34	107
4 (High)	70.42 – 203.23	108

Key Attributes and Model-Derived Weights
Attribute	Correlation with Cycle Time
pli_type_SETUP EXISTING	0.322
paper_eob_conversion_to_835_flag	0.269
has_multiple_flags_on (derived)	0.267
pli_type_SETUP NEW	0.210
835_outbound_transmission_commercial_flag	0.176
patient_payment_flag	0.160
corr_index_flag	0.149
outbound_hcl_image_transmission_flag	0.114
835_outbound_transmission_patientpay_flag	0.105
is_image_file_used (derived)	0.103
is_recon (derived)	0.053

Derived Attributes Summary
We created composite features to simplify scoring logic without losing signal:

has_multiple_flags_on: Counts active complexity-driving flags (correlation: 0.267)

is_image_file_used: Combines multiple image-based flags (correlation: 0.103)

is_recon: Captures reconciliation setup presence (correlation: 0.053)

These features help streamline the scoring process while preserving accuracy.

Complexity Score Distribution
Score	Count
1	73
2	248
3	107
4	108

Request for Feedback
We’d appreciate your guidance on the following areas:

Do the model-derived weights align with your experience of real-world implementation effort?

Are the derived features useful in simplifying scoring while maintaining accuracy?

Does the complexity distribution feel appropriate for downstream use cases like job grading or resource allocation?

Are there specific attributes or flags you believe should be prioritized higher or lower, regardless of correlation?

Would you prefer a hybrid scoring model that combines SME-assigned weights with model insights for greater flexibility?

We’re happy to walk through examples, adjust thresholds, or share a comparative view with SME-based scoring.
