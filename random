# Updated Prompt Template
prompt_template = """
You are an AI tasked with classifying financial and business process texts. For each text, first **understand the context** and then classify it into one of the following main categories, subcategories, or assign it to "Other" if none of the categories fit. If none of the subcategories fit, assign the subcategory to "Other" under the relevant main category.

**Main Categories:**
1. **Product Capability** (e.g., ACH Direct Send, BAI Reporting, Treasury Services)
    - Subcategories: Wires, Online Access (e.g., JPM Access or Chase Connect), Lockbox, Checks
    - Example: Deals won due to strong product offerings such as ACH Direct Send, BAI Reporting, treasury services, Wires, and online access through JPM Access or Chase Connect. Clients found products tailored to their needs and easy to integrate with their business processes.
    
2. **Client-Specific Solutions** (e.g., Customization for new account setups, Specific Currency Accounts)
    - Subcategories: New Account Setup, Specific Client Currency Accounts, Customization
    - Example: Winning deals due to responsiveness to client-specific needs.

3. **Market Disruption** (e.g., Stability in times of market volatility)
    - Subcategories: Market Volatility, Stability Solutions
    - Example: Clients switching from competitors during disruptions.

4. **Relationship and Wallet Share** (e.g., Expanding existing client relationships, increasing wallet share)
    - Subcategories: Expanding Services, Cross-Sell, Client Retention
    - Example: Strengthening relationships to win additional services from clients.

5. **Geographical Expansion** (e.g., Opening new accounts in international regions)
    - Subcategories: New International Accounts, Regional Expansion
    - Example: Deals involving opening accounts in new regions or entities.

6. **Competitor Comparison** (e.g., Highlighting advantages over competitors in offerings or pricing)
    - Subcategories: Pricing Advantages, Product Superiority, Service Offering Superiority
    - Example: Winning deals by emphasizing advantages over competitors.

7. **Client Onboarding and Implementation** (e.g., Smooth transition and service implementation)
    - Subcategories: Onboarding Efficiency, Smooth Service Implementation
    - Example: Deals won due to efficient onboarding and smooth service delivery.

8. **Other**: If the text does not match any of the above categories, classify it as "Other" and assign a relevant sub-category or "Other" if no sub-category fits.

For each text, provide a high-level category, a relevant sub-category with specifics, and a 2-3 word process description based on the context.

Text: "{input_text}"

### Response Format:
1. Main Category: [Product Capability / Client-Specific Solutions / Market Disruption / Relationship and Wallet Share / Geographical Expansion / Competitor Comparison / Client Onboarding and Implementation / Other]
2. Sub-Category: [Assigned Sub-Category with details (if applicable) or "Other"]
3. Specific Process: [2-3 word description]
"""

# Function to chunk text based on token length
def chunk_text(text, max_length=128):
    tokens = tokenizer.encode(text, truncation=False)
    # Split tokens into chunks of size max_length
    return [tokens[i:i+max_length] for i in range(0, len(tokens), max_length)]

# Function to classify texts
def classify_texts(df_batch):
    assigned_categories = []
    sub_categories = []
    specific_processes = []

    for idx, row in df_batch.iterrows():
        input_text = row["combined_text"]
        # **Chunk the input text**
        text_chunks = chunk_text(input_text)

        # Variables to collect results from chunks
        combined_category = []
        combined_sub_category = []
        combined_process = []

        for chunk in text_chunks:
            # Convert chunk back to text
            chunk_text_decoded = tokenizer.decode(chunk)

            # Create prompt for each chunk
            prompt = prompt_template.format(input_text=chunk_text_decoded)

            # Tokenization and model inference for each chunk
            input_tokens = tokenizer(prompt, return_tensors="pt", truncation=True, padding=True, max_length=512).to(model.device)
            output = model.generate(input_tokens['input_ids'], max_new_tokens=200, pad_token_id=tokenizer.eos_token_id)

            # Decode and extract results
            generated_text = tokenizer.decode(output[0], skip_special_tokens=True)

            # Debugging: Check what the model has generated
            print(f"Generated text for chunk in row {idx}: {generated_text}")

            # Extract classification results from each chunk
            category, sub_category, process = extract_classification(generated_text)

            # Collect results from each chunk
            combined_category.append(category)
            combined_sub_category.append(sub_category)
            combined_process.append(process)

        # **Aggregate chunk results**
        final_category = max(set(combined_category), key=combined_category.count)
        final_sub_category = max(set(combined_sub_category), key=combined_sub_category.count)
        final_process = " ".join(combined_process)  # Combine processes from chunks

        # Save the classification results for each row
        assigned_categories.append(final_category)
        sub_categories.append(final_sub_category)
        specific_processes.append(final_process)

    df_batch['assigned_category'] = assigned_categories
    df_batch['sub_category'] = sub_categories
    df_batch['specific_process'] = specific_processes

    return df_batch

# Updated function to extract categories, sub-categories, and specific processes
def extract_classification(text):
    # Split the text into lines
    lines = text.split("\n")
    high_level_category = "Not available"
    sub_category = "Not available"
    specific_process = "Not available"

    for line in lines:
        line = line.strip()  # Clean the line

        # Look for high-level category
        if "Main Category" in line:
            high_level_category = line.split("Main Category:", 1)[1].strip()

        # Look for sub-category
        elif "Sub-Category" in line:
            sub_category = line.split("Sub-Category:", 1)[1].strip()

        # Look for specific process
        elif "Specific Process" in line:
            specific_process = line.split("Specific Process:", 1)[1].strip()

    # Debugging: Print extracted values to verify
    print(f"Extracted - Category: {high_level_category}, Sub-Category: {sub_category}, Specific Process: {specific_process}")

    return high_level_category, sub_category, specific_process

# Process data in batches
def process_batches(df, batch_size):
    all_results = pd.DataFrame()

    start_time = time.time()

    for start in tqdm(range(0, total_rows, batch_size), desc="Processing Batches"):
        end = min(start + batch_size, total_rows)

        df_batch = df.iloc[start:end].copy()

        # Run classification task for the batch
        df_batch = classify_texts(df_batch)

        # Concatenate results into the full dataframe
        all_results = pd.concat([all_results, df_batch], ignore_index=True)

    # Measure total time taken
    end_time = time.time()
    elapsed_time = end_time - start_time
    print(f"Time taken to process {len(df)} records: {elapsed_time:.2f} seconds")

    # Save final combined results to a CSV file
    all_results.to_csv('final_classification_output.csv', index=False)
    print("Final results saved to final_classification_output.csv.")

# Running the batch process with batch size of 100
process_batches(df, batch_size=100)

# Display the first few rows of the final output
df_final = pd.read_csv('final_classification_output.csv')
print("First few rows of the final saved output:")
print(df_final.head())
