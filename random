# Import necessary libraries
import pandas as pd
import spacy
from collections import Counter
import matplotlib.pyplot as plt
import seaborn as sns
import re
import string
from spacy.lang.en.stop_words import STOP_WORDS
import gensim
import pyLDAvis.gensim_models as gensimvis
import pyLDAvis
from gensim.models import CoherenceModel
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# Load Spacy model and data
nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])
df_salesforce = pd.read_excel('/mnt/data/C04CEE5D-B307-4900-9397-558F2770CC69.xlsx')

# Function to check missing values, column types, and add dataset information
def check_missing_and_categorize(df):
    column_info = {'Column': [], 'Type': [], 'Missing Values': [], 'Missing Percentage': []}
    total_rows = len(df)
    total_columns = len(df.columns)
    memory_usage = df.memory_usage(deep=True).sum() / (1024**2)  # Memory in MB

    for col in df.columns:
        if df[col].dtype == 'object':
            column_type = 'text'
        elif pd.api.types.is_numeric_dtype(df[col]):
            column_type = 'numerical'
        else:
            column_type = 'both'  # In case there's a mixed type (though rare)

        # Check for missing values
        if column_type == 'text':
            missing_count = df[col].isna().sum() + (df[col] == '').sum()
        else:
            missing_count = df[col].isna().sum()

        missing_percentage = (missing_count / total_rows) * 100
        column_info['Column'].append(col)
        column_info['Type'].append(column_type)
        column_info['Missing Values'].append(missing_count)
        column_info['Missing Percentage'].append(missing_percentage)

    missing_values_df = pd.DataFrame(column_info)
    dataset_summary = pd.DataFrame({
        'Dataset Information': ['Total Rows', 'Total Columns', 'Memory Usage (MB)'],
        'Value': [total_rows, total_columns, f'{memory_usage:.2f} MB']
    })
    return missing_values_df, dataset_summary

# Call the function to check missing values
missing_values_df, dataset_summary = check_missing_and_categorize(df_salesforce)
print("\n==== DATASET SUMMARY ====")
print(dataset_summary.to_string(index=False))
print("\n==== MISSING VALUES SUMMARY ====")
print(missing_values_df.to_string(index=False))

# Concatenate text columns into a single string variable
df_salesforce['combined_text'] = df_salesforce[['record_comment_text', 'executive_summary_text', 'win_loss_comments_text']].apply(lambda x: ' '.join(x.dropna()), axis=1)

# Define stopwords, additional stopwords, and multi-word phrases
df_stopwords = pd.read_csv('list_stopwords.csv')
stopword_list = df_stopwords['Words'].to_list()

for word in stopword_list:
    STOP_WORDS.add(word)
    nlp.vocab[word].is_stop = True

extra_stopwords = set(['d', 'ir', '--', '---', 't', 'k'])
for word in extra_stopwords:
    STOP_WORDS.add(word)
    nlp.vocab[word].is_stop = True

multi_word_phrases = {
    "account open": ["account open", "open account", "account opening", "open accounts"],
    "client acquisition": ["acquired new client", "acquiring new clients"],
    "new account": ["account new", "open new account"]
}

# Full text preprocessing pipeline
def full_preprocess(text, word_counter=None, min_freq=2):
    contractions_dict = {"ain't": "are not", "can't": "cannot"}  # Add more as required
    for word, replacement in contractions_dict.items():
        text = text.replace(word, replacement)
    
    text = text.lower()
    
    for normalized, variants in multi_word_phrases.items():
        for variant in variants:
            text = text.replace(variant, normalized)
    
    text = re.sub(r'\d+', '', text)
    text = re.sub(r'[^\w\s]', '', text)
    text = ' '.join([token.text for token in nlp(text) if not token.is_stop])
    text = ' '.join([token.lemma_ for token in nlp(text)])
    
    if word_counter:
        text = ' '.join([word for word in text.split() if word_counter[word] >= min_freq])

    return text

# Preprocess and tokenize the text
non_rst_cleaned = df_salesforce[df_salesforce['combined_text'].notna() & (df_salesforce['combined_text'] != '')]
non_rst_cleaned['processed_text'] = non_rst_cleaned['combined_text'].apply(lambda x: full_preprocess(x))

# Function to apply LDA after bigram and trigram creation
def apply_lda(processed_texts, num_topics=10, passes=10):
    # Create bigram and trigram models
    bigram = gensim.models.Phrases(processed_texts, min_count=10)
    trigram = gensim.models.Phrases(bigram[processed_texts], min_count=10)

    # Add bigrams and trigrams to texts
    processed_texts_with_ngrams = []
    for idx in range(len(processed_texts)):
        trigram_tokens = trigram[bigram[processed_texts[idx]]]
        processed_texts_with_ngrams.append(trigram_tokens)
    
    # Create dictionary and corpus
    dictionary = gensim.corpora.Dictionary(processed_texts_with_ngrams)
    dictionary.filter_extremes(no_below=5, no_above=0.5)
    corpus = [dictionary.doc2bow(text) for text in processed_texts_with_ngrams]

    # Train LDA model
    lda_model = gensim.models.LdaMulticore(
        corpus=corpus,
        id2word=dictionary,
        num_topics=num_topics,
        random_state=100,
        chunksize=100,
        passes=passes,
        alpha='asymmetric',
        eta='auto'
    )

    return lda_model, corpus, dictionary, processed_texts_with_ngrams

# Function to print ngrams
def print_ngrams(processed_texts_with_ngrams):
    all_words = ' '.join([' '.join(text) for text in processed_texts_with_ngrams])
    unigram_freq = get_ngram_freq(all_words.split(), 1)
    bigram_freq = get_ngram_freq(all_words.split(), 2)
    trigram_freq = get_ngram_freq(all_words.split(), 3)
    
    print("Unigrams Feeding into LDA:")
    display_ngrams(unigram_freq, "Top Unigrams")
    
    print("Bigrams Feeding into LDA:")
    display_ngrams(bigram_freq, "Top Bigrams")
    
    print("Trigrams Feeding into LDA:")
    display_ngrams(trigram_freq, "Top Trigrams")

# Function to compute coherence score
def compute_coherence_score(lda_model, processed_texts_with_ngrams, dictionary):
    coherence_model_lda = CoherenceModel(model=lda_model, texts=processed_texts_with_ngrams, dictionary=dictionary, coherence='c_v')
    coherence_score = coherence_model_lda.get_coherence()
    return coherence_score

# Function to visualize topic frequency using a bar graph
def plot_topic_frequency(lda_model, corpus):
    # Get the frequency of each topic in the corpus
    topic_counts = [sum([count for id, count in topic]) for topic in corpus]
    
    # Get the topics from the LDA model
    topics = lda_model.show_topics(num_topics=lda_model.num_topics, num_words=5, formatted=False)
    
    # Create a DataFrame for frequency and topics
    topic_data = pd.DataFrame({
        'Topic': [f'Topic {i}' for i in range(len(topics))],
        'Topic Name': [' '.join([word for word, prob in topics[i][1]]) for i in range(len(topics))],  # Properly unpack the (word, probability) pairs
        'Frequency': topic_counts[:len(topics)]  # Match frequency with the number of topics
    })

    # Print Topic Frequency Summary
    print("\nLDA Topic Frequency Summary:\n")
    print(topic_data)

    # Plot the bar graph
    sns.barplot(x='Topic Name', y='Frequency', data=topic_data)
    plt.title("LDA Topic Frequency Summary")
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    plt.show()

# Apply LDA and print the ngrams
processed_texts = non_rst_cleaned['processed_text'].apply(lambda x: x.split()).tolist()  # Tokenization step
lda_model, corpus, dictionary, processed_texts_with_ngrams = apply_lda(processed_texts, num_topics=10)

# Print unigrams, bigrams, and trigrams used in LDA
print_ngrams(processed_texts_with_ngrams)

# Compute and print the coherence score
coherence_score = compute_coherence_score(lda_model, processed_texts_with_ngrams, dictionary)
print(f"\nCoherence Score for the LDA Model: {coherence_score:.4f}")

# Visualize the topic frequency with a bar chart
plot_topic_frequency(lda_model, corpus)
