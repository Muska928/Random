import pandas as pd
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from tqdm import tqdm
import torch
from transformers import BitsAndBytesConfig

# Define the function to extract information from the generated text
def extract_information(text):
    lines = text.split("\n")
    topic_alignment = "Not available"
    category = "Not available"
    explanation = "Not available"
    
    for line in lines:
        if line.startswith("Topic Alignment:"):
            topic_alignment = line.split(": ")[1]
        elif line.startswith("Category:"):
            category = line.split(": ")[1]
        elif line.startswith("Explanation:"):
            explanation = line.split(": ")[1]
    
    return topic_alignment, category, explanation

# Load the dataset in chunks
chunk_size = 1000  # Can be adjusted based on system memory
df_iterator = pd.read_excel('/mnt/data/Salesforce_Deals_Text_Analytics.xlsx', chunksize=chunk_size)

# Load Mistral model and tokenizer
model_path = "mistral/Mistral-7B-Instruct-v0.2"
tokenizer = AutoTokenizer.from_pretrained(
    model_path,
    trust_remote_code=True,
    padding_side="left",
    use_fast=True,
    add_bos_token=True,
    add_eos_token=True
)
tokenizer.pad_token = tokenizer.eos_token

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

model = AutoModelForCausalLM.from_pretrained(
    model_path,
    quantization_config=bnb_config,
    torch_dtype=torch.bfloat16,
    device_map="auto",  # Automatically map to available devices
    trust_remote_code=True,
)

model.config.use_cache = False
model.config.pretraining_tp = 1
model.gradient_checkpointing_enable()

generation_pipeline = pipeline('text-generation', model=model, tokenizer=tokenizer, device=0)


# Function to process each chunk
def process_chunk(df_chunk, batch_size=64):
    results = []
    
    for i in tqdm(range(0, len(df_chunk), batch_size), desc="Processing chunk"):
        batch_texts = df_chunk['combined_text'].iloc[i:i+batch_size].tolist()
        assigned_topics = df_chunk['assigned_topic_name'].iloc[i:i+batch_size].tolist()
        
        # Combine the assigned topic and text into the prompt
        prompts = [updated_prompt_template.format(topic_name=topic, input_text=text) for topic, text in zip(assigned_topics, batch_texts)]
        
        # Process batches asynchronously
        batch_responses = generation_pipeline(prompts, max_new_tokens=50, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)
        
        # Extract generated text from batch_responses
        for response in batch_responses:
            if isinstance(response, list) and len(response) > 0 and 'generated_text' in response[0]:
                generated_text = response[0]['generated_text']
            elif 'generated_text' in response:
                generated_text = response['generated_text']
            else:
                generated_text = "No valid response"
            results.append(generated_text)
    
    # Combine results with chunk
    df_chunk['final_response'] = results
    df_chunk[['topic_alignment', 'category', 'explanation']] = df_chunk['final_response'].apply(lambda x: pd.Series(extract_information(x)))
    
    # Save the processed chunk to a CSV file
    df_chunk.to_csv('/mnt/data/Salesforce_Deals_Text_Analytics_Output_Chunked.csv', mode='a', index=False, header=False)

# Batch size for processing
batch_size = 64  # Adjust based on memory, increase to maximize GPU utilization

# Process each chunk
for df_chunk in df_iterator:
    process_chunk(df_chunk, batch_size)


------------
import pandas as pd
import time
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from tqdm import tqdm
import torch
from transformers import BitsAndBytesConfig

# Define the function to extract information from the generated text
def extract_information(text):
    lines = text.split("\n")
    topic_alignment = "Not available"
    category = "Not available"
    explanation = "Not available"
    
    for line in lines:
        if line.startswith("Topic Alignment:"):
            topic_alignment = line.split(": ")[1]
        elif line.startswith("Category:"):
            category = line.split(": ")[1]
        elif line.startswith("Explanation:"):
            explanation = line.split(": ")[1]
    
    return topic_alignment, category, explanation

# Load the dataset

# Sample the first 1,000 records
df_sample = df_salesforce.head(1000)

# Load Mistral model and tokenizer
model_path = "mistral/Mistral-7B-Instruct-v0.2"
tokenizer = AutoTokenizer.from_pretrained(
    model_path,
    trust_remote_code=True,
    padding_side="left",
    use_fast=True,
    add_bos_token=True,
    add_eos_token=True
)
tokenizer.pad_token = tokenizer.eos_token

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

model = AutoModelForCausalLM.from_pretrained(
    model_path,
    quantization_config=bnb_config,
    torch_dtype=torch.bfloat16,
    device_map="auto",  # Automatically map to available devices
    trust_remote_code=True,
)

model.config.use_cache = False
model.config.pretraining_tp = 1
model.gradient_checkpointing_enable()

generation_pipeline = pipeline('text-generation', model=model, tokenizer=tokenizer, device=0)

# Define the updated prompt template
updated_prompt_template = """
You are an AI tasked with verifying if the assigned topic name matches the content of the text. Additionally, classify the content into one of the following categories: Strong Client Relationship and Market Engagement, Effective Pricing and Profitability, Operational Excellence and Process Efficiency, Product and Service Delivery, or Innovative and Tailored Solutions.

Here are the categories and their descriptions:
1. Strong Client Relationship and Market Engagement: Deals won by fostering strong client relationships and market engagement.
2. Effective Pricing and Profitability: Deals structured for profitability and competitive pricing.
3. Operational Excellence and Process Efficiency: Deals secured through reliable and efficient processes.
4. Product and Service Delivery: Deals won by meeting specific client needs with product or service offerings.
5. Innovative and Tailored Solutions: Deals won through innovative and customized solutions.

Given the following text, identify if the assigned topic name aligns with the content and categorize the deal:

Assigned Topic Name: {topic_name}
Text: {input_text}

Provide your response in the following format:
Topic Alignment: [Yes/No]
Category: [Category Name]
Explanation: [Brief explanation of why the category was chosen]
"""

# Function to process the sample
def process_sample(df_sample, batch_size=64):
    results = []
    
    for i in tqdm(range(0, len(df_sample), batch_size), desc="Processing sample"):
        batch_texts = df_sample['combined_text'].iloc[i:i+batch_size].tolist()
        assigned_topics = df_sample['assigned_topic_name'].iloc[i:i+batch_size].tolist()
        
        # Combine the assigned topic and text into the prompt
        prompts = [updated_prompt_template.format(topic_name=topic, input_text=text) for topic, text in zip(assigned_topics, batch_texts)]
        
        # Process batches asynchronously
        batch_responses = generation_pipeline(prompts, max_new_tokens=50, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)
        
        # Extract generated text from batch_responses
        for response in batch_responses:
            if isinstance(response, list) and len(response) > 0 and 'generated_text' in response[0]:
                generated_text = response[0]['generated_text']
            elif 'generated_text' in response:
                generated_text = response['generated_text']
            else:
                generated_text = "No valid response"
            results.append(generated_text)
    
    # Combine results with the sample
    df_sample['final_response'] = results
    df_sample[['topic_alignment', 'category', 'explanation']] = df_sample['final_response'].apply(lambda x: pd.Series(extract_information(x)))
    
    # Print the sample results (or save them to a file)
    print(df_sample[['topic_alignment', 'category', 'explanation']].head())

# Measure the time taken for the sample
start_time = time.time()
process_sample(df_sample)
end_time = time.time()

# Calculate the time taken
time_taken = end_time - start_time
print(f"Time taken for 1,000 records: {time_taken} seconds")
