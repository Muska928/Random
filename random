import pandas as pd
import spacy
from collections import Counter
import gensim
import re
from gensim.models.phrases import Phrases, Phraser
import pyLDAvis.gensim_models as gensimvis
import pyLDAvis
import matplotlib.pyplot as plt
import seaborn as sns

# Load Spacy model
nlp = spacy.load('en_core_web_md', disable=['parser', 'ner'])

# Function to preprocess text
def preprocess(document):
    document = document.lower()
    document = re.sub(r'[^a-zA-Z\s]', '', document)
    document = re.sub(r'\s+', ' ', document).strip()
    doc = nlp(document)
    words = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct and len(token.lemma_) > 2]
    return words

# Function to get n-gram frequency
def get_ngram_freqs(tokens_list, n=1):
    if n == 1:
        return Counter(tokens_list).most_common(20)
    else:
        ngrams = zip(*[tokens_list[i:] for i in range(n)])
        return Counter([' '.join(ngram) for ngram in ngrams]).most_common(20)

# Function to print frequency table and plot n-grams with cool-warm color palette
def display_ngrams(ngram_freq, title):
    # Create a DataFrame to display as a table
    ngram_df = pd.DataFrame(ngram_freq, columns=['ngram', 'frequency'])
    
    # Print frequency table
    print(f"\n{title} Frequency Table:\n")
    print(ngram_df.to_string(index=False))
    
    # Plot the top 20 n-grams using Seaborn
    plt.figure(figsize=(12, 8))
    sns.barplot(x='ngram', y='frequency', data=ngram_df, palette='coolwarm')
    plt.xticks(rotation=45, ha='right', fontsize=12)
    plt.yticks(fontsize=12)
    plt.title(f'Top 20 {title}', fontsize=16, weight='bold')
    plt.xlabel('N-gram', fontsize=14, weight='bold')
    plt.ylabel('Frequency', fontsize=14, weight='bold')
    
    # Add data labels for each bar
    for index, value in enumerate(ngram_df['frequency']):
        plt.text(index, value + 1, str(value), ha='center', fontsize=12)

    plt.tight_layout()
    plt.show()

# Function to analyze and display n-grams for each dataset
def analyze_ngrams(df_subset, title):
    all_tokens = [token for sublist in df_subset['processed_text'] for token in sublist]
    
    print(f"\n--- {title} ---\n")
    
    # Unigrams
    unigram_freq = get_ngram_freqs(all_tokens, n=1)
    display_ngrams(unigram_freq, title=f'Unigrams for {title}')
    
    # Bigrams
    bigram_freq = get_ngram_freqs(all_tokens, n=2)
    display_ngrams(bigram_freq, title=f'Bigrams for {title}')
    
    # Trigrams
    trigram_freq = get_ngram_freqs(all_tokens, n=3)
    display_ngrams(trigram_freq, title=f'Trigrams for {title}')
    
    # Four-Grams
    fourgram_freq = get_ngram_freqs(all_tokens, n=4)
    display_ngrams(fourgram_freq, title=f'Four-Grams for {title}')

# Read data from Salesforce analytics (replace with your file path)
df_salesforce = pd.read_excel('Data/exclude_won_rst.xlsx')

# Concatenate text columns into a single string variable
df_salesforce['combined_text'] = df_salesforce[['record_comment_text', 
                                                'executive_summary_text', 
                                                'win_loss_reason_text', 
                                                'win_loss_comments_text']].apply(lambda x: ' '.join(x.dropna()), axis=1)

# Preprocess the combined text
df_salesforce['processed_text'] = df_salesforce['combined_text'].apply(preprocess)

# Token count for each row
df_salesforce['token_count'] = df_salesforce['processed_text'].apply(len)

# Separate the data into RST and Non-RST datasets
rst_data = df_salesforce[df_salesforce['account_eci'] == 'RST']
non_rst_data = df_salesforce[df_salesforce['account_eci'] != 'RST']

# Separate data based on token length
token_len_le_3_data = df_salesforce[df_salesforce['token_count'] <= 3]
token_len_ge_4_data = df_salesforce[df_salesforce['token_count'] >= 4]

# List of datasets to analyze
datasets = [
    (rst_data, "RST Data"),
    (non_rst_data, "Non-RST Data"),
    (token_len_le_3_data, "Token Length <= 3"),
    (token_len_ge_4_data, "Token Length >= 4")
]

# Analyze n-grams and LDA for each dataset
def build_phrases(processed_texts):
    bigram = Phrases(processed_texts, min_count=10)
    trigram = Phrases(bigram[processed_texts], min_count=10)
    return Phraser(bigram), Phraser(trigram)

def run_lda_and_visualize(df_subset, category):
    # Create dictionary and corpus for LDA
    dictionary = gensim.corpora.Dictionary(df_subset['processed_text'])
    corpus = [dictionary.doc2bow(text) for text in df_subset['processed_text']]
    
    # Train LDA model
    lda_model = gensim.models.LdaMulticore(corpus=corpus, num_topics=5, id2word=dictionary, passes=10)
    
    # Print topics for the current category
    print(f"\n--- Topics for {category} ---\n")
    for idx, topic in lda_model.print_topics(-1):
        print(f"Topic {idx}: {topic}")
    
    # Visualize the topics with PyLDAvis
    vis = gensimvis.prepare(lda_model, corpus, dictionary)
    pyLDAvis.display(vis)

# Iterate over each dataset for analysis
for df_subset, category in datasets:
    # Analyze n-grams
    analyze_ngrams(df_subset, title=f'N-Gram Frequency for {category}')
    
    # Build bi-grams and tri-grams for phrases
    processed_texts = df_subset['processed_text'].tolist()
    bigram_model, trigram_model = build_phrases(processed_texts)

    for idx, tokens in enumerate(processed_texts):
        bigrams = bigram_model[tokens]
        trigrams = trigram_model[bigrams]
        df_subset['processed_text'][idx] = trigrams

    # Run LDA and PyLDAvis
    run_lda_and_visualize(df_subset, category=category)
