import pandas as pd
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from tqdm import tqdm
import torch
from transformers import BitsAndBytesConfig

# Load your dataset
df = pd.read_excel('/mnt/data/Text_Classification_Data.xlsx')

# Predefined categories for classification
categories = [
    "Strong Client Relationship and Market Engagement",
    "Effective Pricing and Profitability",
    "Operational Excellence and Process Efficiency",
    "Product and Service Delivery",
    "Innovative and Tailored Solutions"
]

# Define the prompt template for classification
classification_prompt_template = """
You are an AI tasked with classifying the following text into one of the following categories: 
1. Strong Client Relationship and Market Engagement
2. Effective Pricing and Profitability
3. Operational Excellence and Process Efficiency
4. Product and Service Delivery
5. Innovative and Tailored Solutions

Text: {input_text}

Based on the content of this text, which category does it belong to?
"""

# Load Mistral model and tokenizer
model_path = "mistral/Mistral-7B-Instruct-v0.2"
tokenizer = AutoTokenizer.from_pretrained(
    model_path, 
    trust_remote_code=True, 
    padding_side="left", 
    use_fast=True, 
    add_bos_token=True, 
    add_eos_token=True
)

tokenizer.pad_token = tokenizer.eos_token

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

model = AutoModelForCausalLM.from_pretrained(
    model_path,
    quantization_config=bnb_config,
    torch_dtype=torch.bfloat16,
    device_map="auto",
    trust_remote_code=True,
)

model.config.use_cache = False
model.gradient_checkpointing_enable()

# Pipeline for text generation
generation_pipeline = pipeline('text-generation', model=model, tokenizer=tokenizer)

# Process the dataset in batches
results = []
batch_size = 10

for i in tqdm(range(0, len(df), batch_size), desc="Processing"):
    batch_texts = df['text_column'].iloc[i:i+batch_size].tolist()

    # Prepare prompts for classification
    prompts = [classification_prompt_template.format(input_text=text) for text in batch_texts]
    
    # Generate the classification output from the LLM
    batch_responses = generation_pipeline(prompts, max_new_tokens=50, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)
    
    # Extract the generated classification response
    for response in batch_responses:
        if isinstance(response, list) and len(response) > 0 and 'generated_text' in response[0]:
            generated_text = response[0]['generated_text']
        elif 'generated_text' in response:
            generated_text = response['generated_text']
        else:
            generated_text = "No valid response"
        results.append(generated_text)

# Add the classification results to the DataFrame
df['classification'] = results

# Save the results to a CSV file
df.to_csv('/mnt/data/Text_Classification_Results.csv', index=False)

# Display the classification output
print(df[['text_column', 'classification']])
