import pandas as pd
import spacy
from collections import Counter
import matplotlib.pyplot as plt
import seaborn as sns
import re
import gensim
from datetime import datetime
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import string
from spacy.lang.en.stop_words import STOP_WORDS

# Load Spacy model with parser enabled for dependency parsing
nlp = spacy.load('en_core_web_sm', disable=['ner'])
nlp.max_length = 2000000  # Increase SpaCy's max_length to handle larger texts

# Load data
df_salesforce = pd.read_excel('Data/Closed_Won Reasons/Closed_Won_Product_Capability.xlsx')

# Concatenate text columns into a single string variable
df_salesforce['combined_text'] = df_salesforce[['record_comment_text', 
                                                'executive_summary_text', 
                                                'win_loss_comments_text']].apply(lambda x: ' '.join(x.dropna()), axis=1)

# Import custom stopwords
df_stopwords = pd.read_csv('list_stopwords.csv')
stopword_list = df_stopwords['Words'].to_list()

# Adding stopwords to spacy module
for word in stopword_list:
    STOP_WORDS.add(word)
    nlp.vocab[word].is_stop = True

# Contractions and abbreviations mapping
contractions_dict = {
    # (Same contractions dictionary as before)
    "ain't": "are not", "'s":" is", "aren't": "are not", "can't": "cannot", "can't've": "cannot have",
    "'cause": "because", "could've": "could have", "couldn't": "could not", "couldn't've": "could not have",
    "didn't": "did not", "doesn't": "does not", "don't": "do not", "hadn't": "had not", "hadn't've": "had not have",
    # More contractions...
}

# Function to expand contractions
contractions_re = re.compile('(%s)' % '|'.join(contractions_dict.keys()))
def expand_contractions(text):
    def replace(match):
        return contractions_dict[match.group(0)]
    return contractions_re.sub(replace, text)

# Function to remove digits
def remove_digits(text):
    return re.sub(r'\d+', '', text)

# Function to remove extra punctuation/marks
def remove_extra_marks(text):
    return re.sub(r'[^\w\s]', '', text)

# Function to remove stopwords
def remove_stopwords(text):
    doc = nlp(text)
    return ' '.join([token.text for token in doc if not token.is_stop])

# **Additional Cleanup Function** to handle erroneous stopwords like 'th'
def remove_invalid_tokens(text):
    invalid_tokens = ['th']  # Add more if necessary
    tokens = text.split()
    return ' '.join([word for word in tokens if word not in invalid_tokens])

# Multi-word lemmatization using Spacy dependency parsing
def multi_word_lemmatizer(text):
    doc = nlp(text)
    lemmatized_tokens = []
    
    for token in doc:
        # Lemmatizing based on dependency parsing for compound and descriptive phrases
        if token.dep_ in ['compound', 'amod', 'nsubj', 'dobj', 'pobj']:
            lemmatized_tokens.append(f"{token.head.lemma_} {token.lemma_}")
        else:
            lemmatized_tokens.append(token.lemma_)
    
    return ' '.join(lemmatized_tokens)

# Full preprocessing pipeline
def full_preprocess(text):
    text = expand_contractions(text)  # Step 1: Expand contractions
    text = text.lower()  # Step 2: Convert to lowercase
    text = remove_digits(text)  # Step 3: Remove digits
    text = remove_extra_marks(text)  # Step 4: Remove extra marks
    text = remove_stopwords(text)  # Step 5: Remove stopwords
    text = multi_word_lemmatizer(text)  # Step 6: Multi-word lemmatization
    text = remove_invalid_tokens(text)  # Step 7: Remove invalid tokens (like 'th')
    return text

# Example text
first_row_text = "I'll buy 20% of Union Bank's shares on 5th May! It's a great 5-year opportunity, isn't it?"

# Print the original text
print("Original Text:\n", first_row_text)

# Apply full preprocessing on the first row
processed_first_row = full_preprocess(first_row_text)

# Print the processed text
print("\nProcessed Text:\n", processed_first_row)

# Apply preprocessing to the Non-RST data
non_rst_data = df_salesforce[df_salesforce['account_eci'] != 'RST']
non_rst_data.loc[:, 'processed_text'] = non_rst_data['combined_text'].apply(full_preprocess)

# Print first 5 processed Non-RST records
print("\nProcessed Non-RST Deals (first 5 rows):\n", non_rst_data[['combined_text', 'processed_text']].head())

# Function to get n-gram frequency
def get_ngrams(corpus, n=1):
    """Returns the n most common n-grams from a corpus"""
    ngrams = zip(*[corpus[i:] for i in range(n)])
    return Counter([' '.join(ngram) for ngram in ngrams]).most_common(30)  # Top 30 n-grams

# Function to plot n-grams
def plot_ngrams(ngram_freq, title):
    ngram_df = pd.DataFrame(ngram_freq, columns=['ngram', 'frequency'])
    
    # Plot using Seaborn
    plt.figure(figsize=(12, 8))
    sns.barplot(x='ngram', y='frequency', data=ngram_df, palette='coolwarm')
    
    # Add frequency numbers on top of bars
    for index, value in enumerate(ngram_df['frequency']):
        plt.text(index, value, str(value), ha='center', va='bottom', fontsize=10)
    
    plt.xticks(rotation=45, ha='right', fontsize=12)
    plt.yticks(fontsize=12)
    plt.title(f'Top 30 {title}', fontsize=16, weight='bold')
    plt.xlabel(f'{title}', fontsize=14, weight='bold')
    plt.ylabel('Frequency', fontsize=14, weight='bold')
    plt.tight_layout()
    plt.show()

# Join all processed text into a single corpus
all_tokens = ' '.join(non_rst_data['processed_text']).split()

# Unigram, Bigram, Trigram Frequency Analysis
unigram_freq = get_ngrams(all_tokens, n=1)
bigram_freq = get_ngrams(all_tokens, n=2)
trigram_freq = get_ngrams(all_tokens, n=3)

# Plot n-grams
plot_ngrams(unigram_freq, 'Unigrams')
plot_ngrams(bigram_freq, 'Bigrams')
plot_ngrams(trigram_freq, 'Trigrams')
