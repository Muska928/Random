import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import OneHotEncoder, KBinsDiscretizer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, r2_score

# 1. Load Excel file
df = pd.read_excel("your_excel_file.xlsx")  # Replace with actual file name

# 2. Clean and convert date columns
df["as_of_date"] = pd.to_datetime(df["as_of_date"].astype(str), format="%Y%m%d", errors="coerce")
df["max_as_of_date"] = pd.to_datetime(df["max_as_of_date"].astype(str), format="%Y%m%d", errors="coerce")

# 3. EDA Summary
print("\nColumn Names:")
print(df.columns.tolist())
print(f"\nDataset Shape: {df.shape[0]} rows × {df.shape[1]} columns")

print("\nMissing Value Check:")
missing = df.isnull().sum()
missing = missing[missing > 0]
if not missing.empty:
    print(missing)
else:
    print("No missing values found.")

# 4. Date Range Info
as_of_min = df["as_of_date"].min()
as_of_max = df["as_of_date"].max()
if pd.isnull(as_of_min) or pd.isnull(as_of_max):
    print("\nData Time Range: Invalid dates found (NaT)")
else:
    print(f"\nData Time Range: From {as_of_min.date()} to {as_of_max.date()}")

# 5. Drop rows only where target is missing
target = "pli_active_cycle_time"
df = df.dropna(subset=[target])
print(f"\nAfter dropping rows with missing target: {df.shape[0]} rows remain")

# 6. Data Type Summary
print("\nData Type Summary:")
print(df.dtypes.value_counts())

# 7. Identify column types
excluded_cols = [target, 'as_of_date', 'max_as_of_date']
binary_features = [col for col in df.columns if df[col].nunique() == 2 and df[col].dtype in [int, float]]
categorical_features = [col for col in df.columns if (df[col].dtype == 'object' or df[col].nunique() < 15) and col not in excluded_cols]
numeric_features = [col for col in df.columns if df[col].dtype in [int, float] and col not in binary_features + excluded_cols]

print("\nFeature Type Breakdown:")
print(f"- Binary features     : {len(binary_features)}")
print(f"- Categorical features: {len(categorical_features)}")
print(f"- Numeric features    : {len(numeric_features)}")

# Optional: Preview feature names
print("\nBinary Features:", binary_features)
print("\nCategorical Features:", categorical_features)
print("\nNumeric Features:", numeric_features)

# 8. Define features for model
binary_categorical_features = binary_features
multi_categorical_features = categorical_features

# Remove duplicates and excluded columns
exclude_cols = ['grade', 'prod_tx', 'num_active_flags']
categorical_model_features = list(set(binary_categorical_features + multi_categorical_features))
categorical_model_features = [col for col in categorical_model_features if col not in exclude_cols]

# 9. Value counts for binary flags
print("\nValue Distribution of Binary Flags:")
for col in binary_categorical_features:
    print(f"{col}:\n{df[col].value_counts(dropna=False)}\n")

# 10. Feature Engineering
df["num_active_flags"] = df[binary_categorical_features].sum(axis=1)

# 11. Correlation with target
print("\nCorrelation of binary features with cycle time:")
corr_cols = binary_categorical_features + [target]
correlations = df[corr_cols].corr()[target].sort_values(key=abs, ascending=False)
print(correlations)

# 12. Final features for modeling
model_features = categorical_model_features + ["num_active_flags"]
X = df[model_features]
y = df[target]

# 13. Preprocessing and Modeling Pipeline
preprocessor = ColumnTransformer([
    ("cat", OneHotEncoder(drop='first', handle_unknown='ignore'), model_features)
])

pipeline = Pipeline([
    ("preprocessor", preprocessor),
    ("regressor", LinearRegression())
])

# 14. Train-Test Split
if df.shape[0] < 10:
    raise ValueError("🚫 Not enough data to train/test split after dropping nulls. Check source file.")
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 15. Fit Model
pipeline.fit(X_train, y_train)

# 16. Predictions
y_pred = pipeline.predict(X_test)

# 17. Evaluation
print("\nModel Evaluation:")
print(f"MAE      : {mean_absolute_error(y_test, y_pred):.2f}")
print(f"R² Score : {r2_score(y_test, y_pred):.2f}")

# 18. Predict on full data
df["predicted_cycle_time"] = pipeline.predict(X)

# 19. Complexity Score Binning
binning = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='quantile')
df["complexity_score"] = binning.fit_transform(df[["predicted_cycle_time"]]).astype(int) + 1

# 20. Chart - Actual vs Predicted
plt.figure(figsize=(8, 5))
sns.scatterplot(x=y_test, y=y_pred)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
plt.xlabel("Actual Cycle Time")
plt.ylabel("Predicted Cycle Time")
plt.title("Actual vs Predicted")
plt.grid(True)
plt.tight_layout()
plt.show()

# 21. Chart - Complexity Score Distribution
plt.figure(figsize=(6, 4))
sns.countplot(x="complexity_score", data=df)
plt.title("Complexity Score Distribution")
plt.xlabel("Complexity (1 = Low, 5 = High)")
plt.tight_layout()
plt.show()

# 22. Feature Importance
regressor = pipeline.named_steps["regressor"]
onehot = pipeline.named_steps["preprocessor"].named_transformers_["cat"]
feature_names = onehot.get_feature_names_out(model_features)

coef_df = pd.DataFrame({
    "Feature": feature_names,
    "Coefficient": regressor.coef_
}).sort_values(by="Coefficient", key=abs, ascending=False)

print("\nTop Feature Coefficients:")
print(coef_df.head(10))

# Chart - Top Feature Importance
top_n = 15
plt.figure(figsize=(10, 6))
sns.barplot(x="Coefficient", y="Feature", data=coef_df.head(top_n), palette="viridis")
plt.title(f"Top {top_n} Feature Importances")
plt.xlabel("Coefficient Value")
plt.ylabel("Feature")
plt.grid(True, axis='x')
plt.tight_layout()
plt.show()

# 23. Save Outputs
df.to_excel("pli_with_complexity_scores.xlsx", index=False)
coef_df.to_excel("feature_importance_linear_model.xlsx", index=False)
print("\n📁 Excel files saved: 'pli_with_complexity_scores.xlsx', 'feature_importance_linear_model.xlsx'")

