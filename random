import pandas as pd
import spacy
from collections import Counter
import matplotlib.pyplot as plt
import seaborn as sns
import re
import string
from spacy.lang.en.stop_words import STOP_WORDS
import gensim
import pyLDAvis.gensim_models as gensimvis
import pyLDAvis

# Load Spacy model
nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])
nlp.max_length = 2000000  # Increase SpaCy's max_length to handle larger texts

# Load data
df_salesforce = pd.read_excel('Data/Closed_Won Reasons/Closed_Won_Product_Capability.xlsx')

# Concatenate text columns into a single string variable
df_salesforce['combined_text'] = df_salesforce[['record_comment_text', 
                                                'executive_summary_text', 
                                                'win_loss_comments_text']].apply(lambda x: ' '.join(x.dropna()), axis=1)

# Import custom stopwords
df_stopwords = pd.read_csv('list_stopwords.csv')
stopword_list = df_stopwords['Words'].to_list()

# Adding stopwords to spacy module
for word in stopword_list:
    STOP_WORDS.add(word)
    nlp.vocab[word].is_stop = True

# Additional stopwords to filter out single characters and unwanted terms
extra_stopwords = set(['d', 'ir', '--', '---', '-', 'new', 'account', 't', 'deal', 'represent'])
for word in extra_stopwords:
    STOP_WORDS.add(word)
    nlp.vocab[word].is_stop = True

# Contractions and abbreviations mapping
contractions_dict = {
    "ain't": "are not", "'s": " is", "aren't": "are not", "can't": "cannot",
    "'cause": "because", "could've": "could have", "couldn't": "could not", "didn't": "did not", "doesn't": "does not", 
    "don't": "do not", "hadn't": "had not", "hasn't": "has not", "haven't": "have not", "he'd": "he would",
    "he'll": "he will", "I'd": "I would", "I'll": "I will", "I'm": "I am", "I've": "I have", "isn't": "is not", 
    "it'd": "it would", "it'll": "it will", "let's": "let us", "ma'am": "madam", "might've": "might have", 
    "must've": "must have", "needn't": "need not", "o'clock": "of the clock", "shan't": "shall not", "she'd": "she would", 
    "she'll": "she will", "should've": "should have", "shouldn't": "should not", "that'd": "that would", 
    "there'd": "there would", "they'd": "they would", "they'll": "they will", "they're": "they are", "they've": "they have",
}

# Function to expand contractions
contractions_re = re.compile('(%s)' % '|'.join(contractions_dict.keys()))
def expand_contractions(text):
    def replace(match):
        return contractions_dict[match.group(0)]
    return contractions_re.sub(replace, text)

# **Function to remove unwanted dashes and special characters**
def remove_hyphens_and_special_chars(text):
    return re.sub(r'\s*[-–—]+\s*', ' ', text)  # Replace multiple dashes (e.g., --, ---, -) with a space

# Function to normalize multi-word phrases
multi_word_phrases = {
    "account open": ["open account", "account opening"],
    "new client": ["client new"],
    "market disruption": ["disruption market"],
    "client acquisition": ["acquired new client", "acquiring new clients"],
}

def normalize_phrases(text):
    for normalized, variants in multi_word_phrases.items():
        for variant in variants:
            text = text.replace(variant, normalized)
    return text

# Function to remove digits
def remove_digits(text):
    return re.sub(r'\d+', '', text)

# Function to remove extra punctuation/marks
def remove_extra_marks(text):
    return re.sub(r'[^\w\s]', '', text)

# **Function to remove single-character tokens**
def remove_single_char_tokens(text):
    return ' '.join([word for word in text.split() if len(word) > 1])

# Function to remove stopwords
def remove_stopwords(text):
    doc = nlp(text)
    return ' '.join([token.text for token in doc if not token.is_stop])

# Function to remove erroneous tokens like 'th'
def remove_invalid_tokens(text):
    invalid_tokens = ['th']
    tokens = text.split()
    return ' '.join([word for word in tokens if word not in invalid_tokens])

# Function to lemmatize
def lemmatize(text):
    doc = nlp(text)
    return ' '.join([token.lemma_ for token in doc])

# **Rare word filtering**
def remove_rare_words(text, word_counter, min_freq=2):
    tokens = text.split()
    return ' '.join([word for word in tokens if word_counter[word] >= min_freq])

# Full preprocessing pipeline with rare word removal and character filtering
def full_preprocess(text, word_counter=None, min_freq=2):
    text = expand_contractions(text)  # Step 1: Expand contractions
    text = remove_hyphens_and_special_chars(text)  # Step 2: Remove hyphens and unwanted characters
    text = text.lower()  # Step 3: Convert to lowercase
    text = remove_digits(text)  # Step 4: Remove digits
    text = remove_extra_marks(text)  # Step 5: Remove extra marks
    text = normalize_phrases(text)  # Step 6: Normalize multi-word phrases
    text = remove_stopwords(text)  # Step 7: Remove stopwords
    text = lemmatize(text)  # Step 8: Lemmatization
    text = remove_invalid_tokens(text)  # Step 9: Remove invalid tokens (like 'th')
    text = remove_single_char_tokens(text)  # Step 10: Remove single-character tokens
    
    # Remove rare words based on frequency
    if word_counter is not None:
        text = remove_rare_words(text, word_counter, min_freq=min_freq)
    
    return text

# ---- N-gram Frequency and Plotting (Non-RST Data) ----

# Filter Non-RST data
non_rst_data = df_salesforce[df_salesforce['account_eci'] != 'RST']

# Step to create word frequency counter across the entire dataset for rare word removal
all_words = ' '.join(non_rst_data['combined_text']).split()
word_counter = Counter(all_words)

# Apply full preprocessing to Non-RST data with rare word removal
non_rst_data['processed_text'] = non_rst_data['combined_text'].apply(lambda x: full_preprocess(x, word_counter=word_counter, min_freq=2))

# Tokenize the processed text
def tokenize_text(text):
    return text.split()

# Get all tokens from the processed Non-RST deals
all_tokens_non_rst = [token for text in non_rst_data['processed_text'] for token in tokenize_text(text)]

# Function to get n-gram frequency for Non-RST data
def get_ngram_freqs(tokens_list, n=1):
    ngrams = zip(*[tokens_list[i:] for i in range(n)])
    return Counter([' '.join(ngram) for ngram in ngrams]).most_common(30)

# **Function to save n-gram frequency tables to Excel**
def save_ngram_freqs_to_excel(ngram_freqs, sheet_name, writer):
    ngram_df = pd.DataFrame(ngram_freqs, columns=['ngram', 'frequency'])
    ngram_df.to_excel(writer, sheet_name=sheet_name, index=False)

# Save n-grams to an Excel file
with pd.ExcelWriter('ngram_frequencies_non_rst.xlsx') as writer:
    unigram_freq_non_rst = get_ngram_freqs(all_tokens_non_rst, n=1)
    bigram_freq_non_rst = get_ngram_freqs(all_tokens_non_rst, n=2)
    trigram_freq_non_rst = get_ngram_freqs(all_tokens_non_rst, n=3)

    # Save each frequency table to a separate sheet
    save_ngram_freqs_to_excel(unigram_freq_non_rst, 'Unigrams', writer)
    save_ngram_freqs_to_excel(bigram_freq_non_rst, 'Bigrams', writer)
    save_ngram_freqs_to_excel(trigram_freq_non_rst, 'Trigrams', writer)

# Function to display and plot n-grams for Non-RST data with sum of frequencies
def display_ngrams(ngram_freq, title):
    ngram_df = pd.DataFrame(ngram_freq, columns=['ngram', 'frequency'])
    ngram_sum = ngram_df['frequency'].sum()  # Calculate sum of frequencies
    
    print(f"\n{title} Frequency Table (Non-RST):")
    print(ngram_df.to_string(index=False))
    print(f"\nTotal Frequency Sum: {ngram_sum}")

    plt.figure(figsize=(12, 8))
    sns.barplot(x='ngram', y='frequency', data=ngram_df, palette='coolwarm')

    # Add frequency numbers on top of bars
    for index, value in enumerate(ngram_df['frequency']):
        plt.text(index, value, str(value), ha='center', va='bottom', fontsize=10)

    plt.xticks(rotation=45, ha='right', fontsize=12)
    plt.title(f'Top 30 {title} (Non-RST)', fontsize=16, weight='bold')
    plt.tight_layout()
    plt.show()

# Display and plot unigrams, bigrams, trigrams for Non-RST deals
display_ngrams(unigram_freq_non_rst, 'Unigrams')
display_ngrams(bigram_freq_non_rst, 'Bigrams')
display_ngrams(trigram_freq_non_rst, 'Trigrams')

# ---- LDA Topic Modeling ----

# Preprocess text for LDA
def preprocess_lda(document):
    document = document.lower()
    document = re.sub(r'[^a-zA-Z\s]', '', document)
    document = re.sub(r'\s+', ' ', document).strip()
    doc = nlp(document)
    words = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct and len(token.lemma_) > 2]
    return words

# Preprocess non-RST data for LDA
non_rst_gt_3_tokens = non_rst_data.loc[non_rst_data['processed_text'].apply(lambda x: len(x.split()) > 3)].copy()
non_rst_gt_3_tokens['processed_lda'] = non_rst_gt_3_tokens['processed_text'].map(preprocess_lda)
processed_texts = non_rst_gt_3_tokens['processed_lda'].tolist()

# Build bigram and trigram models
bigram = gensim.models.Phrases(processed_texts, min_count=10)
trigram = gensim.models.Phrases(bigram[processed_texts], min_count=10)

# Add bigrams and trigrams
for idx in range(len(processed_texts)):
    for token in trigram[bigram[processed_texts[idx]]]:
        if '_' in token:
            processed_texts[idx].append(token)

# Create dictionary and corpus
dictionary = gensim.corpora.Dictionary(processed_texts)
dictionary.filter_extremes(no_below=5, no_above=0.5)
corpus = [dictionary.doc2bow(text) for text in processed_texts]

# Find the optimal number of topics
model_list, coherence_values = compute_coherence_values(dictionary, corpus, processed_texts, limit=20, start=2, step=2)
optimal_model = model_list[coherence_values.index(max(coherence_values))]

# Assign main topic to each document
def get_main_topic(corpus):
    topic_weights = optimal_model[corpus]
    main_topic = max(topic_weights, key=lambda x: x[1])
    return main_topic[0]

non_rst_gt_3_tokens['main_topic'] = [get_main_topic(corp) for corp in corpus]

# Save LDA topic model results
non_rst_gt_3_tokens.to_csv('non_rst_eci_topic_modeling.csv', index=False)

# Visualize topics with pyLDAvis
vis = gensimvis.prepare(optimal_model, corpus, dictionary)
pyLDAvis.save_html(vis, 'lda_visualization.html')

print("LDA Topic Modeling and N-gram frequency analysis completed!")
