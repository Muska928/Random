import pandas as pd
import re
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from tqdm import tqdm
import torch
from transformers import BitsAndBytesConfig

# Load Mistral model and tokenizer from the specified folder with device_map='auto'
model_path = "mistral/Mistral-7B-Instruct-v0.2"

tokenizer = AutoTokenizer.from_pretrained(
    model_path,
    trust_remote_code=True,
    padding_side="left",
    use_fast=True,
    add_bos_token=True,
    add_eos_token=True
)

tokenizer.pad_token = tokenizer.eos_token

compute_dtype = getattr(torch, "float16")
print(compute_dtype)

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

model = AutoModelForCausalLM.from_pretrained(
    model_path,
    quantization_config=bnb_config,
    torch_dtype=torch.bfloat16,
    device_map="auto",
    trust_remote_code=True,
)

model.config.use_cache = False  # silence the warnings
model.config.pretraining_tp = 1
model.gradient_checkpointing_enable()

# Check if GPU is available and set device accordingly
device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

# Sentiment analysis pipeline with the Mistral instruct model
generation_pipeline = pipeline('text-generation', model=model, tokenizer=tokenizer, device=device)

# Maximum length of input tokens
max_input_length = 200

# Batch processing size
batch_size = 10

# Generate the output
results = []
for i in tqdm(range(0, len(df_sample), batch_size), desc="Processing"):
    batch_texts = df_sample['combined_text'].iloc[i:i+batch_size].tolist()
    truncated_texts = [tokenizer.decode(tokenizer.encode(text, max_length=max_input_length, truncation=True), skip_special_tokens=True) for text in batch_texts]
    
    prompts = [one_shot_template.format(input_text=text) for text in truncated_texts]
    
    batch_responses = generation_pipeline(prompts, max_new_tokens=50, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)
    results.extend([response[0]['generated_text'] for response in batch_responses])

# Split the results into separate columns
df_sample['final_response'] = results
df_sample[['LOB', 'Sentiment', 'Content Retrieved']] = df_sample['final_response'].str.extract(
    r'LOB: (.*?)\nSentiment: (.*?)\nContent Retrieved: (.*)', expand=True
)

# Save the results to an Excel file
df_sample.to_excel('/mnt/data/Salesforce_Deals_Text_Analytics_Output.xlsx', index=False)
