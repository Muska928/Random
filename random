
# Maximum length of input tokens
max_input_length = 200

# Batch processing size
batch_size = 10

# Generate the output
results = []
for i in tqdm(range(0, len(df_sample), batch_size), desc="Processing"):
    batch_texts = df_sample['combined_text'].iloc[i:i+batch_size].tolist()
    truncated_texts = [tokenizer.decode(tokenizer.encode(text, max_length=max_input_length, truncation=True), skip_special_tokens=True) for text in batch_texts]
    
    prompts = [one_shot_template.format(input_text=text) for text in truncated_texts]
    
    # Process batches asynchronously
    batch_responses = generation_pipeline(prompts, max_new_tokens=50, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)
    
    # Extract generated text from batch_responses
    for response in batch_responses:
        generated_text = response[0]['generated_text'] if isinstance(response, list) and 'generated_text' in response[0] else response['generated_text']
        results.append(generated_text)

# Combine the results into separate columns
df_sample['final_response'] = results
df_sample['LOB'] = df_sample['final_response'].apply(lambda x: re.search(r'LOB: (.*?)\n', x).group(1) if re.search(r'LOB: (.*?)\n', x) else 'Not available')
df_sample['Sentiment'] = df_sample['final_response'].apply(lambda x: re.search(r'Sentiment: (.*?)\n', x).group(1) if re.search(r'Sentiment: (.*?)\n', x) else 'Not available')
df_sample['Content_Retrieved'] = df_sample['final_response'].apply(lambda x: re.search(r'Content Retrieved: (.*)', x).group(1) if re.search(r'Content Retrieved: (.*)', x) else 'Not available')

# Save the results to a CSV file
df_sample.to_csv('1.csv', index=False)

# Print the first row to verify results
print(df_sample.iloc[0][['LOB', 'Sentiment', 'Content_Retrieved']])
