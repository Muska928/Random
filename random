import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import OneHotEncoder, KBinsDiscretizer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, r2_score

# 1. Load Excel file
df = pd.read_excel("your_excel_file.xlsx")  # Replace with actual file name

# 2. Clean and convert date columns
df["as_of_date"] = pd.to_datetime(df["as_of_date"].astype(str), format="%Y%m%d", errors="coerce")
df["max_as_of_date"] = pd.to_datetime(df["max_as_of_date"].astype(str), format="%Y%m%d", errors="coerce")

# 3. EDA Summary
print("\nColumn Names:")
print(df.columns.tolist())
print(f"\nDataset Shape: {df.shape[0]} rows × {df.shape[1]} columns")

print("\nMissing Value Check:")
missing = df.isnull().sum()
missing = missing[missing > 0]
if not missing.empty:
    print(missing)
else:
    print("No missing values found.")

# 4. Date Range Info
as_of_min = df["as_of_date"].min()
as_of_max = df["as_of_date"].max()
if as_of_min == as_of_max:
    print(f"\nData Time Info: All records have the same date → {as_of_min.date()}")
else:
    print(f"\nData Time Range: From {as_of_min.date()} to {as_of_max.date()}")

# 5. Drop null rows (if any)
df = df.dropna()
print(f"\nAfter dropping nulls: {df.shape[0]} rows remain")

# 5A. Data Type Summary
print("\nData Type Summary:")
print(df.dtypes.value_counts())

# 5B. Identify column types
target = "pli_active_cycle_time"
excluded_cols = [target, 'as_of_date', 'max_as_of_date']

binary_features = [col for col in df.columns if df[col].nunique() == 2 and df[col].dtype in [int, float]]
categorical_features = [col for col in df.columns if (df[col].dtype == 'object' or df[col].nunique() < 15) and col not in excluded_cols]
numeric_features = [col for col in df.columns if df[col].dtype in [int, float] and col not in binary_features + excluded_cols]

print("\nFeature Type Breakdown:")
print(f"- Binary features     : {len(binary_features)}")
print(f"- Categorical features: {len(categorical_features)}")
print(f"- Numeric features    : {len(numeric_features)}")

# Optional: Preview feature names
print("\nBinary Features:", binary_features)
print("\nCategorical Features:", categorical_features)
print("\nNumeric Features:", numeric_features)

# 6. Define features for model
binary_categorical_features = binary_features
multi_categorical_features = categorical_features

# Remove duplicates and excluded columns
exclude_cols = ['grade', 'prod_tx', 'num_active_flags']
categorical_model_features = list(set(binary_categorical_features + multi_categorical_features))
categorical_model_features = [col for col in categorical_model_features if col not in exclude_cols]

# 7. EDA: Distribution of binary features
print("\nValue Distribution of Binary Flags:")
for col in binary_categorical_features:
    print(f"{col}:\n{df[col].value_counts(dropna=False)}\n")

# 8. Feature engineering
df["num_active_flags"] = df[binary_categorical_features].sum(axis=1)

# 9. Correlation with target
print("\nCorrelation of binary features with cycle time:")
correlations = df[binary_categorical_features + [target]].corr()[target].sort_values(key=abs, ascending=False)
print(correlations)

# 10. Final feature list for modeling
model_features = categorical_model_features + ["num_active_flags"]

X = df[model_features]
y = df[target]

# 11. Preprocessing + Model pipeline
preprocessor = ColumnTransformer([
    ("cat", OneHotEncoder(drop='first', handle_unknown='ignore'), model_features)
])

pipeline = Pipeline([
    ("preprocessor", preprocessor),
    ("regressor", LinearRegression())
])

# 12. Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 13. Fit model
pipeline.fit(X_train, y_train)

# 14. Predict
y_pred = pipeline.predict(X_test)

# 15. Evaluation
print("\nModel Evaluation:")
print(f"MAE: {mean_absolute_error(y_test, y_pred):.2f}")
print(f"R² Score: {r2_score(y_test, y_pred):.2f}")

# 16. Predict full dataset
df["predicted_cycle_time"] = pipeline.predict(X)

# 17. Binning complexity scores
binning = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='quantile')
df["complexity_score"] = binning.fit_transform(df[["predicted_cycle_time"]]).astype(int) + 1

# 18. Plot: Actual vs Predicted
plt.figure(figsize=(8, 5))
sns.scatterplot(x=y_test, y=y_pred)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
plt.xlabel("Actual Cycle Time")
plt.ylabel("Predicted Cycle Time")
plt.title("Actual vs Predicted (Test Set)")
plt.grid(True)
plt.tight_layout()
plt.show()

# 19. Plot: Complexity Score Distribution
plt.figure(figsize=(6, 4))
sns.countplot(x="complexity_score", data=df)
plt.title("Complexity Score Distribution")
plt.xlabel("Complexity (1 = Low, 5 = High)")
plt.tight_layout()
plt.show()

# 20. Feature Importance
regressor = pipeline.named_steps["regressor"]
onehot = pipeline.named_steps["preprocessor"].named_transformers_["cat"]
feature_names = onehot.get_feature_names_out(model_features)

coef_df = pd.DataFrame({
    "Feature": feature_names,
    "Coefficient": regressor.coef_
}).sort_values(by="Coefficient", key=abs, ascending=False)

print("\nTop Feature Coefficients:")
print(coef_df.head(10))

# Plot feature importance
top_n = 15
plt.figure(figsize=(10, 6))
sns.barplot(x="Coefficient", y="Feature", data=coef_df.head(top_n), palette="viridis")
plt.title(f"Top {top_n} Feature Importances (Linear Regression)")
plt.xlabel("Coefficient Value (Impact on Cycle Time)")
plt.ylabel("Feature")
plt.tight_layout()
plt.grid(True, axis='x')
plt.show()

# 21. Save Outputs
df.to_excel("pli_with_complexity_scores.xlsx", index=False)
coef_df.to_excel("feature_importance_linear_model.xlsx", index=False)
print("\nExcel files saved.")
