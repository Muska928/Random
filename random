import pandas as pd
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from tqdm import tqdm
import torch
from transformers import BitsAndBytesConfig


# Process the entire dataset without sampling
df_sample = df_salesforce  # Use the entire DataFrame

# Load Mistral model and tokenizer from the specified folder with device_map='auto'
model_path = "mistral/Mistral-7B-Instruct-v0.2"

# Tokenizer configuration remains the same
tokenizer = AutoTokenizer.from_pretrained(
    model_path,
    trust_remote_code=True,
    padding_side="left",
    use_fast=True,
    add_bos_token=True,
    add_eos_token=True
)

tokenizer.pad_token = tokenizer.eos_token

# Use FP16 precision for faster computation and efficient memory usage
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16  # Switch to FP16
)

# Ensure proper device mapping and loading, with FP16 precision
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    quantization_config=bnb_config,
    torch_dtype=torch.float16,  # Use FP16 for faster computation
    device_map="auto",
    trust_remote_code=True,
)

model.config.use_cache = False  # silence the warnings
model.config.pretraining_tp = 1
model.gradient_checkpointing_enable()

# Sentiment analysis pipeline with the Mistral instruct model
generation_pipeline = pipeline('text-generation', model=model, tokenizer=tokenizer)


# Increase batch size to process more records in parallel
batch_size = 50  # Adjust as needed based on resource usage

# Generate the output for the entire dataset
results = []
for i in tqdm(range(0, len(df_sample), batch_size), desc="Processing"):
    batch_texts = df_sample['combined_text'].iloc[i:i+batch_size].tolist()
    assigned_topics = df_sample['assigned_topic_name'].iloc[i:i+batch_size].tolist()

    # Combine the assigned topic and text into the prompt
    prompts = [updated_prompt_template.format(topic_name=topic, input_text=text) for topic, text in zip(assigned_topics, batch_texts)]
    
    # Process batches asynchronously
    batch_responses = generation_pipeline(prompts, max_new_tokens=50, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)
    
    # Extract generated text from batch_responses
    for response in batch_responses:
        if isinstance(response, list) and len(response) > 0 and 'generated_text' in response[0]:
            generated_text = response[0]['generated_text']
        elif 'generated_text' in response:
            generated_text = response['generated_text']
        else:
            generated_text = "No valid response"
        results.append(generated_text)

# Combine the results into a single column
df_sample['final_response'] = results

# Extract the topic alignment, category, and explanation into separate columns
def extract_information(text):
    lines = text.split("\n")
    topic_alignment = "Not available"
    category = "Not available"
    explanation = "Not available"
    
    for line in lines:
        if line.startswith("Topic Alignment:"):
            topic_alignment = line.split(": ")[1]
        elif line.startswith("Category:"):
            category = line.split(": ")[1]
        elif line.startswith("Explanation:"):
            explanation = line.split(": ")[1]
    
    return topic_alignment, category, explanation

# Apply the extraction function and create new columns
df_sample[['topic_alignment', 'category', 'explanation']] = df_sample['final_response'].apply(lambda x: pd.Series(extract_information(x)))

# Display the DataFrame to verify results
print(df_sample[['topic_alignment', 'category', 'explanation']])
