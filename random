import pandas as pd
import spacy
from collections import Counter
import matplotlib.pyplot as plt
import seaborn as sns
import re
import string
from spacy.lang.en.stop_words import STOP_WORDS

# Load Spacy model
nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])
nlp.max_length = 2000000  # Increase SpaCy's max_length to handle larger texts

# Load data
df_salesforce = pd.read_excel('Data/Closed_Won Reasons/Closed_Won_Product_Capability.xlsx')

# Concatenate text columns into a single string variable
df_salesforce['combined_text'] = df_salesforce[['record_comment_text', 
                                                'executive_summary_text', 
                                                'win_loss_comments_text']].apply(lambda x: ' '.join(x.dropna()), axis=1)

# Import custom stopwords
df_stopwords = pd.read_csv('list_stopwords.csv')
stopword_list = df_stopwords['Words'].to_list()

# Adding stopwords to spacy module
for word in stopword_list:
    STOP_WORDS.add(word)
    nlp.vocab[word].is_stop = True

# Contractions and abbreviations mapping
contractions_dict = {
    "ain't": "are not", "'s": " is", "aren't": "are not", "can't": "cannot",
    "'cause": "because", "could've": "could have", "couldn't": "could not", "didn't": "did not", "doesn't": "does not", 
    "don't": "do not", "hadn't": "had not", "hasn't": "has not", "haven't": "have not", "he'd": "he would",
    "he'll": "he will", "I'd": "I would", "I'll": "I will", "I'm": "I am", "I've": "I have", "isn't": "is not", 
    "it'd": "it would", "it'll": "it will", "let's": "let us", "ma'am": "madam", "might've": "might have", 
    "must've": "must have", "needn't": "need not", "o'clock": "of the clock", "shan't": "shall not", "she'd": "she would", 
    "she'll": "she will", "should've": "should have", "shouldn't": "should not", "that'd": "that would", 
    "there'd": "there would", "they'd": "they would", "they'll": "they will", "they're": "they are", "they've": "they have",
    "to've": "to have", "wasn't": "was not", "we'd": "we would", "we'll": "we will", "we're": "we are", 
    "we've": "we have", "weren't": "were not", "what'll": "what will", "what're": "what are", "what've": "what have",
    "when've": "when have", "where'd": "where did", "where've": "where have", "who'll": "who will", "who've": "who have",
    "why've": "why have", "won't": "will not", "would've": "would have", "wouldn't": "would not", "y'all": "you all",
}

# Function to expand contractions
contractions_re = re.compile('(%s)' % '|'.join(contractions_dict.keys()))
def expand_contractions(text):
    def replace(match):
        return contractions_dict[match.group(0)]
    return contractions_re.sub(replace, text)

# Function to normalize multi-word phrases
multi_word_phrases = {
    "account open": ["open account", "account opening"],
    "new client": ["client new"],
    "market disruption": ["disruption market"],
    "client acquisition": ["acquired new client", "acquiring new clients"],
    # Add more phrases as needed...
}

def normalize_phrases(text):
    for normalized, variants in multi_word_phrases.items():
        for variant in variants:
            text = text.replace(variant, normalized)
    return text

# Function to remove digits
def remove_digits(text):
    return re.sub(r'\d+', '', text)

# Function to remove extra punctuation/marks
def remove_extra_marks(text):
    return re.sub(r'[^\w\s]', '', text)

# Function to remove stopwords
def remove_stopwords(text):
    doc = nlp(text)
    return ' '.join([token.text for token in doc if not token.is_stop])

# Function to remove erroneous tokens like 'th'
def remove_invalid_tokens(text):
    invalid_tokens = ['th']
    tokens = text.split()
    return ' '.join([word for word in tokens if word not in invalid_tokens])

# Function to lemmatize
def lemmatize(text):
    doc = nlp(text)
    return ' '.join([token.lemma_ for token in doc])

# Compute word frequency across the entire corpus
def compute_word_frequency(corpus):
    word_counter = Counter()
    for text in corpus:
        word_counter.update(tokenize_text(text))
    return word_counter

# Remove rare words based on frequency threshold
def remove_rare_words(text, word_counter, min_freq=2):
    tokens = tokenize_text(text)
    return ' '.join([word for word in tokens if word_counter[word] >= min_freq])

# Full preprocessing pipeline with rare word removal
def full_preprocess(text, word_counter=None, min_freq=2):
    text = expand_contractions(text)  # Step 1: Expand contractions
    text = text.lower()  # Step 2: Convert to lowercase
    text = remove_digits(text)  # Step 3: Remove digits
    text = remove_extra_marks(text)  # Step 4: Remove extra marks
    text = normalize_phrases(text)  # Step 5: Normalize multi-word phrases
    text = remove_stopwords(text)  # Step 6: Remove stopwords
    text = lemmatize(text)  # Step 7: Lemmatization
    text = remove_invalid_tokens(text)  # Step 8: Remove invalid tokens (like 'th')
    
    # Step 9: Remove rare words (if a word counter is provided)
    if word_counter:
        text = remove_rare_words(text, word_counter, min_freq)
    
    return text

# Example text for testing
first_row_text = "The account opening process at the Union Bank was tedious. However, account open procedures were later simplified."

# Apply full preprocessing on the example text

# Step 1: Compute word frequencies from the entire dataset
all_processed_texts = df_salesforce['combined_text'].apply(lambda x: full_preprocess(x))  # Without rare word removal for now
word_counter = compute_word_frequency(all_processed_texts)

# Apply preprocessing with rare word removal
df_salesforce['processed_text'] = df_salesforce['combined_text'].apply(lambda x: full_preprocess(x, word_counter=word_counter, min_freq=2))

# Print the processed text for the example
processed_first_row = full_preprocess(first_row_text, word_counter=word_counter, min_freq=2)
print("Original Text:\n", first_row_text)
print("\nProcessed Text with Rare Words Removed:\n", processed_first_row)

# ---- N-gram Frequency and Plotting ----

# Tokenize the processed text
def tokenize_text(text):
    return text.split()

# Get all tokens from the processed data
all_tokens = [token for text in df_salesforce['processed_text'] for token in tokenize_text(text)]

# Function to get n-gram frequency
def get_ngram_freqs(tokens_list, n=1):
    ngrams = zip(*[tokens_list[i:] for i in range(n)])
    return Counter([' '.join(ngram) for ngram in ngrams]).most_common(30)

# Function to display and plot n-grams
def display_ngrams(ngram_freq, title):
    ngram_df = pd.DataFrame(ngram_freq, columns=['ngram', 'frequency'])
    print(f"\n{title} Frequency Table:")
    print(ngram_df.to_string(index=False))

    plt.figure(figsize=(12, 8))
    sns.barplot(x='ngram', y='frequency', data=ngram_df, palette='coolwarm')
    plt.xticks(rotation=45, ha='right', fontsize=12)
    plt.title(f'Top 30 {title}', fontsize=16, weight='bold')
    plt.tight_layout()
    plt.show()

# Get Unigrams, Bigrams, Trigrams frequencies
unigram_freq = get_ngram_freqs(all_tokens, n=1)
bigram_freq = get_ngram_freqs(all_tokens, n=2)
trigram_freq = get_ngram_freqs(all_tokens, n=3)

# Display and plot unigrams, bigrams, trigrams
display_ngrams(unigram_freq, 'Unigrams')
display_ngrams(bigram_freq, 'Bigrams')
display_ngrams(trigram_freq, 'Trigrams')
