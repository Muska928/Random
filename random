import pandas as pd
import re
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from tqdm import tqdm
import torch
from transformers import BitsAndBytesConfig


# Preprocess the text
def preprocess_text(text):
    text = text.lower()  # Convert text to lowercase
    text = re.sub(r'[^a-zA-Z\s]', '', text)  # Remove special characters and numbers
    text = re.sub(r'\s+', ' ', text).strip()  # Remove extra whitespace
    return text

# Apply preprocessing
df_salesforce['combined_text'] = df_salesforce['combined_text'].apply(preprocess_text)

# Maximum length of input tokens
max_input_length = 200

# Batch processing size
batch_size = 10

# Function to process and parse results
def process_and_parse(df_sample, one_shot_template, batch_size=10):
    results = []
    for i in tqdm(range(0, len(df_sample), batch_size), desc="Processing"):
        batch_texts = df_sample['combined_text'].iloc[i:i+batch_size].tolist()
        truncated_texts = [tokenizer.decode(tokenizer.encode(text, max_length=max_input_length, truncation=True), skip_special_tokens=True) for text in batch_texts]
        prompts = [one_shot_template.format(input_text=text) for text in truncated_texts]

        batch_responses = generation_pipeline(prompts, max_new_tokens=50, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)
        for response in batch_responses:
            generated_text = response[0]['generated_text'] if isinstance(response, list) and 'generated_text' in response[0] else response['generated_text']
            results.append(generated_text)
    
    df_sample['final_response'] = results
    df_sample['all_information'] = df_sample['final_response'].apply(parse_response)
    return df_sample






