import pandas as pd
import spacy
from collections import Counter
import matplotlib.pyplot as plt
import seaborn as sns
import re
import string
from spacy.lang.en.stop_words import STOP_WORDS

# Load Spacy model
nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])
nlp.max_length = 2000000  # Increase SpaCy's max_length to handle larger texts

# Load data
df_salesforce = pd.read_excel('Data/Closed_Won Reasons/Closed_Won_Product_Capability.xlsx')

# Concatenate text columns into a single string variable
df_salesforce['combined_text'] = df_salesforce[['record_comment_text', 
                                                'executive_summary_text', 
                                                'win_loss_comments_text']].apply(lambda x: ' '.join(x.dropna()), axis=1)

# Additional stopwords to filter out single characters and unwanted terms
extra_stopwords = set(['d', 'ir', '--', 'new', 'account'])  # Add more based on observations

# Adding extra stopwords to Spacy's stopword list
for word in extra_stopwords:
    STOP_WORDS.add(word)
    nlp.vocab[word].is_stop = True

# Contractions and abbreviations mapping
contractions_dict = {
    "ain't": "are not", "'s": "is", "aren't": "are not", "can't": "cannot",
    "'cause": "because", "could've": "could have", "couldn't": "could not", 
    # Add more contractions as needed...
}

# Function to expand contractions
contractions_re = re.compile('(%s)' % '|'.join(contractions_dict.keys()))
def expand_contractions(text):
    def replace(match):
        return contractions_dict[match.group(0)]
    return contractions_re.sub(replace, text)

# Function to normalize multi-word phrases
multi_word_phrases = {
    "account open": ["open account", "account opening"],
    "new client": ["client new"],
    "market disruption": ["disruption market"],
    "client acquisition": ["acquired new client", "acquiring new clients"],
    # Add more multi-word phrases as needed...
}

def normalize_phrases(text):
    for normalized, variants in multi_word_phrases.items():
        for variant in variants:
            text = text.replace(variant, normalized)
    return text

# Function to remove digits
def remove_digits(text):
    return re.sub(r'\d+', '', text)

# Function to remove extra punctuation/marks
def remove_extra_marks(text):
    return re.sub(r'[^\w\s]', '', text)

# Function to remove stopwords
def remove_stopwords(text):
    doc = nlp(text)
    return ' '.join([token.text for token in doc if not token.is_stop and len(token.text) > 1])  # Remove single chars

# Function to remove erroneous tokens like 'th'
def remove_invalid_tokens(text):
    invalid_tokens = ['th', 'ir', 'd']
    tokens = text.split()
    return ' '.join([word for word in tokens if word not in invalid_tokens])

# Function to lemmatize
def lemmatize(text):
    doc = nlp(text)
    return ' '.join([token.lemma_ for token in doc])

# Full preprocessing pipeline
def full_preprocess(text):
    text = expand_contractions(text)  # Step 1: Expand contractions
    text = text.lower()  # Step 2: Convert to lowercase
    text = remove_digits(text)  # Step 3: Remove digits
    text = remove_extra_marks(text)  # Step 4: Remove extra marks
    text = normalize_phrases(text)  # Step 5: Normalize multi-word phrases
    text = remove_stopwords(text)  # Step 6: Remove stopwords
    text = lemmatize(text)  # Step 7: Lemmatization
    text = remove_invalid_tokens(text)  # Step 8: Remove invalid tokens (like 'th', 'ir', 'd')
    return text

# Example text for testing
first_row_text = "The account opening process at the Union Bank was tedious. However, account open procedures were later simplified."

# Apply full preprocessing on the example text
processed_first_row = full_preprocess(first_row_text)

# Print the original and processed text
print("Original Text:\n", first_row_text)
print("\nProcessed Text:\n", processed_first_row)

# ---- N-gram Frequency and Plotting (Non-RST Data) ----

# Filter Non-RST data
non_rst_data = df_salesforce[df_salesforce['account_eci'] != 'RST']

# Apply full preprocessing to Non-RST data
non_rst_data['processed_text'] = non_rst_data['combined_text'].apply(lambda x: full_preprocess(x))

# Tokenize the processed text
def tokenize_text(text):
    return text.split()

# Get all tokens from the processed Non-RST deals
all_tokens_non_rst = [token for text in non_rst_data['processed_text'] for token in tokenize_text(text)]

# Function to get n-gram frequency for Non-RST data
def get_ngram_freqs(tokens_list, n=1):
    ngrams = zip(*[tokens_list[i:] for i in range(n)])
    return Counter([' '.join(ngram) for ngram in ngrams if ' '.join(ngram).strip() != '--']).most_common(30)

# Function to display and plot n-grams for Non-RST data
def display_ngrams(ngram_freq, title):
    ngram_df = pd.DataFrame(ngram_freq, columns=['ngram', 'frequency'])
    print(f"\n{title} Frequency Table (Non-RST):")
    print(ngram_df.to_string(index=False))

    plt.figure(figsize=(12, 8))
    sns.barplot(x='ngram', y='frequency', data=ngram_df, palette='coolwarm')
    
    # Add frequency numbers on top of the bars
    for index, value in enumerate(ngram_df['frequency']):
        plt.text(index, value, str(value), ha='center', va='bottom', fontsize=10)
    
    plt.xticks(rotation=45, ha='right', fontsize=12)
    plt.title(f'Top 30 {title} (Non-RST)', fontsize=16, weight='bold')
    plt.tight_layout()
    plt.show()

# Get Unigrams, Bigrams, Trigrams frequencies for Non-RST data
unigram_freq_non_rst = get_ngram_freqs(all_tokens_non_rst, n=1)
bigram_freq_non_rst = get_ngram_freqs(all_tokens_non_rst, n=2)
trigram_freq_non_rst = get_ngram_freqs(all_tokens_non_rst, n=3)

# Display and plot unigrams, bigrams, trigrams for Non-RST deals
display_ngrams(unigram_freq_non_rst, 'Unigrams')
display_ngrams(bigram_freq_non_rst, 'Bigrams')
display_ngrams(trigram_freq_non_rst, 'Trigrams')
