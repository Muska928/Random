import pandas as pd
from collections import Counter

# Assuming df_salesforce has been read and preprocessed as before
df_salesforce = pd.read_excel('Data/exclude_won_rst.xlsx')

# Concatenate text columns into a single string variable
df_salesforce['combined_text'] = df_salesforce[['record_comment_text', 
                                                'executive_summary_text', 
                                                'win_loss_reason_text', 
                                                'win_loss_comments_text']].apply(lambda x: ' '.join(x.dropna()), axis=1)

# Count word tokens for each row
df_salesforce['token_count'] = df_salesforce['combined_text'].apply(lambda x: len(x.split()))

# Filter rows with less than or equal to 3 tokens
short_texts = df_salesforce[df_salesforce['token_count'] <= 3]

# Print the count of rows with less than or equal to 3 tokens
print(f"Rows with less than or equal to 3 tokens: {len(short_texts)}")

# Save the rows with less than or equal to 3 tokens to a file
short_texts.to_csv('rows_with_less_than_3_tokens.csv', index=False)

# Analyze the short texts: Get word frequencies
word_list = ' '.join(short_texts['combined_text']).split()
word_frequencies = Counter(word_list)

# Print the top 10 most frequent words in short texts
print("Top 10 frequent words in short texts:")
for word, freq in word_frequencies.most_common(10):
    print(f"{word}: {freq}")

# Plotting the word frequencies
import matplotlib.pyplot as plt

top_words, top_freqs = zip(*word_frequencies.most_common(10))

plt.figure(figsize=(10, 6))
plt.barh(top_words, top_freqs, color='skyblue')
plt.xlabel('Frequency')
plt.ylabel('Words')
plt.title('Top 10 Words in Rows with <= 3 Tokens')
plt.gca().invert_yaxis()  # Invert to have the highest frequency on top
plt.show()

-------

import pandas as pd
from collections import Counter
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from nltk import ngrams
from textblob import TextBlob
import spacy
from sklearn.cluster import KMeans
import numpy as np

# Load Spacy model
nlp = spacy.load('en_core_web_md')

# Assuming non_rst_dataset has already been created
# Count word tokens for each row in non_rst_dataset
non_rst_dataset['token_count'] = non_rst_dataset['combined_text'].apply(lambda x: len(x.split()))

# Filter rows with less than or equal to 3 tokens
short_texts_non_rst = non_rst_dataset[non_rst_dataset['token_count'] <= 3]

# Print the count of rows with less than or equal to 3 tokens in non_rst_dataset
print(f"Rows with less than or equal to 3 tokens in non_rst_dataset: {len(short_texts_non_rst)}")

# Save the rows with less than or equal to 3 tokens to a file
short_texts_non_rst.to_csv('non_rst_rows_with_less_than_3_tokens.csv', index=False)

# Analyze the short texts: Get word frequencies
word_list = ' '.join(short_texts_non_rst['combined_text']).split()
word_frequencies = Counter(word_list)

# Print the top 10 most frequent words in short texts for non_rst_dataset
print("Top 10 frequent words in short texts (non_rst_dataset):")
for word, freq in word_frequencies.most_common(10):
    print(f"{word}: {freq}")

# Plotting the word frequencies
top_words, top_freqs = zip(*word_frequencies.most_common(10))

plt.figure(figsize=(10, 6))
plt.barh(top_words, top_freqs, color='skyblue')
plt.xlabel('Frequency')
plt.ylabel('Words')
plt.title('Top 10 Words in non_rst_dataset Rows with <= 3 Tokens')
plt.gca().invert_yaxis()  # Invert to have the highest frequency on top
plt.show()

# Generate a word cloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_frequencies)

# Plot the word cloud
plt.figure(figsize=(10, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')  # Turn off the axis
plt.title('Word Cloud for non_rst_dataset Rows with <= 3 Tokens')
plt.show()

# Generate bigrams and trigrams
bigrams = list(ngrams(word_list, 2))
trigrams = list(ngrams(word_list, 3))

# Count frequencies of bigrams and trigrams
bigram_freq = Counter(bigrams)
trigram_freq = Counter(trigrams)

# Print top 5 bigrams
print("Top 5 Bigrams:")
for bigram, freq in bigram_freq.most_common(5):
    print(f"{bigram}: {freq}")

# Print top 5 trigrams
print("Top 5 Trigrams:")
for trigram, freq in trigram_freq.most_common(5):
    print(f"{trigram}: {freq}")

# Visualize top 5 bigrams
top_bigrams, top_bigram_freqs = zip(*bigram_freq.most_common(5))
top_bigrams = [' '.join(bigram) for bigram in top_bigrams]

plt.figure(figsize=(10, 5))
plt.barh(top_bigrams, top_bigram_freqs, color='lightcoral')
plt.xlabel('Frequency')
plt.ylabel('Bigrams')
plt.title('Top 5 Bigrams in Rows with <= 3 Tokens')
plt.gca().invert_yaxis()  # Highest at the top
plt.show()

# Visualize top 5 trigrams
top_trigrams, top_trigram_freqs = zip(*trigram_freq.most_common(5))
top_trigrams = [' '.join(trigram) for trigram in top_trigrams]

plt.figure(figsize=(10, 5))
plt.barh(top_trigrams, top_trigram_freqs, color='lightblue')
plt.xlabel('Frequency')
plt.ylabel('Trigrams')
plt.title('Top 5 Trigrams in Rows with <= 3 Tokens')
plt.gca().invert_yaxis()  # Highest at the top
plt.show()

# Sentiment Analysis using TextBlob
short_texts_non_rst['sentiment'] = short_texts_non_rst['combined_text'].apply(lambda x: TextBlob(x).sentiment.polarity)

# Categorize sentiment as positive, neutral, or negative
short_texts_non_rst['sentiment_category'] = pd.cut(short_texts_non_rst['sentiment'], 
                                                   bins=[-1, -0.1, 0.1, 1], 
                                                   labels=['Negative', 'Neutral', 'Positive'])

# Print sentiment distribution
print(short_texts_non_rst['sentiment_category'].value_counts())

# Plot the sentiment distribution
plt.figure(figsize=(8, 4))
short_texts_non_rst['sentiment_category'].value_counts().plot(kind='bar', color='skyblue')
plt.title('Sentiment Analysis of Rows with <= 3 Tokens')
plt.xlabel('Sentiment Category')
plt.ylabel('Number of Deals')
plt.show()

# Parts of Speech Tagging
short_texts_non_rst['pos_tags'] = short_texts_non_rst['combined_text'].apply(lambda x: [(token.text, token.pos_) for token in nlp(x)])

# View example POS tagging
print(short_texts_non_rst['pos_tags'].head())

# Clustering on word vectors (optional)
# Convert each text into vector using Spacy embeddings
vectors = [nlp(text).vector for text in short_texts_non_rst['combined_text']]

# Apply KMeans clustering
kmeans = KMeans(n_clusters=3, random_state=42)
short_texts_non_rst['cluster'] = kmeans.fit_predict(vectors)

# Plot the cluster distribution
plt.figure(figsize=(8, 4))
short_texts_non_rst['cluster'].value_counts().plot(kind='bar', color='lightgreen')
plt.title('Cluster Distribution of Rows with <= 3 Tokens')
plt.xlabel('Cluster')
plt.ylabel('Number of Deals')
plt.show()
