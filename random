import pandas as pd
import spacy
from collections import Counter
import matplotlib.pyplot as plt
import seaborn as sns
import re
import string
from spacy.lang.en.stop_words import STOP_WORDS

# Load Spacy model
nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])
nlp.max_length = 2000000  # Increase SpaCy's max_length to handle larger texts

# Load data
df_salesforce = pd.read_excel('Data/Closed_Won Reasons/Closed_Won_Product_Capability.xlsx')

# Concatenate text columns into a single string variable
df_salesforce['combined_text'] = df_salesforce[['record_comment_text', 
                                                'executive_summary_text', 
                                                'win_loss_comments_text']].apply(lambda x: ' '.join(x.dropna()), axis=1)

# Import custom stopwords
df_stopwords = pd.read_csv('list_stopwords.csv')
stopword_list = df_stopwords['Words'].to_list()

# Adding stopwords to spacy module
for word in stopword_list:
    STOP_WORDS.add(word)
    nlp.vocab[word].is_stop = True

# Additional stopwords to filter out single characters and unwanted terms
extra_stopwords = set(['d', 'ir', '--', 'new', 'account'])
for word in extra_stopwords:
    STOP_WORDS.add(word)
    nlp.vocab[word].is_stop = True

# Contractions and abbreviations mapping
contractions_dict = {
    "ain't": "are not", "'s": " is", "aren't": "are not", "can't": "cannot",
    "'cause": "because", "could've": "could have", "couldn't": "could not", "didn't": "did not", "doesn't": "does not", 
    "don't": "do not", "hadn't": "had not", "hasn't": "has not", "haven't": "have not", "he'd": "he would",
    "he'll": "he will", "I'd": "I would", "I'll": "I will", "I'm": "I am", "I've": "I have", "isn't": "is not", 
    "it'd": "it would", "it'll": "it will", "let's": "let us", "ma'am": "madam", "might've": "might have", 
    "must've": "must have", "needn't": "need not", "o'clock": "of the clock", "shan't": "shall not", "she'd": "she would", 
    "she'll": "she will", "should've": "should have", "shouldn't": "should not", "that'd": "that would", 
    "there'd": "there would", "they'd": "they would", "they'll": "they will", "they're": "they are", "they've": "they have",
    "to've": "to have", "wasn't": "was not", "we'd": "we would", "we'll": "we will", "we're": "we are", 
    "we've": "we have", "weren't": "were not", "what'll": "what will", "what're": "what are", "what've": "what have",
    "when've": "when have", "where'd": "where did", "where've": "where have", "who'll": "who will", "who've": "who have",
    "why've": "why have", "won't": "will not", "would've": "would have", "wouldn't": "would not", "y'all": "you all",
}

# Function to expand contractions
contractions_re = re.compile('(%s)' % '|'.join(contractions_dict.keys()))
def expand_contractions(text):
    def replace(match):
        return contractions_dict[match.group(0)]
    return contractions_re.sub(replace, text)

# Function to normalize multi-word phrases
multi_word_phrases = {
    "account open": ["open account", "account opening"],
    "new client": ["client new"],
    "market disruption": ["disruption market"],
    "client acquisition": ["acquired new client", "acquiring new clients"],
}

def normalize_phrases(text):
    for normalized, variants in multi_word_phrases.items():
        for variant in variants:
            text = text.replace(variant, normalized)
    return text

# Function to remove digits
def remove_digits(text):
    return re.sub(r'\d+', '', text)

# Function to remove extra punctuation/marks
def remove_extra_marks(text):
    return re.sub(r'[^\w\s]', '', text)

# Function to remove stopwords
def remove_stopwords(text):
    doc = nlp(text)
    return ' '.join([token.text for token in doc if not token.is_stop])

# Function to remove erroneous tokens like 'th'
def remove_invalid_tokens(text):
    invalid_tokens = ['th']
    tokens = text.split()
    return ' '.join([word for word in tokens if word not in invalid_tokens])

# Function to lemmatize
def lemmatize(text):
    doc = nlp(text)
    return ' '.join([token.lemma_ for token in doc])

# Full preprocessing pipeline
def full_preprocess(text):
    text = expand_contractions(text)  # Step 1: Expand contractions
    text = text.lower()  # Step 2: Convert to lowercase
    text = remove_digits(text)  # Step 3: Remove digits
    text = remove_extra_marks(text)  # Step 4: Remove extra marks
    text = normalize_phrases(text)  # Step 5: Normalize multi-word phrases
    text = remove_stopwords(text)  # Step 6: Remove stopwords
    text = lemmatize(text)  # Step 7: Lemmatization
    text = remove_invalid_tokens(text)  # Step 8: Remove invalid tokens (like 'th')
    return text

# ---- N-gram Frequency and Plotting (Non-RST Data) ----

# Filter Non-RST data
non_rst_data = df_salesforce[df_salesforce['account_eci'] != 'RST']

# Apply full preprocessing to Non-RST data
non_rst_data['processed_text'] = non_rst_data['combined_text'].apply(full_preprocess)

# Tokenize the processed text
def tokenize_text(text):
    return text.split()

# Get all tokens from the processed Non-RST deals
all_tokens_non_rst = [token for text in non_rst_data['processed_text'] for token in tokenize_text(text)]

# Function to get n-gram frequency for Non-RST data
def get_ngram_freqs(tokens_list, n=1):
    ngrams = zip(*[tokens_list[i:] for i in range(n)])
    return Counter([' '.join(ngram) for ngram in ngrams]).most_common(30)

# Function to display and plot n-grams for Non-RST data with sum of frequencies
def display_ngrams(ngram_freq, title):
    ngram_df = pd.DataFrame(ngram_freq, columns=['ngram', 'frequency'])
    ngram_sum = ngram_df['frequency'].sum()  # Calculate sum of frequencies
    
    print(f"\n{title} Frequency Table (Non-RST):")
    print(ngram_df.to_string(index=False))
    print(f"\nTotal Frequency Sum: {ngram_sum}")

    plt.figure(figsize=(12, 8))
    sns.barplot(x='ngram', y='frequency', data=ngram_df, palette='coolwarm')

    # Add frequency numbers on top of bars
    for index, value in enumerate(ngram_df['frequency']):
        plt.text(index, value, str(value), ha='center', va='bottom', fontsize=10)

    plt.xticks(rotation=45, ha='right', fontsize=12)
    plt.title(f'Top 30 {title} (Non-RST)', fontsize=16, weight='bold')
    plt.tight_layout()
    plt.show()

# Get Unigrams, Bigrams, Trigrams frequencies for Non-RST data
unigram_freq_non_rst = get_ngram_freqs(all_tokens_non_rst, n=1)
bigram_freq_non_rst = get_ngram_freqs(all_tokens_non_rst, n=2)
trigram_freq_non_rst = get_ngram_freqs(all_tokens_non_rst, n=3)

# Display and plot unigrams, bigrams, trigrams for Non-RST deals
display_ngrams(unigram_freq_non_rst, 'Unigrams')
display_ngrams(bigram_freq_non_rst, 'Bigrams')
display_ngrams(trigram_freq_non_rst, 'Trigrams')
