import pandas as pd
import re
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline

# Load data from Salesforce analytics
df_salesforce = pd.read_excel('/mnt/data/Salesforce_Deals_Text_Analytics.xlsx')

# Concatenate text columns into a single string variable
df_salesforce['combined_text'] = df_salesforce[
    ['record_comment_text', 'description_text', 'executive_summary_text', 'win_loss_reason_text', 'win_loss_comments_text']
].apply(lambda x: ' '.join(x.dropna()), axis=1)

# Preprocess the text
def preprocess_text(text):
    text = text.lower()
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

df_salesforce['combined_text'] = df_salesforce['combined_text'].apply(preprocess_text)

# Define Middle Office related keywords
middle_office_keywords = ['service', 'onboarding']

# Add Middle Office flag to the dataframe
df_salesforce['middle_office_related'] = df_salesforce['combined_text'].apply(
    lambda x: 'yes' if any(keyword in x.lower() for keyword in middle_office_keywords) else 'no'
)

# Filter out the middle office related deals
df_middle_office = df_salesforce[df_salesforce['middle_office_related'] == 'yes']

# Load Mistral model and tokenizer
model_name = "Mistral-7B-Instruct-v0.2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

# Sentiment analysis pipeline
sentiment_pipeline = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)

# Function to get sentiment and triggers
def analyze_text(text):
    sentiment = sentiment_pipeline(text)
    sentiment_label = sentiment[0]['label']
    
    # Zero-shot approach to extract triggers
    trigger_prompt = f"Extract the key reason or trigger for the deal being {'won' if sentiment_label == 'POSITIVE' else 'lost'} from the following text:\n\n{text}"
    trigger_response = sentiment_pipeline(trigger_prompt)
    
    return sentiment_label, trigger_response[0]['label']

# Apply the sentiment and trigger analysis to each row
df_middle_office[['sentiment', 'trigger']] = df_middle_office['combined_text'].apply(lambda x: pd.Series(analyze_text(x)))

# Filter cases for 'Closed Won' and 'Closed Lost'
df_won = df_middle_office[df_middle_office['sentiment'] == 'POSITIVE']
df_lost = df_middle_office[df_middle_office['sentiment'] == 'NEGATIVE']

# Generate Word Clouds for 'Closed Won' and 'Closed Lost'
won_text = ' '.join(df_won['combined_text'])
lost_text = ' '.join(df_lost['combined_text'])

wordcloud_won = WordCloud(width=800, height=400, background_color='white').generate(won_text)
wordcloud_lost = WordCloud(width=800, height=400, background_color='white').generate(lost_text)

# Display Word Clouds
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.imshow(wordcloud_won, interpolation='bilinear')
plt.title('Closed Won Deals Word Cloud')
plt.axis('off')

plt.subplot(1, 2, 2)
plt.imshow(wordcloud_lost, interpolation='bilinear')
plt.title('Closed Lost Deals Word Cloud')
plt.axis('off')

plt.show()

# Validate and summarize
sample_size = 1000
df_sample_won = df_won.sample(min(sample_size, len(df_won)))
df_sample_lost = df_lost.sample(min(sample_size, len(df_lost)))

# Summarize triggers
won_triggers = df_sample_won['trigger'].value_counts().head(10)
lost_triggers = df_sample_lost['trigger'].value_counts().head(10)

print("Top 10 Triggers for Closed Won Deals:")
print(won_triggers)

print("\nTop 10 Triggers for Closed Lost Deals:")
print(lost_triggers)



------
import pandas as pd
import re
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline, Trainer, TrainingArguments
from sklearn.model_selection import train_test_split
from datasets import Dataset

# Load data from Salesforce analytics
df_salesforce = pd.read_excel('/mnt/data/Salesforce_Deals_Text_Analytics.xlsx')

# Concatenate text columns into a single string variable
df_salesforce['combined_text'] = df_salesforce[
    ['record_comment_text', 'description_text', 'executive_summary_text', 'win_loss_reason_text', 'win_loss_comments_text']
].apply(lambda x: ' '.join(x.dropna()), axis=1)

# Preprocess the text
def preprocess_text(text):
    text = text.lower()
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

df_salesforce['combined_text'] = df_salesforce['combined_text'].apply(preprocess_text)

# Define Middle Office related keywords
middle_office_keywords = ['service', 'onboarding']

# Add Middle Office flag to the dataframe
df_salesforce['middle_office_related'] = df_salesforce['combined_text'].apply(
    lambda x: 'yes' if any(keyword in x.lower() for keyword in middle_office_keywords) else 'no'
)

# Filter out the middle office related deals
df_middle_office = df_salesforce[df_salesforce['middle_office_related'] == 'yes']

# Load Mistral model and tokenizer
model_name = "Mistral-7B-Instruct-v0.2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

# Load and preprocess the training dataset for fine-tuning
# Assuming you have a labeled dataset 'training_dataset.csv' for fine-tuning
df_train = pd.read_csv('training_dataset.csv')
df_train['text'] = df_train['text'].apply(preprocess_text)
df_train, df_val = train_test_split(df_train, test_size=0.2, random_state=42)

# Convert to Hugging Face datasets
train_dataset = Dataset.from_pandas(df_train)
val_dataset = Dataset.from_pandas(df_val)

# Tokenize the datasets
def tokenize_function(examples):
    return tokenizer(examples['text'], padding='max_length', truncation=True)

train_dataset = train_dataset.map(tokenize_function, batched=True)
val_dataset = val_dataset.map(tokenize_function, batched=True)

train_dataset = train_dataset.rename_column('label', 'labels')
val_dataset = val_dataset.rename_column('label', 'labels')

train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])
val_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])

# Define training arguments
training_args = TrainingArguments(
    output_dir='./results',
    evaluation_strategy='epoch',
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
)

# Initialize the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=tokenizer,
)

# Fine-tune the model
trainer.train()

# Save the fine-tuned model
model.save_pretrained('fine_tuned_mistral')
tokenizer.save_pretrained('fine_tuned_mistral')

# Load the fine-tuned model for sentiment analysis
fine_tuned_model = AutoModelForSequenceClassification.from_pretrained('fine_tuned_mistral')
sentiment_pipeline = pipeline('sentiment-analysis', model=fine_tuned_model, tokenizer=tokenizer)

# Function to get sentiment and triggers
def analyze_text(text):
    sentiment = sentiment_pipeline(text)
    sentiment_label = sentiment[0]['label']
    
    # Zero-shot approach to extract triggers
    trigger_prompt = f"Extract the key reason or trigger for the deal being {'won' if sentiment_label == 'POSITIVE' else 'lost'} from the following text:\n\n{text}"
    trigger_response = sentiment_pipeline(trigger_prompt)
    
    return sentiment_label, trigger_response[0]['label']

# Apply the sentiment and trigger analysis to each row
df_middle_office[['sentiment', 'trigger']] = df_middle_office['combined_text'].apply(lambda x: pd.Series(analyze_text(x)))

# Filter cases for 'Closed Won' and 'Closed Lost'
df_won = df_middle_office[df_middle_office['sentiment'] == 'POSITIVE']
df_lost = df_middle_office[df_middle_office['sentiment'] == 'NEGATIVE']

# Generate Word Clouds for 'Closed Won' and 'Closed Lost'
won_text = ' '.join(df_won['combined_text'])
lost_text = ' '.join(df_lost['combined_text'])

wordcloud_won = WordCloud(width=800, height=400, background_color='white').generate(won_text)
wordcloud_lost = WordCloud(width=800, height=400, background_color='white').generate(lost_text)

# Display Word Clouds
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.imshow(wordcloud_won, interpolation='bilinear')
plt.title('Closed Won Deals Word Cloud')
plt.axis('off')

plt.subplot(1, 2, 2)
plt.imshow(wordcloud_lost, interpolation='bilinear')
plt.title('Closed Lost Deals Word Cloud')
plt.axis('off')

plt.show()

# Validate and summarize
sample_size = 1000
df_sample_won = df_won.sample(min(sample_size, len(df_won)))
df_sample_lost = df_lost.sample(min(sample_size, len(df_lost)))

# Summarize triggers
won_triggers = df_sample_won['trigger'].value_counts().head(10)
lost_triggers = df_sample_lost['trigger'].value_counts().head(10)

print("Top 10 Triggers for Closed Won Deals:")
print(won_triggers)

print("\nTop 10 Triggers for Closed Lost Deals:")
print(lost_triggers)



------

import pandas as pd
import spacy
import gensim
import matplotlib.pyplot as plt
import seaborn as sns
from textblob import TextBlob
import re

# Load data from Salesforce analytics
df_salesforce = pd.read_excel('Salesforce_Deals_Text_Analytics.xlsx')

# Concatenate text columns into a single string variable
df_salesforce['combined_text'] = df_salesforce[['record_comment_text', 'description_text', 'executive_summary_text', 'win_loss_reason_text', 'win_loss_comments_text']].apply(lambda x: ' '.join(x.dropna()), axis=1)

# Load Spacy model
nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])

# Function to preprocess text
def preprocess(document):
    document = document.lower()  # Lowercase the text
    document = re.sub(r'[^a-zA-Z\s]', '', document)  # Remove special characters and numbers
    document = re.sub(r'\s+', ' ', document).strip()  # Remove extra whitespace
    doc = nlp(document)  # Tokenize and lemmatize
    words = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct and len(token.lemma_) > 2]
    return words

# Preprocessing text columns
df_salesforce['processed_text'] = df_salesforce['combined_text'].map(preprocess)
processed_texts = df_salesforce['processed_text'].tolist()

# Define Middle Office related keywords
keywords = ['middle office', 'service', 'onboarding', 'relationship']

# Add Middle Office flag to the dataframe
df_salesforce['middle_office_related'] = df_salesforce['combined_text'].apply(lambda x: 'yes' if any(keyword in x.lower() for keyword in keywords) else 'no')

# Create a new DataFrame for Middle Office related deals
df_middle_office = df_salesforce[df_salesforce['middle_office_related'] == 'yes']

# Sentiment Analysis
df_middle_office['sentiment'] = df_middle_office['combined_text'].apply(lambda x: TextBlob(x).sentiment.polarity)

# Prepare the data for topic modeling
processed_texts_mo = df_middle_office['processed_text'].tolist()

# Create bigram and trigram models
bigram_mo = gensim.models.Phrases(processed_texts_mo, min_count=5, threshold=100)
trigram_mo = gensim.models.Phrases(bigram_mo[processed_texts_mo], threshold=100)
bigram_mod_mo = gensim.models.phrases.Phraser(bigram_mo)
trigram_mod_mo = gensim.models.phrases.Phraser(trigram_mo)

# Applying bigrams and trigrams to the preprocessed texts
texts_bigrams_mo = [bigram_mod_mo[doc] for doc in processed_texts_mo]
texts_trigrams_mo = [trigram_mod_mo[bigram_mod_mo[doc]] for doc in texts_bigrams_mo]

# Creating dictionary
dictionary_mo = gensim.corpora.Dictionary(texts_trigrams_mo)

# Filtering dictionary
dictionary_mo.filter_extremes(no_below=int(len(texts_trigrams_mo) * 0.01), no_above=0.5)

# Creating the corpus
corpus_mo = [dictionary_mo.doc2bow(text) for text in texts_trigrams_mo]

# Function to compute coherence values for LDA models
def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=1):
    coherence_values = []
    model_list = []
    for num_topics in range(start, limit, step):
        model = gensim.models.LdaMulticore(corpus=corpus, num_topics=num_topics, id2word=dictionary, passes=10, workers=2, random_state=5)
        model_list.append(model)
        coherence_model = gensim.models.CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')
        coherence_values.append(coherence_model.get_coherence())
    return model_list, coherence_values

# Computing coherence values and selecting optimal model for Middle Office deals
model_list_mo, coherence_values_mo = compute_coherence_values(dictionary=dictionary_mo, corpus=corpus_mo, texts=texts_trigrams_mo, start=2, limit=13, step=1)
optimal_model_mo = model_list_mo[coherence_values_mo.index(max(coherence_values_mo))]

# Extracting top words from each topic for Middle Office deals
def extract_top_words(model, num_words):
    top_words_per_topic = []
    for topic_id in range(model.num_topics):
        top_words = model.show_topic(topic_id, num_words)
        top_words_per_topic.append(' '.join([word for word, _ in top_words]))
    return top_words_per_topic

top_words_per_topic_mo = extract_top_words(optimal_model_mo, 5)

# Assigning main topic column to the Middle Office dataframe
def get_main_topic(corpus):
    topic_weights = optimal_model_mo[corpus]
    main_topic = max(topic_weights, key=lambda x: x[1])[0]
    return main_topic

df_middle_office['main_topic'] = [get_main_topic(corp) for corp in corpus_mo]

# Adding the topic name correlated with main topic rank to the Middle Office dataframe
df_middle_office['main_topic_name'] = df_middle_office['main_topic'].map(lambda x: top_words_per_topic_mo[x])

# Plotting the distribution of topics with top words as labels for Middle Office deals
topic_counts_mo = df_middle_office['main_topic'].value_counts().sort_index()
plt.style.use('fivethirtyeight')
plt.figure(figsize=(12, 6))
sns.barplot(x=topic_counts_mo.index, y=topic_counts_mo.values)
plt.xticks(range(len(top_words_per_topic_mo)), top_words_per_topic_mo, rotation=45, ha='right')
plt.xlabel('Topic')
plt.ylabel('Number of Deals')
plt.title('Distribution of Topics in Middle Office Deals')
plt.show()

# Export the Middle Office dataframe with main topics and sentiment analysis to Excel
df_middle_office.to_excel('Middle_Office_Topics_with_Sentiment.xlsx', index=False)
