# Load Data
df = pd.read_csv('/mnt/data/closed_won_final.csv')  # Adjust the path to your data
df = df.head(1000)  # Limiting to 1000 rows for demonstration
total_rows = len(df)

# Function to chunk text based on token length
def chunk_text(text, max_length=128):
    tokens = tokenizer.encode(text, truncation=False)
    # Split tokens into chunks of size max_length
    return [tokens[i:i+max_length] for i in range(0, len(tokens), max_length)]

def classify_texts(df_batch):
    assigned_categories = []
    sub_categories = []
    specific_processes = []
    account_opening_flags = []

    # Updated prompt with categories
    prompt_template = """
    You are an AI tasked with classifying financial and business process texts. For each text, first **understand the context** and then classify it into one of the following main categories, subcategories, or assign it to "Other" if none of the categories fit.

    **Main Categories:**
    1. **Product Capability** (e.g., Strong product offerings such as ACH Direct Send, BAI reporting, treasury services, cash management solutions, corporate banking solutions)
       - Subcategories: ACH Direct Send, BAI Reporting, Treasury Services, Other Product Capability, Cash Management Solutions, Reporting Enhancements

    2. **Client-Specific Solutions** (e.g., New Account Opening, Existing Account Management, Account Modification, Account Setup, Subsidiary Accounts, Cross-border Transactions, Multi-currency Account Setup)
       - Subcategories: New Account (e.g., "New Account International," Corporate DDA Opening, Multi-currency Account Setup), Account Modification (Account Maintenance, Currency Account Modification, Limits Adjustments), Incremental Account (Subsidiary Account Addition, Multiple Account Setup), Account Transfer (Cross-border Account Transfer, Entity Transfer, Currency Switch)

    3. **Client Services** (e.g., Client Onboarding, Client Support, Client Mandate, Client Interest, Client Engagement, Client Implementation, Client Customization)
       - Subcategories: Client Onboarding (Smooth Onboarding, Implementation Services, Initial Setup), Client Mandate (Corporate Client Mandate, Legal Entity Directive, Mandate Changes), Client Deposit (Liquidity Management, Deposit Solutions, Multi-currency Deposits), Client Request, Client Interest

    4. **Relationship and Wallet Share** (e.g., Expanding relationships and increasing wallet share by offering additional services)
       - Subcategories: Relationship Expansion, Additional Services, Increased Wallet Share

    5. **Geographical Expansion** (e.g., Opening accounts in new regions or international entities, global business setup, foreign entity creation)
       - Subcategories: New Region Account (Regional Expansion, Cross-border Account Setup, Global Accounts), International Expansion (Foreign Entity Creation, Global Business Setup, Currency Expansion)

    6. **Competitor Comparison** (e.g., Highlighting advantages over competitors, particularly in pricing and service offerings, competitive benchmarking)
       - Subcategories: Pricing Advantages (Lower Pricing, Cost Reduction, Service Pricing), Service Comparison (Feature Comparison, Competitor Differentiation, Service Offering Comparison), Competitor Analysis

    7. **Client Onboarding and Implementation** (e.g., Efficient onboarding and smooth transitions, client integration, effective delivery)
       - Subcategories: Onboarding Efficiency, Smooth Transitions (Transition Assistance, Operational Shift, Business Continuity), Delivery Effectiveness

    8. **Market Disruption** (e.g., Stability during market disruptions, crisis-induced client switching, market switching, financial disruption)
       - Subcategories: Competitor Switch (Competitive Advantage, Service Improvement, Competitor Takeover), Market Disruption (Economic Instability, Crisis Response, Financial Disruption), Stability Offering (Market Stability, Risk Mitigation, Crisis Management)

    9. **Other Processes** (e.g., Regulatory Compliance, System Upgrades, Process Automation, H2H Connectivity)
       - Subcategories: System Development (Core Banking Systems, Payment Infrastructure), Market Disruption, Vault Services (Cash Vault, Safekeeping, Vault Operations), H2H Connectivity (Host-to-Host Connectivity, Direct Banking Integration)

    10. **Other**: If the text does not match any of the above categories, classify it as "Other" and assign a relevant sub-category or "Other" if no sub-category fits.

    For each text, provide a high-level category, a relevant sub-category with specifics, and a 2-3 word process description based on the context.

    Text: "{input_text}"

    ### Response Format:
    1. Main Category: [Product Capability / Client-Specific Solutions / Client Services / Relationship and Wallet Share / Geographical Expansion / Competitor Comparison / Client Onboarding and Implementation / Market Disruption / Other Processes / Other]
    2. Sub-Category: [Assigned Sub-Category with details (if applicable) or "Other"]
    3. Specific Process: [2-3 word description]
    """

    for idx, row in df_batch.iterrows():
        input_text = row["combined_text"]
        # **Chunk the input text**
        text_chunks = chunk_text(input_text)  # Use input_text, not the prompt
        
        # Variables to collect results from chunks
        combined_category = []
        combined_sub_category = []
        combined_process = []

        for chunk in text_chunks:
            # Convert chunk back to text
            chunk_text_decoded = tokenizer.decode(chunk)

            # Create prompt for each chunk
            prompt = prompt_template.format(input_text=chunk_text_decoded)

            # Tokenization and model inference for each chunk
            input_tokens = tokenizer(prompt, return_tensors="pt", truncation=True, padding=True, max_length=512).to(model.device)
            output = model.generate(
                input_tokens['input_ids'], 
                max_new_tokens=200, 
                pad_token_id=tokenizer.eos_token_id, 
                temperature=0.7  # Adjusting the temperature
            )

            # Decode and extract results
            generated_text = tokenizer.decode(output[0], skip_special_tokens=True)

            # Debugging: Check what the model has generated
            print(f"Generated text for chunk in row {idx}: {generated_text}")

            # Extract classification results from each chunk
            category, sub_category, process = extract_classification(generated_text)

            # Collect results from each chunk
            combined_category.append(category)
            combined_sub_category.append(sub_category)
            combined_process.append(process)

        # **Aggregate chunk results**
        # For simplicity, you can take the most frequent category and process across chunks.
        final_category = max(set(combined_category), key=combined_category.count)
        final_sub_category = max(set(combined_sub_category), key=combined_sub_category.count)
        final_process = " ".join(combined_process)  # Combine processes from chunks

        # Save the classification results for each row
        assigned_categories.append(final_category)
        sub_categories.append(final_sub_category)
        specific_processes.append(final_process)

        # Set flag for account opening
        if final_category == "Account Management" and "New Account" in final_sub_category:
            account_opening_flags.append("Yes")
        else:
            account_opening_flags.append("No")

    df_batch['assigned_category'] = assigned_categories
    df_batch['sub_category'] = sub_categories
    df_batch['specific_process'] = specific_processes
    df_batch['is_related_to_account_opening'] = account_opening_flags

    return df_batch


# Function to extract categories, sub-categories, and specific processes
def extract_classification(text):
    # Initialize default values
    high_level_category = "Not available"
    sub_category = "Not available"
    specific_process = "Not available"

    # Log full generated text for debugging
    print(f"Generated text:\n{text}\n")

    # Normalize text to handle extra spaces or inconsistencies
    text = text.replace("**", "").strip()  # Remove any Markdown formatting (**bold**)

    # Split the generated text by newlines
    lines = text.split("\n")
    
    # Iterate over each line to extract the relevant fields
    for line in lines:
        line = line.strip()  # Remove leading/trailing whitespace
        
        # Handle potential cases where the text might not follow the exact format
        if "Category:" in line:
            high_level_category = line.split("Category:")[1].strip() if "Category:" in line else high_level_category
        elif "Sub-category:" in line:
            sub_category = line.split("Sub-category:")[1].strip() if "Sub-category:" in line else sub_category
        elif "Process Description:" in line:
            specific_process = line.split("Process Description:")[1].strip() if "Process Description:" in line else specific_process
    
    # Log the extracted categories for debugging
    print(f"Extracted Category: {high_level_category}, Sub-Category: {sub_category}, Specific Process: {specific_process}")
    
    return high_level_category, sub_category, specific_process

# Process data in batches
def process_batches(df, batch_size):
    all_results = pd.DataFrame()

    start_time = time.time()

    for start in tqdm(range(0, total_rows, batch_size), desc="Processing Batches"):
        end = min(start + batch_size, total_rows)

        df_batch = df.iloc[start:end].copy()

        # Run classification task for the batch
        df_batch = classify_texts(df_batch)

        # Concatenate results into the full dataframe
        all_results = pd.concat([all_results, df_batch], ignore_index=True)

    # Measure total time taken
    end_time = time.time()
    elapsed_time = end_time - start_time
    print(f"Time taken to process {len(df)} records: {elapsed_time:.2f} seconds")

    # Save final combined results to a CSV file
    all_results.to_csv('final_classification_output.csv', index=False)
    print("Final results saved to final_classification_output.csv.")

# Running the batch process with batch size of 100
process_batches(df, batch_size=100)

# Display the first few rows of the final output
df_final = pd.read_csv('final_classification_output.csv')
print("First few rows of the final saved output:")
print(df_final.head())
