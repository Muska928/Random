import pandas as pd
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from tqdm import tqdm
import torch
from transformers import BitsAndBytesConfig

# Load data from Salesforce 

# Filter the dataset 

# Define the products of interest
products_of_interest = ["CB Credit Products", "Investment Bank", "Loans", "Loans and Trade", "Treasury Services"]

# Filter dataset for these products
df_filtered = df_closed_won[df_closed_won['product'].isin(products_of_interest)]

# Concatenate relevant text columns into a single string
df_filtered['combined_text'] = df_filtered[
    ['record_comment_text', 'description_text', 'executive_summary_text', 'win_loss_reason_text', 'win_loss_comments_text']
].apply(lambda x: ' '.join(x.dropna()), axis=1)

# Load the language model and tokenizer
model_path = "mistral/Mistral-7B-Instruct-v0.2"
tokenizer = AutoTokenizer.from_pretrained(
    model_path,
    trust_remote_code=True,
    padding_side="left",
    use_fast=True,
    add_bos_token=True,
    add_eos_token=True
)

tokenizer.pad_token = tokenizer.eos_token

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

model = AutoModelForCausalLM.from_pretrained(
    model_path,
    quantization_config=bnb_config,
    torch_dtype=torch.bfloat16,
    device_map="auto",
    trust_remote_code=True,
)

model.config.use_cache = False
model.config.pretraining_tp = 1
model.gradient_checkpointing_enable()

# Define the text-generation pipeline
generation_pipeline = pipeline('text-generation', model=model, tokenizer=tokenizer)

# One-shot prompt template for classifying 

# Maximum length of input tokens
max_input_length = 200

# Classify each deal as 
classifications = []
for text in tqdm(df_filtered['combined_text'], desc="Classifying LOB"):
    truncated_text = tokenizer.decode(tokenizer.encode(text, max_length=max_input_length, truncation=True), skip_special_tokens=True)
    prompt = classification_prompt_template.format(input_text=truncated_text)
    
    response = generation_pipeline(prompt, max_new_tokens=10, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)
    lob_classification = response[0]['generated_text'].strip() if isinstance(response, list) and 'generated_text' in response[0] else response['generated_text'].strip()
    
    classifications.append(lob_classification)

# Add classifications to the DataFrame
df_filtered['LOB_Classification'] = classifications

# Separate into two DataFrames for 
df_middle_office = df_filtered[df_filtered['LOB_Classification'].str.contains("Middle Office")]
df_non_middle_office = df_filtered[df_filtered['LOB_Classification'].str.contains("Non-Middle Office")]

# Optionally, display the DataFrames to verify results
print("Middle Office Deals:\n", df_middle_office.head())
print("\nNon-Middle Office Deals:\n", df_non_middle_office.head())
