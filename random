# --- Imports
import numpy as np
import pandas as pd
from sklearn.model_selection import KFold, train_test_split
from sklearn.linear_model import LinearRegression   # positive=True => non-negative coefs
from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# =========================
# 0) Load & basic cleaning
# =========================
# If df already exists in your notebook, comment the next line.
df = pd.read_excel("elockbox_spec_pli.xlsx")  # <-- update path if needed

target_col = "pli_active_cycle_time"

# Categorical columns you want scores for + to encode
cat_cols = [
    "prod_tx",
    "transaction_repair_ind",
    "pli_type",
    "assoc_region_accnt",       # use assoc_sub_region_account if that's your field
]

# Numeric columns (leave numeric; no manual binning)
num_cols = [
    "bnd_ct_accnts"             # add more true numeric predictors here
]

# Clean categorical: to string, strip, replace missing-like with "other"
def clean_cats(df, cat_cols):
    for c in cat_cols:
        df[c] = df[c].astype("string").str.strip()
        df[c] = df[c].fillna("other")
        df[c] = df[c].replace(
            to_replace=pd.Series(["nan", "None", "True", "False", ""], dtype="string"),
            value="other"
        )
    return df

df = clean_cats(df, cat_cols)

# ======================================================
# 1) Score tables (category mean of target, divided by 10)
# ======================================================
def make_score_dict(df, col, target=target_col, scale_div=10):
    """Return {category: round(mean(target)/scale_div)} sorted desc by score."""
    return (
        df.groupby(col)[target]
          .mean()
          .div(scale_div)
          .round(0)
          .sort_values(ascending=False)
          .to_dict()
    )

# Build score dictionaries exactly like your screenshot (divide by 10)
wo_request_type_scores   = make_score_dict(df, "prod_tx")
transaction_repair_scores = make_score_dict(df, "transaction_repair_ind")
pli_type_scores          = make_score_dict(df, "pli_type")
assoc_region_scores      = make_score_dict(df, "assoc_region_accnt")

print("wo_request_type_scores =", wo_request_type_scores)
print("payment_type_scores =", pli_type_scores)        # rename/adjust if you have payment_type
print("assoc_region_scores =", assoc_region_scores)

# ===============================================
# 2) Train/test split BEFORE any target encoding
# ===============================================
y = df[target_col].values
X = df[cat_cols + num_cols].copy()

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.20, random_state=42
)

# ==================================================
# 3) Out-of-fold target encoding (with smoothing)
# ==================================================
def oof_target_encode(X_tr, y_tr, X_te, cols, n_splits=5, smoothing=20.0):
    """
    Leakage-safe K-fold target encoding.
    - Train OOF averages for train rows
    - Full-train averages for test rows
    - Smoothing toward global mean
    """
    X_tr_enc = X_tr.copy()
    X_te_enc = X_te.copy()
    global_mean = float(np.mean(y_tr))
    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)

    for col in cols:
        oof_vals = pd.Series(index=X_tr.index, dtype=float)

        # OOF mapping
        for tr_idx, val_idx in kf.split(X_tr):
            tr_fold = X_tr.iloc[tr_idx]
            y_fold  = y_tr[tr_idx]
            means   = tr_fold.groupby(col)[y_fold].mean()
            counts  = tr_fold.groupby(col)[y_fold].count()
            means_smooth = (means * counts + global_mean * smoothing) / (counts + smoothing)
            oof_vals.iloc[val_idx] = (
                X_tr.iloc[val_idx][col].map(means_smooth).fillna(global_mean)
            )

        # Final map for test using full train
        means_full  = X_tr.groupby(col)[y_tr].mean()
        counts_full = X_tr.groupby(col)[y_tr].count()
        means_full_smooth = (means_full * counts_full + global_mean * smoothing) / (counts_full + smoothing)

        X_tr_enc[col + "_avg"] = oof_vals.values
        X_te_enc[col + "_avg"] = X_te[col].map(means_full_smooth).fillna(global_mean).values

    return X_tr_enc.drop(columns=cols), X_te_enc.drop(columns=cols)

Xtr_enc, Xte_enc = oof_target_encode(
    X_train.copy(), y_train, X_test.copy(),
    cols=cat_cols, n_splits=5, smoothing=20.0
)

# Build feature lists post-encoding
encoded_cat_cols = [c + "_avg" for c in cat_cols]
numeric_features = [c for c in Xtr_enc.columns if c in num_cols]

# =================================================
# 4) Pipeline: scale numerics + NON-NEGATIVE OLS
# =================================================
preproc = ColumnTransformer(
    transformers=[
        ("num", StandardScaler(with_mean=True, with_std=True), numeric_features),
        ("cats", "passthrough", encoded_cat_cols),  # already on target scale
    ],
    remainder="drop"
)

# Non-negative coefficients (intercept unconstrained; set fit_intercept=False if needed)
model = LinearRegression(positive=True)

pipe = Pipeline([("prep", preproc), ("reg", model)])

# Fit & evaluate
pipe.fit(Xtr_enc, y_train)
r2_train = pipe.score(Xtr_enc, y_train)
r2_test  = pipe.score(Xte_enc, y_test)
print(f"\nTrain R²: {r2_train:.4f} | Test R²: {r2_test:.4f}")

# =================================================
# 5) Coefficients table + normalized weights
# =================================================
feature_names = pipe.named_steps["prep"].get_feature_names_out()
coefs = pipe.named_steps["reg"].coef_.astype(float)

# All coefs are >= 0 by construction
coef_df = (
    pd.DataFrame({"attributes": feature_names, "coefficients": coefs})
      .sort_values("coefficients", ascending=False)
      .reset_index(drop=True)
)

# Normalized weights (percentage contribution) based on coefficients
coef_sum = coef_df["coefficients"].sum()
coef_df["weight_%"] = (coef_df["coefficients"] / coef_sum * 100.0) if coef_sum > 0 else 0.0
coef_df["weight_%"] = coef_df["weight_%"].round(2)

# Pretty print like your image
print("\nCoefficients Table:")
print("-" * 60)
print(f"{'No.':<4} {'coefficients':>12}   {'attributes'}")
print("-" * 60)
for i, row in coef_df.iterrows():
    print(f"{i:<4} {row['coefficients']:>12.6f}   {row['attributes']}")
print("-" * 60)

# Optional: also print with weights
print("\nCoefficients with Weights (% of total coefficient mass):")
print("-" * 60)
print(f"{'No.':<4} {'coef':>10}   {'weight_%':>8}   {'attributes'}")
print("-" * 60)
for i, row in coef_df.iterrows():
    print(f"{i:<4} {row['coefficients']:>10.6f}   {row['weight_%']:>8.2f}   {row['attributes']}")
print("-" * 60)

# Show first few processed rows (train/test) if you want to mirror your earlier prints
Xtr_proc = pd.DataFrame(
    pipe.named_steps["prep"].transform(Xtr_enc),
    columns=feature_names, index=Xtr_enc.index
)
Xte_proc = pd.DataFrame(
    pipe.named_steps["prep"].transform(Xte_enc),
    columns=feature_names, index=Xte_enc.index
)

print("\nFirst few rows of processed train data:")
print(Xtr_proc.head())
print("\nFirst few rows of processed test data:")
print(Xte_proc.head())
