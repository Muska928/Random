from sklearn.model_selection import ParameterGrid
import gensim
from gensim.models.coherencemodel import CoherenceModel
import pyLDAvis
import pyLDAvis.gensim_models as gensimvis  # Interface for gensim-based LDA models
import matplotlib.pyplot as plt
import seaborn as sns

# Function for LDA grid search
def lda_grid_search(dictionary, corpus, texts, param_grid):
    best_coherence = -1
    best_params = None
    best_model = None
    coherence_values = []

    # Grid search over the hyperparameter space
    for params in ParameterGrid(param_grid):
        print(f"Training model with: {params}")
        model = gensim.models.LdaModel(
            corpus=corpus,
            id2word=dictionary,
            num_topics=params['num_topics'],
            passes=params['passes'],
            alpha=params['alpha'],
            eta=params['eta'],
            random_state=42
        )

        # Compute coherence score
        coherence_model = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')
        coherence = coherence_model.get_coherence()
        coherence_values.append((params, coherence))
        
        print(f"Coherence Score: {coherence}")

        if coherence > best_coherence:
            best_coherence = coherence
            best_params = params
            best_model = model

    print(f"\nBest Model -> Topics: {best_params['num_topics']}, Passes: {best_params['passes']}, Alpha: {best_params['alpha']}, Eta: {best_params['eta']}, Coherence: {best_coherence}")
    return best_model, coherence_values

# Grid search parameter grid
param_grid = {
    'num_topics': [5, 10, 15],  # Adjust as needed
    'passes': [5, 10],  # Number of passes over the dataset
    'alpha': ['auto', 'symmetric', 0.01, 0.1, 0.5],  # Different alpha values
    'eta': ['auto', 'symmetric', 0.01, 0.1, 0.5]  # Different eta values
}

# Step 1: Process and tokenize the dataset (assuming `non_rst_cleaned` is available)
non_rst_cleaned['tokens'] = non_rst_cleaned['processed_text'].apply(tokenize_text)

# Step 2: Prepare corpus for LDA using bigrams/trigrams if needed
texts = non_rst_cleaned['tokens'].tolist()

# Step 3: Create dictionary and corpus for LDA
dictionary = gensim.corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]

# Step 4: Run grid search with LDA to find the optimal model
optimal_model, coherence_values = lda_grid_search(dictionary, corpus, texts, param_grid)

# Step 5: Visualizing the tuning results (optional)
params, coherence = zip(*coherence_values)
params_combined = [f"{p['num_topics']}_{p['passes']}_{p['alpha']}_{p['eta']}" for p in params]

plt.figure(figsize=(12, 6))
plt.plot(params_combined, coherence)
plt.xticks(rotation=45, ha='right')
plt.xlabel('Params (num_topics_passes_alpha_eta)', fontsize=14)
plt.ylabel('Coherence Score', fontsize=14)
plt.title('LDA Model Tuning Results', fontsize=16)
plt.tight_layout()
plt.show()

# Step 6: Apply the best model
def get_main_topic(corpus, model):
    topic_weights = model[corpus]
    main_topic = max(topic_weights, key=lambda x: x[1])  # Get the topic with the highest weight
    return main_topic[0]

non_rst_cleaned['main_topic'] = [get_main_topic(corp, optimal_model) for corp in corpus]

# Extract topic names
top_words_per_topic = extract_top_words(optimal_model, 5)

# Assign the topic name correlated with the main topic rank
non_rst_cleaned['main_topic_name'] = non_rst_cleaned['main_topic'].apply(lambda x: top_words_per_topic[x])

# Step 7: Calculate the frequency of each topic
topic_frequency = non_rst_cleaned['main_topic_name'].value_counts().reset_index()
topic_frequency.columns = ['Topic Name', 'Frequency']

# Print the topic frequency summary
print("\nLDA Topic Frequency Summary:")
print(topic_frequency)

# Optional: Save to Excel
topic_frequency.to_excel('lda_topic_frequency_summary.xlsx', index=False)

# Step 8: Plot the distribution of topics
plt.figure(figsize=(12, 6))
sns.barplot(x='Topic Name', y='Frequency', data=topic_frequency, palette='coolwarm')

# Adding frequencies on top of bars
for i, freq in enumerate(topic_frequency['Frequency']):
    plt.text(i, freq + 20, str(freq), ha='center', fontsize=12, fontweight='bold')

plt.xticks(rotation=45, ha='right', fontsize=12)
plt.xlabel('Topic Name', fontsize=14)
plt.ylabel('Frequency', fontsize=14)
plt.title('LDA Topic Frequency Summary', fontsize=16, weight='bold')
plt.tight_layout()
plt.show()

# Step 9: Visualization with pyLDAvis
lda_vis_data = gensimvis.prepare(optimal_model, corpus, dictionary)
pyLDAvis.save_html(lda_vis_data, 'lda_visualization.html')  # Save the visualization to an HTML file
