# Load Data
df = pd.read_csv('closed_won_final.csv')  # Adjust the path to your data
df = df.head(1)  # Limiting to 1000 rows for demonstration
total_rows = len(df)

# Function to chunk text based on token length
def chunk_text(text, max_length=128):
    tokens = tokenizer.encode(text, truncation=False)
    # Split tokens into chunks of size max_length
    return [tokens[i:i+max_length] for i in range(0, len(tokens), max_length)]

# Updated classify_texts function with debugging
def classify_texts(df_batch):
    assigned_categories = []
    sub_categories = []
    specific_processes = []
    account_opening_flags = []

    # Updated prompt with detailed category descriptions
    prompt_template = """
    You are an AI tasked with classifying financial and business process texts. For each text, first understand the context and then classify it into one of the following main categories, subcategories, or assign it to "Other" if none of the categories fit.

    Text: {input_text}

    ### Main Categories:

    1. **Product Capability** (e.g., ACH Direct Send, BAI reporting, treasury services, etc.)
    2. **Client-Specific Solutions** (e.g., New Account Opening, Account Modification, Cross-border Transactions, etc.)
    3. **Client Services** (e.g., Client Onboarding, Client Mandate, etc.)
    4. **Relationship and Wallet Share** (e.g., Relationship Expansion, Increased Wallet Share, etc.)
    5. **Geographical Expansion** (e.g., Regional Expansion, International Expansion, etc.)
    6. **Competitor Comparison** (e.g., Pricing Advantages, Service Comparison, etc.)
    7. **Client Onboarding and Implementation** (e.g., Onboarding Efficiency, Smooth Transitions, etc.)
    8. **Market Disruption** (e.g., Competitor Switch, Market Instability, etc.)
    9. **Other Processes** (e.g., System Development, Vault Services, etc.)
    10. **Other**: If none of the above categories fit.

    ### Expected Output Format:
    1. Main Category: [Chosen Main Category]
    2. Sub-Category: [Chosen Sub-Category]
    3. Specific Process: [Brief Process Description]
    """

    for idx, row in df_batch.iterrows():
        input_text = row["combined_text"]
        print(f"Processing row {idx+1}/{len(df_batch)}: {input_text[:100]}...")  # Debug: Show first 100 characters of the text

        # **Chunk the input text**
        text_chunks = chunk_text(input_text)  # Use input_text, not the prompt
        print(f"Text split into {len(text_chunks)} chunk(s).")  # Debug: Number of chunks
        
        # Variables to collect results from chunks
        combined_category = []
        combined_sub_category = []
        combined_process = []

        for chunk_idx, chunk in enumerate(text_chunks):
            # Convert chunk back to text
            chunk_text_decoded = tokenizer.decode(chunk)
            print(f"Processing chunk {chunk_idx+1}/{len(text_chunks)}: {chunk_text_decoded[:100]}...")  # Debug: Show first 100 characters of the chunk
            
            # Create prompt for each chunk
            prompt = prompt_template.format(input_text=chunk_text_decoded)

            # Tokenization and model inference for each chunk
            input_tokens = tokenizer(prompt, return_tensors="pt", truncation=True, padding=True, max_length=512).to(model.device)
            output = model.generate(
                input_tokens['input_ids'], 
                max_new_tokens=100,  # Limit tokens generated to avoid excessive output
                pad_token_id=tokenizer.eos_token_id,
                do_sample=True,  # Enable sampling for diverse responses
                temperature=0.7  # Adjusting the temperature
            )

            # Decode and extract results
            generated_text = tokenizer.decode(output[0], skip_special_tokens=True)
            print(f"Generated text for chunk {chunk_idx+1}: {generated_text[:200]}...")  # Debug: Show first 200 characters of generated text

            # Extract classification results from each chunk
            category, sub_category, process = extract_classification(generated_text)

            # Collect results from each chunk
            combined_category.append(category)
            combined_sub_category.append(sub_category)
            combined_process.append(process)

        # **Aggregate chunk results**
        # For simplicity, you can take the most frequent category and process across chunks.
        final_category = max(set(combined_category), key=combined_category.count)
        final_sub_category = max(set(combined_sub_category), key=combined_sub_category.count)
        final_process = " ".join(combined_process)  # Combine processes from chunks
        print(f"Final category: {final_category}, sub-category: {final_sub_category}, process: {final_process}")  # Debug: Final output for the row

        # Save the classification results for each row
        assigned_categories.append(final_category)
        sub_categories.append(final_sub_category)
        specific_processes.append(final_process)

        # Set flag for account opening
        if final_category == "Account Management" and "New Account" in final_sub_category:
            account_opening_flags.append("Yes")
        else:
            account_opening_flags.append("No")

    df_batch['assigned_category'] = assigned_categories
    df_batch['sub_category'] = sub_categories
    df_batch['specific_process'] = specific_processes
    df_batch['is_related_to_account_opening'] = account_opening_flags

    return df_batch

# Function to extract categories, sub-categories, and specific processes
def extract_classification(text):
    # Initialize default values
    high_level_category = "Not available"
    sub_category = "Not available"
    specific_process = "Not available"

    # Log full generated text for debugging
    print(f"Generated text:\n{text}\n")  # Debug: Show full generated text

    # Normalize text to handle extra spaces or inconsistencies
    text = text.replace("**", "").strip()  # Remove any Markdown formatting (**bold**)

    # Split the generated text by newlines
    lines = text.split("\n")
    
    # Iterate over each line to extract the relevant fields
    for line in lines:
        line = line.strip()  # Remove leading/trailing whitespace
        
        # Handle potential cases where the text might not follow the exact format
        if "Main Category:" in line:
            high_level_category = line.split("Main Category:")[1].strip() if "Main Category:" in line else high_level_category
        elif "Sub-Category:" in line:
            sub_category = line.split("Sub-Category:")[1].strip() if "Sub-Category:" in line else sub_category
        elif "Specific Process:" in line:
            specific_process = line.split("Specific Process:")[1].strip() if "Specific Process:" in line else specific_process
    
    # Log the extracted categories for debugging
    print(f"Extracted Category: {high_level_category}, Sub-Category: {sub_category}, Specific Process: {specific_process}")  # Debug: Print extracted categories
    
    return high_level_category, sub_category, specific_process

# Process data in batches
def process_batches(df, batch_size):
    all_results = pd.DataFrame()

    start_time = time.time()

    for start in tqdm(range(0, total_rows, batch_size), desc="Processing Batches"):
        end = min(start + batch_size, total_rows)

        df_batch = df.iloc[start:end].copy()

        # Run classification task for the batch
        df_batch = classify_texts(df_batch)

        # Concatenate results into the full dataframe
        all_results = pd.concat([all_results, df_batch], ignore_index=True)

    # Measure total time taken
    end_time = time.time()
    elapsed_time = end_time - start_time
    print(f"Time taken to process {len(df)} records: {elapsed_time:.2f} seconds")

    # Save final combined results to a CSV file
    all_results.to_csv('final_classification_output.csv', index=False)
    print("Final results saved to final_classification_output.csv.")

# Running the batch process with batch size of 100
process_batches(df, batch_size=1)

# Display the first few rows of the final output
df_final = pd.read_csv('final_classification_output.csv')
print("First few rows of the final saved output:")
print(df_final.head())
