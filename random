import pyLDAvis
import pyLDAvis.gensim_models as gensimvis

# Generate the pyLDAvis visualization
vis_data = gensimvis.prepare(optimal_model, corpus, dictionary)

# Filter out only the rows where 'Category' is actually a topic (e.g., 'Topic1', 'Topic2', ..., 'Topic14')
topic_categories = [f"Topic{i+1}" for i in range(optimal_model.num_topics)]  # Generates 'Topic1', 'Topic2', ..., 'Topic14'
topic_proportions = vis_data.topic_info.loc[vis_data.topic_info['Category'].isin(topic_categories), ['Category', 'Freq']]

# Normalize the frequencies to get percentages
total_freq = topic_proportions['Freq'].sum()
topic_proportions['Freq'] = (topic_proportions['Freq'] / total_freq) * 100  # Convert to percentage

# Remove duplicates if any exist
topic_proportions = topic_proportions.drop_duplicates(subset=['Category'])

# Sort topics by their proportions
sorted_topics = topic_proportions.sort_values(by='Freq', ascending=False)

# Fetch the top words for each topic (matching PyLDAvis' λ=1 setting)
print("Normalized Topic Proportions and Top Words:")

# Extracting the top words for each topic based on λ=1
for idx, row in sorted_topics.iterrows():
    topic_id = int(row['Category'].replace("Topic", ""))  # Extract topic number
    top_words = optimal_model.show_topic(topic_id - 1, topn=10)  # Get the top 10 words for the topic
    top_words_str = ', '.join([word for word, _ in top_words])  # Convert to a string
    print(f"{row['Category']} ({row['Freq']:.2f}%): {top_words_str}")

# Visualize the pyLDAvis output (in a Jupyter Notebook, for example)
pyLDAvis.display(vis_data)
