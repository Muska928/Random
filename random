import re
import pandas as pd
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from tqdm import tqdm
import torch
from transformers import BitsAndBytesConfig


# Concatenate text columns into a single string variable
df_salesforce['combined_text'] = df_salesforce[
    ['record_comment_text', 'description_text', 'executive_summary_text', 'win_loss_reason_text', 'win_loss_comments_text']
].apply(lambda x: ' '.join(x.dropna()), axis=1)

# Preprocess the text
def preprocess_text(text):
    text = text.lower()  # Convert text to lowercase
    text = re.sub(r'[^a-zA-Z\s]', '', text)  # Remove special characters and numbers
    text = re.sub(r'\s+', ' ', text).strip()  # Remove extra whitespace
    return text

# Apply preprocessing
df_salesforce['combined_text'] = df_salesforce['combined_text'].apply(preprocess_text)

# Sample 10 records for testing
df_sample = df_salesforce.sample(n=10, random_state=42)  # random_state for reproducibility

# Load Mistral model and tokenizer from the specified folder with device_map='auto'
model_path = "mistral/Mistral-7B-Instruct-v0.2"

tokenizer = AutoTokenizer.from_pretrained(
    model_path,
    trust_remote_code=True,
    padding_side="left",
    use_fast=True,
    add_bos_token=True,
    add_eos_token=True
)

tokenizer.pad_token = tokenizer.eos_token

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

# Ensure proper device mapping and loading
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    quantization_config=bnb_config,
    torch_dtype=torch.bfloat16,
    device_map="auto",  # Ensure model is loaded with correct device mapping
    trust_remote_code=True,
)

model.config.use_cache = False  # silence the warnings
model.config.pretraining_tp = 1
model.gradient_checkpointing_enable()



# Maximum length of input tokens
max_input_length = 200

# Batch processing size
batch_size = 10

# Generate the output
results = []
for i in tqdm(range(0, len(df_sample), batch_size), desc="Processing"):
    batch_texts = df_sample['combined_text'].iloc[i:i+batch_size].tolist()
    truncated_texts = [tokenizer.decode(tokenizer.encode(text, max_length=max_input_length, truncation=True), skip_special_tokens=True) for text in batch_texts]
    prompts = [one_shot_template.format(input_text=text) for text in truncated_texts]

    # Process batches asynchronously
    batch_responses = generation_pipeline(prompts, max_new_tokens=50, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)

    # Print batch_responses to understand its structure
    print(batch_responses)

    # Extract generated text from batch_responses
    for response in batch_responses:
        generated_text = response[0]['generated_text'] if isinstance(response, list) and 'generated_text' in response[0] else response['generated_text']
        results.append(generated_text)

# Combine the results into a single column
df_sample['final_response'] = results
df_sample['all_information'] = df_sample['final_response'].apply(lambda x: 'LOB: ' + re.search(r'LOB: (.*?)\n', x).group(1) + ', Sentiment: ' + re.search(r'Sentiment: (.*?)\n', x).group(1) + ', Content Retrieved: ' + re.search(r'Content Retrieved: (.*)', x).group(1))

# Save the results to a CSV file
df_sample.to_csv('Salesforce_Deals_Text_Analytics_Output.csv', index=False)

# Optionally, display the DataFrame to verify results
print(df_sample['all_information'])



-----
version 2


import re
import pandas as pd
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from tqdm import tqdm
import torch
from transformers import BitsAndBytesConfig


# Concatenate text columns into a single string variable
df_salesforce['combined_text'] = df_salesforce[
    ['record_comment_text', 'description_text', 'executive_summary_text', 'win_loss_reason_text', 'win_loss_comments_text']
].apply(lambda x: ' '.join(x.dropna()), axis=1)

# Preprocess the text
def preprocess_text(text):
    text = text.lower()  # Convert text to lowercase
    text = re.sub(r'[^a-zA-Z\s]', '', text)  # Remove special characters and numbers
    text = re.sub(r'\s+', ' ', text).strip()  # Remove extra whitespace
    return text

# Apply preprocessing
df_salesforce['combined_text'] = df_salesforce['combined_text'].apply(preprocess_text)

# Sample 10 records for testing
df_sample = df_salesforce.sample(n=10, random_state=42)  # random_state for reproducibility

# Load Mistral model and tokenizer from the specified folder with device_map='auto'
model_path = "mistral/Mistral-7B-Instruct-v0.2"

tokenizer = AutoTokenizer.from_pretrained(
    model_path,
    trust_remote_code=True,
    padding_side="left",
    use_fast=True,
    add_bos_token=True,
    add_eos_token=True
)

tokenizer.pad_token = tokenizer.eos_token

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

# Ensure proper device mapping and loading
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    quantization_config=bnb_config,
    torch_dtype=torch.bfloat16,
    device_map="auto",  # Ensure model is loaded with correct device mapping
    trust_remote_code=True,
)

model.config.use_cache = False  # silence the warnings
model.config.pretraining_tp = 1
model.gradient_checkpointing_enable()

# Maximum length of input tokens
max_input_length = 200

# Batch processing size
batch_size = 10

# Generate the output
results = []
for i in tqdm(range(0, len(df_sample), batch_size), desc="Processing"):
    batch_texts = df_sample['combined_text'].iloc[i:i+batch_size].tolist()
    truncated_texts = [tokenizer.decode(tokenizer.encode(text, max_length=max_input_length, truncation=True), skip_special_tokens=True) for text in batch_texts]
    prompts = [one_shot_template.format(input_text=text) for text in truncated_texts]

    # Process batches asynchronously
    batch_responses = generation_pipeline(prompts, max_new_tokens=50, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)

    # Extract generated text from batch_responses
    for response in batch_responses:
        generated_text = response[0]['generated_text'] if isinstance(response, list) and 'generated_text' in response[0] else response['generated_text']
        results.append(generated_text)

# Parse the results and extract required fields
def parse_response(response):
    lob_match = re.search(r'LOB: (.*?)\n', response)
    sentiment_match = re.search(r'Sentiment: (.*?)\n', response)
    content_match = re.search(r'Content Retrieved: (.*)', response)
    
    lob = lob_match.group(1).strip() if lob_match else 'Not available'
    sentiment = sentiment_match.group(1).strip() if sentiment_match else 'Not available'
    content = content_match.group(1).strip() if content_match else 'Not available'
    
    return f'LOB: {lob}, Sentiment: {sentiment}, Content Retrieved: {content}'

# Combine the results into a single column
df_sample['final_response'] = results
df_sample['all_information'] = df_sample['final_response'].apply(parse_response)

# Save the results to a CSV file
df_sample.to_csv('Salesforce_Deals_Text_Analytics_Output.csv', index=False)

# Optionally, display the DataFrame to verify results
print(df_sample['all_information'])


----version 3:

import pandas as pd
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from tqdm import tqdm
import torch
from transformers import BitsAndBytesConfig


# Concatenate text columns into a single string variable
df_salesforce['combined_text'] = df_salesforce[
    ['record_comment_text', 'description_text', 'executive_summary_text', 'win_loss_reason_text', 'win_loss_comments_text']
].apply(lambda x: ' '.join(x.dropna()), axis=1)

# Preprocess the text
def preprocess_text(text):
    text = text.lower()  # Convert text to lowercase
    text = re.sub(r'[^a-zA-Z\s]', '', text)  # Remove special characters and numbers
    text = re.sub(r'\s+', ' ', text).strip()  # Remove extra whitespace
    return text

# Apply preprocessing
df_salesforce['combined_text'] = df_salesforce['combined_text'].apply(preprocess_text)

# Sample 10 records for testing
df_sample = df_salesforce.sample(n=10, random_state=42)  # random_state for reproducibility

# Load Mistral model and tokenizer from the specified folder with device_map='auto'
model_path = "mistral/Mistral-7B-Instruct-v0.2"

tokenizer = AutoTokenizer.from_pretrained(
    model_path,
    trust_remote_code=True,
    padding_side="left",
    use_fast=True,
    add_bos_token=True,
    add_eos_token=True
)

tokenizer.pad_token = tokenizer.eos_token

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

# Ensure proper device mapping and loading
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    quantization_config=bnb_config,
    torch_dtype=torch.bfloat16,
    device_map="auto",  # Ensure model is loaded with correct device mapping
    trust_remote_code=True,
)

model.config.use_cache = False  # silence the warnings
model.config.pretraining_tp = 1
model.gradient_checkpointing_enable()

# Sentiment analysis pipeline with the Mistral instruct model
generation_pipeline = pipeline('text-generation', model=model, tokenizer=tokenizer)  # No device argument here

# Maximum length of input tokens
max_input_length = 200

# Batch processing size
batch_size = 10

# Generate the output
results = []
for i in tqdm(range(0, len(df_sample), batch_size), desc="Processing"):
    batch_texts = df_sample['combined_text'].iloc[i:i+batch_size].tolist()
    truncated_texts = [tokenizer.decode(tokenizer.encode(text, max_length=max_input_length, truncation=True), skip_special_tokens=True) for text in batch_texts]
    prompts = [one_shot_template.format(input_text=text) for text in truncated_texts]

    # Process batches asynchronously
    batch_responses = generation_pipeline(prompts, max_new_tokens=50, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)

    # Extract generated text from batch_responses
    for response in batch_responses:
        generated_text = response[0]['generated_text'] if isinstance(response, list) and 'generated_text' in response[0] else response['generated_text']
        results.append(generated_text)

# Parse the results and extract required fields
def parse_response(response):
    lines = response.split('\n')
    lob = 'Not available'
    sentiment = 'Not available'
    content = 'Not available'
    
    for line in lines:
        if line.startswith('LOB:'):
            lob = line.replace('LOB:', '').strip()
        elif line.startswith('Sentiment:'):
            sentiment = line.replace('Sentiment:', '').strip()
        elif line.startswith('Content Retrieved:'):
            content = line.replace('Content Retrieved:', '').strip()
    
    return f'LOB: {lob}, Sentiment: {sentiment}, Content Retrieved: {content}'

# Combine the results into a single column
df_sample['final_response'] = results
df_sample['all_information'] = df_sample['final_response'].apply(parse_response)

# Save the results to a CSV file
df_sample.to_csv('Salesforce_Deals_Text_Analytics_Output.csv', index=False)

# Optionally, display the DataFrame to verify results
print(df_sample['all_information'])
