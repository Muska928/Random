from sklearn.model_selection import ParameterGrid
import gensim
from gensim.models.coherencemodel import CoherenceModel
import matplotlib.pyplot as plt
import seaborn as sns

# Function to tokenize and prepare the text for LDA
def tokenize_text(text):
    return [token.text for token in nlp(text)]

# Define the get_ngram_freq function
def get_ngram_freq(tokens_list, n):
    ngrams = zip(*[tokens_list[i:] for i in range(n)])
    return [' '.join(ngram) for ngram in ngrams]

# Function to prepare corpus for LDA and specify n-gram type
def prepare_corpus_for_lda(tokens, ngram_type='unigram'):
    labeled_ngrams = []
    
    # Generate n-grams based on the input n-gram type
    if ngram_type == 'bigram':
        ngram_list = get_ngram_freq(tokens.split(), 2)
    elif ngram_type == 'trigram':
        ngram_list = get_ngram_freq(tokens.split(), 3)
    else:
        ngram_list = tokens.split()

    # Label n-gram type and print output for each record
    labeled_ngrams = [(ngram_type, ngram) for ngram in ngram_list]
    print(f"--- {ngram_type.capitalize()}s being fed into LDA ---")
    
    for idx, record in enumerate(labeled_ngrams):
        print(f"Record {idx + 1}: {record}")

    return labeled_ngrams

# Function to extract top words from each topic
def extract_top_words(model, num_words):
    top_words_per_topic = []
    for topic_id in range(model.num_topics):
        top_words = model.show_topic(topic_id, num_words)
        top_words_per_topic.append(", ".join([word for word, _ in top_words]))
    return top_words_per_topic

# Assign the main topic for each document in the corpus
def get_main_topic(corpus):
    topic_weights = optimal_model[corpus]
    main_topic = max(topic_weights, key=lambda x: x[1])  # Get the topic with the highest weight
    return main_topic[0]

# Function for LDA grid search
def lda_grid_search(dictionary, corpus, texts, param_grid):
    best_coherence = -1
    best_params = None
    best_model = None

    # Grid search over topic number and passes
    for params in ParameterGrid(param_grid):
        model = gensim.models.LdaModel(
            corpus=corpus,
            id2word=dictionary,
            num_topics=params['num_topics'],
            passes=params['passes'],
            random_state=42
        )

        # Compute coherence score
        coherence_model = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')
        coherence = coherence_model.get_coherence()

        # Print the combination of params and coherence score
        print(f"Topics: {params['num_topics']}, Passes: {params['passes']} -> Coherence: {coherence}")

        if coherence > best_coherence:
            best_coherence = coherence
            best_params = params
            best_model = model

    print(f"\nBest Model -> Topics: {best_params['num_topics']}, Passes: {best_params['passes']}, Coherence: {best_coherence}")
    return best_model

# Grid search parameter grid
param_grid = {
    'num_topics': [5, 10, 15],
    'passes': [5, 10]
}

# Step 1: Process and tokenize the dataset (assuming `non_rst_cleaned` is available)
non_rst_cleaned['tokens'] = non_rst_cleaned['processed_text'].apply(tokenize_text)

# Step 2: Prepare the corpus for LDA (you can adjust for unigrams, bigrams, or trigrams)
# You can choose 'unigram', 'bigram', or 'trigram' based on your requirements.
labeled_ngrams = prepare_corpus_for_lda(' '.join(non_rst_cleaned['processed_text']), 'bigram')

# Step 3: Create dictionary and corpus for LDA
dictionary = gensim.corpora.Dictionary(labeled_ngrams)
corpus = [dictionary.doc2bow(text) for text in labeled_ngrams]

# Step 4: Run grid search with LDA to find the optimal model
optimal_model = lda_grid_search(dictionary, corpus, labeled_ngrams, param_grid)

# Step 5: Apply the get_main_topic function to all documents
non_rst_cleaned['main_topic'] = [get_main_topic(corp) for corp in corpus]

# Step 6: Extract topic names
top_words_per_topic = extract_top_words(optimal_model, 5)

# Step 7: Assign the topic name correlated with the main topic rank
non_rst_cleaned['main_topic_name'] = non_rst_cleaned['main_topic'].apply(lambda x: top_words_per_topic[x])

# Step 8: Calculate the frequency of each topic
topic_frequency = non_rst_cleaned['main_topic_name'].value_counts().reset_index()
topic_frequency.columns = ['Topic Name', 'Frequency']

# Step 9: Print the topic frequency summary
print("\nLDA Topic Frequency Summary:")
print(topic_frequency)

# Optionally, save to Excel
topic_frequency.to_excel('lda_topic_frequency_summary.xlsx', index=False)

# Step 10: Plot the distribution of topics
plt.figure(figsize=(12, 6))
sns.barplot(x='Topic Name', y='Frequency', data=topic_frequency, palette='coolwarm')

# Adding frequencies on top of bars
for i, freq in enumerate(topic_frequency['Frequency']):
    plt.text(i, freq + 20, str(freq), ha='center', fontsize=12, fontweight='bold')

plt.xticks(rotation=45, ha='right', fontsize=12)
plt.xlabel('Topic Name', fontsize=14)
plt.ylabel('Frequency', fontsize=14)
plt.title('LDA Topic Frequency Summary', fontsize=16, weight='bold')
plt.tight_layout()
plt.show()
